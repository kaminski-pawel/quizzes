[
  {
    "content": "A company is implementing an application on Amazon EC2 instances. The application needs to process incoming transactions. When the application detects a transaction that is not valid, the application must send a chat message to the company's support team. To send the message, the application needs to retrieve the access token to authenticate by using the chat API.<><>A developer needs to implement a solution to store the access token. The access token must be encrypted at rest and in transit. The access token must also be accessible from other AWS accounts.<><>Which solution will meet these requirements with the LEAST management overhead?",
    "widget": "CI",
    "answers": [
      {
        "content": "Use an AWS Systems Manager Parameter Store SecureString parameter that uses an AWS Key Management Service (AWS KMS) AWS managed key to store the access token. Add a resource-based policy to the parameter to allow access from other accounts. Update the IAM role of the EC2 instances with permissions to access Parameter Store. Retrieve the token from Parameter Store with the decrypt flag enabled. Use the decrypted access token to send the message to the chat.",
        "is_correct": false
      },
      {
        "content": "Encrypt the access token by using an AWS Key Management Service (AWS KMS) customer managed key. Store the access token in an Amazon DynamoDB table. Update the IAM role of the EC2 instances with permissions to access DynamoDB and AWS KMS. Retrieve the token from DynamoDDecrypt the token by using AWS KMS on the EC2 instances. Use the decrypted access token to send the message to the chat.",
        "is_correct": false
      },
      {
        "content": "Use AWS Secrets Manager with an AWS Key Management Service (AWS KMS) customer managed key to store the access token. Add a resource-based policy to the secret to allow access from other accounts. Update the IAM role of the EC2 instances with permissions to access Secrets Manager. Retrieve the token from Secrets Manager. Use the decrypted access token to send the message to the chat.",
        "is_correct": true
      },
      {
        "content": "Encrypt the access token by using an AWS Key Management Service (AWS KMS) AWS managed key. Store the access token in an Amazon S3 bucket. Add a bucket policy to the S3 bucket to allow access from other accounts. Update the IAM role of the EC2 instances with permissions to access Amazon S3 and AWS KMS. Retrieve the token from the S3 bucket. Decrypt the token by using AWS KMS on the EC2 instances. Use the decrypted access token to send the massage to the chat.",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "Use AWS Secrets Manager with an AWS Key Management Service (AWS KMS) - this option uses AWS Secrets Manager, which eliminates the need for explicitly enabling the decryption flag. It also has more granular permissions for accessing the secrets, making it slightly more secure. DynamoDB in Option B and S3 in Option D are also options for storing the access token, but they require additional steps for encryption and decryption, adding more management overhead. Overall, Option A is a valid solution, but the other options have less management overhead while still meeting the requirements.<><>AWS Systems Manager Parameter Store SecureString - a valid solution that meets the requirements of encrypting the access token at rest and in transit, as well as allowing access from other AWS accounts. However, it requires an additional step of retrieving the token with the decrypt flag enabled, which adds some management overhead when compared to other options."
    }
  },
  {
    "content": "A developer has created a Java application that makes HTTP requests directly to AWS services. Application logging shows 5xx HTTP response codes that occur at irregular intervals. The errors are affecting users.<><>How should the developer update the application to improve the application's resiliency?",
    "widget": "CI",
    "answers": [
      {
        "content": "Revise the request content in the application code.",
        "is_correct": false
      },
      {
        "content": "Use the AWS SDK for Java to interact with AWS APIs.",
        "is_correct": true
      },
      {
        "content": "Scale out the application so that more instances of the application are running.",
        "is_correct": false
      },
      {
        "content": "Add additional logging to the application code.",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "All AWS SDKs have a built-in retry mechanism with an algorithm that uses exponential backoff. This algorithm implements increasingly longer wait times between retries for consecutive error responses. Most exponential backoff algorithms use jitter (randomized delay) to prevent successive collisions."
    }
  },
  {
    "content": "A global company has a mobile app with static data stored in an Amazon S3 bucket in the us-east-1 Region. The company serves the content through an Amazon CloudFront distribution. The company is launching the mobile app in South Africa. The data must reside in the af-south-1 Region. The company does not want to deploy a specific mobile client for South Africa.<><>What should the company do to meet these requirements?",
    "widget": "CI",
    "answers": [
      {
        "content": "Use the CloudFront geographic restriction feature to block access to users in South Africa.",
        "is_correct": false
      },
      {
        "content": "Create a Lambda@Edge function. Associate the Lambda@Edge function as an origin request trigger with the CloudFront distribution to change the S3 origin Region.",
        "is_correct": true
      },
      {
        "content": "Create a Lambda@Edge function. Associate the Lambda@Edge function as a viewer response trigger with the CloudFront distribution to change the S3 origin Region.",
        "is_correct": false
      },
      {
        "content": "Include af-south-1 in the alternate domain name (CNAME) of the CloudFront distribution.",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "Lambda@Ege with a Origin Request Trigger. Because of different data for this two destinations.<><>Geographic restriction doesn't change the S3 bucket origin region, it simply restricts the users based trying to access CF."
    }
  },
  {
    "content": "A developer is testing an AWS Lambda function by using the AWS Serverless Application Model (AWS SAM) local CLI. The application that is implemented by the Lambda function makes several AWS API calls by using the AWS software development kit (SDK). The developer wants to allow the function to make AWS API calls in a test AWS account from the developer's laptop.<><>What should the developer do to meet these requirements?",
    "widget": "CI",
    "answers": [
      {
        "content": "Edit the template.yml file. Add the AWS_ACCESS_KEY_ID property and the AWS_SECRET_ACCESS_KEY property in the Globals section.",
        "is_correct": false
      },
      {
        "content": "Add a test profile by using the aws configure command with the --profile option. Run AWS SAM by using the sam local invoke command with the -profile option.",
        "is_correct": true
      },
      {
        "content": "Edit the template.yml tile. For the AWS::Serverless::Function resource, set the role to an IAM role in the AWS account.",
        "is_correct": false
      },
      {
        "content": "Run the function by using the sam local invoke command. Override the AWS_ACCESS_KEY_ID parameter and the AWS_SECRET_ACCESS_KEY parameter by specifying the --parameter-overrides option.",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "The AWS CLI stores this information in a profile (a collection of settings) named default in the credentials and config files. These files are located in the .aws file in your home directory. By default, the information in this profile is used when you run an AWS CLI command that doesn't explicitly specify a profile to use."
    }
  },
  {
    "content": "A developer designed an application on an Amazon EC2 instance. The application makes API requests to objects in an Amazon S3 bucket.<><>Which combination of steps will ensure that the application makes the API requests in the MOST secure manner? (Choose two.)",
    "widget": "CM",
    "answers": [
      {
        "content": "Create an IAM user that has permissions to the S3 bucket. Add the user to an IAM group.",
        "is_correct": false
      },
      {
        "content": "Create an IAM role that has permissions to the S3 bucket.",
        "is_correct": true
      },
      {
        "content": "Add the IAM role to an instance profile. Attach the instance profile to the EC2 instance.",
        "is_correct": true
      },
      {
        "content": "Create an IAM role that has permissions to the S3 bucket. Assign the role to an 1AM group.",
        "is_correct": false
      },
      {
        "content": "Store the credentials of the IAM user in the environment variables on the EC2 instance.",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "You choice is either User or Role. Creating a role and assigning it to something is preffered over creating a user.<><>An IAM role is an IAM identity that you can create in your account that has specific permissions. An IAM role is similar to an IAM user, in that it is an AWS identity with permission policies that determine what the identity can and cannot do in AWS. However, instead of being uniquely associated with one person, a role is intended to be assumable by anyone who needs it. Also, a role does not have standard long-term credentials such as a password or access keys associated with it. Instead, when you assume a role, it provides you with temporary security credentials for your role session."
    }
  },
  {
    "content": "A developer is configuring an Amazon CloudFront distribution for a new application to provide encryption in transit. The application is running in the eu-west-1 Region. The developer creates a new certificate in AWS Certificate Manager (ACM) in eu-west-1, but the certificate is not visible in the CloudFront distribution settings.<><>What should the developer do to fix this problem?",
    "widget": "CI",
    "answers": [
      {
        "content": "Create the certificate for the domain in the same Region as the application. Ensure that the alternate domain name (CNAME) in the distribution settings matches the domain name in the certificate.",
        "is_correct": false
      },
      {
        "content": "Create the certificate in the us-east-1 Region. Ensure that the alternate domain name (CNAME) in the distribution settings matches the domain name in the certificate.",
        "is_correct": true
      },
      {
        "content": "Recreate the CloudFront distribution in the same Region as the certificate.",
        "is_correct": false
      },
      {
        "content": "Specify the ACM certificate name as the default root object of the CloudFront distribution.",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "To use an ACM certificate with Amazon CloudFront, you must request or import the certificate in the US East (N. Virginia) region. ACM certificates in this region that are associated with a CloudFront distribution are distributed to all the geographic locations configured for that distribution."
    }
  },
  {
    "content": "A developer is building an application that runs behind an Application Load Balancer (ALB). The ALB is configured as the origin for an Amazon CloudFront distribution. Users will log in to the application by using their social media accounts.<><>How can the developer authenticate users?",
    "widget": "CI",
    "answers": [
      {
        "content": "Validate the users by inspecting the tokens in an AWS Lambda authorizer on the ALB.",
        "is_correct": false
      },
      {
        "content": "Configure the ALB to use Amazon Cognito as one of the authentication providers.",
        "is_correct": true
      },
      {
        "content": "Configure CloudFront to use Amazon Cognito as one of the authentication providers.",
        "is_correct": false
      },
      {
        "content": "Validate the users by calling the Amazon Cognito API in an AWS Lambda authorizer on the ALB.",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "You cannot directly integrate Cognito User Pools with CloudFront distribution as you have to create a separate Lambda@Edge function to accomplish the authentication via Cognito User Pools.<><>ALB’s new authentication action provides authentication through social Identity Providers (IdP) like Google, Facebook, and Amazon through Amazon Cognito. It also natively integrates with any OpenID Connect protocol compliant IdP, providing secure authentication and a single sign-on experience across your applications."
    }
  },
  {
    "content": "A company has an application that analyzes photographs. A developer is preparing the application for deployment to Amazon EC2 instances. The application's image analysis functions require a mix of GPU instances and CPU instances that run on Amazon Linux. The developer needs to add code to the application so that the functions can determine whether they are running on a GPU instance.<><>What should the functions do to obtain this information?",
    "widget": "CI",
    "answers": [
      {
        "content": "Call the DescribeInstances API operation and filter on the current instance ID. Examine the ElasticGpuAssociations property.",
        "is_correct": false
      },
      {
        "content": "Evaluate the GPU AVAILABLE environment variable.",
        "is_correct": false
      },
      {
        "content": "Call the DescribeElasticGpus API operation.",
        "is_correct": false
      },
      {
        "content": "Retrieve the instance type from the instance metadata.",
        "is_correct": true
      }
    ],
    "explanation": {
      "content": "You can obtain metadata info from EC2 instance using 169.254.169.254 IP.<><>The only way to tell is getting the instance metadata. Don't be confused by the other options provided because they are there to confuse you because they are related to Elastic Elastic Graphics accelerators. Those are different from GPU instances. Elastic Graphics accelerators are a low-cost alternative to using GPU graphics instance types (such as G2 and G3)."
    }
  },
  {
    "content": "A company has an application that uses Amazon Cognito user pools as an identity provider. The company must secure access to user records. The company has set up multi-factor authentication (MFA). The company also wants to send a login activity notification by email every time a user logs in.<><>What is the MOST operationally efficient solution that meets this requirement?",
    "widget": "CI",
    "answers": [
      {
        "content": "Create an AWS Lambda function that uses Amazon Simple Email Service (Amazon SES) to send the email notification. Add an Amazon API Gateway API to invoke the function. Call the API from the client side when login confirmation is received.",
        "is_correct": false
      },
      {
        "content": "Create an AWS Lambda function that uses Amazon Simple Email Service (Amazon SES) to send the email notification. Add an Amazon Cognito post authentication Lambda trigger for the function.",
        "is_correct": true
      },
      {
        "content": "Create an AWS Lambda function that uses Amazon Simple Email Service (Amazon SES) to send the email notification. Create an Amazon CloudWatch Logs log subscription filter to invoke the function based on the login status.",
        "is_correct": false
      },
      {
        "content": "Configure Amazon Cognito to stream all logs to Amazon Kinesis Data Firehose. Create an AWS Lambda function to process the streamed logs and to send the email notification based on the login status of each user.",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "[Cognito post authentication Lambda trigger](https://docs.aws.amazon.com/cognito/latest/developerguide/user-pool-lambda-post-authentication.html)"
    }
  },
  {
    "content": "A company hosts a three-tier web application on AWS behind an Amazon CloudFront distribution. A developer wants a dashboard to monitor error rates and anomalies of the CloudFront distribution with the shortest possible refresh interval.<><>Which combination of slops should the developer take to meet these requirements? (Choose two.)",
    "widget": "CM",
    "answers": [
      {
        "content": "Activate real-time logs on the CloudFront distribution. Create a stream in Amazon Kinesis Data Streams.",
        "is_correct": true
      },
      {
        "content": "Export the CloudFront logs to an Amazon S3 bucket. Detect anomalies and error rates with Amazon QuickSight.",
        "is_correct": false
      },
      {
        "content": "Configure Amazon Kinesis Data Streams to deliver logs to Amazon OpenSearch Service (Amazon Elasticsearch Service). Create a dashboard in OpenSearch Dashboards (Kibana).",
        "is_correct": true
      },
      {
        "content": "Create Amazon CloudWatch alarms based on expected values of selected CloudWatch metrics to detect anomalies and errors.",
        "is_correct": false
      },
      {
        "content": "Design an Amazon CloudWatch dashboard of the selected CloudFront distribution metrics.",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "With real-time logs, you can customize the information collected and where it gets delivered. The real-time logs are integrated with Amazon Kinesis Data Streams to enable delivery of these logs to a generic HTTP endpoint using Amazon Kinesis Data Firehose. Amazon Kinesis Data Firehose can deliver logs to Amazon S3, Amazon Redshift, Amazon Elasticsearch Service (Amazon ES), and service providers like Datadog, Datazoom, New Relic, and Splunk. Using these logs, you can create real-time dashboards, set up alerts, and investigate anomalies or respond to operational events quickly."
    }
  },
  {
    "content": "A developer creates a customer managed key for multiple AWS users to encrypt data in Amazon S3. The developer configures Amazon Simple Notification Service (Amazon SNS) to publish a message if key deletion is scheduled. The developer needs to preserve any SNS messages that cannot be delivered so that those messages can be reprocessed.<><>Which AWS service or feature should the developer use to meet this requirement?",
    "widget": "CI",
    "answers": [
      {
        "content": "Amazon Simple Email Service (Amazon SES)",
        "is_correct": false
      },
      {
        "content": "AWS Lambda",
        "is_correct": false
      },
      {
        "content": "Amazon Simple Queue Service (Amazon SQS)",
        "is_correct": true
      },
      {
        "content": "Amazon CloudWatch alarm",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "SQS Dead letter queue"
    }
  },
  {
    "content": "A developer needs to deploy an application to AWS Elastic Beanstalk for a company. The application consists of a single Docker image. The company's automated continuous integration and continuous delivery (CI/CD) process builds the Docker image and pushes the image to a public Docker registry.<><>How should the developer deploy the application to Elastic Beanstalk?",
    "widget": "CI",
    "answers": [
      {
        "content": "Create a Dockerfile. Configure Elastic Beanstalk to build the application as a Docker image.",
        "is_correct": false
      },
      {
        "content": "Create a docker-compose.yml file. Use the Elastic Beanstalk CLI to deploy the application.",
        "is_correct": true
      },
      {
        "content": "Create a .zip file that contains the Docker image. Upload the .zip file to Elastic Beanstalk.",
        "is_correct": false
      },
      {
        "content": "Create a Dockerfile. Run the Elastic Beanstalk CLI eb local run command in the same directory.",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "AWS Elastic Beanstalk can launch Docker environments by building an image described in a Dockerfile or pulling a remote Docker image. If you're deploying a remote Docker image, you don't need to include a Dockerfile. Instead, if you are also using Docker Compose, use a docker-compose.yml file, which specifies an image to use and additional configuration options. If you are not using Docker Compose with your Docker environments, use a Dockerrun.aws.json file instead.<><><><>Deploy a remote Docker image to Elastic Beanstalk.<><>After testing your container locally, deploy it to an Elastic Beanstalk environment. Elastic Beanstalk uses the docker-compose.yml file to pull and run your image if you are using Docker Compose. Otherwise, Elastic Beanstalk uses the Dockerrun.aws.json instead.<><>Use the EB CLI to create an environment and deploy your image.<><>~/remote-docker$ eb create environment-name."
    }
  },
  {
    "content": "A company is using AWS CodeDeploy for all production deployments. A developer has an Amazon Elastic Container Service (Amazon ECS) application that uses the CodeDeployDefault.ECSAIIAtOnce configuration. The developer needs to update the production environment in increments of 10% until the entire production environment is updated.<><>Which CodeDeploy configuration should the developer use to meet these requirements?",
    "widget": "CI",
    "answers": [
      {
        "content": "CodeDeployDefault.ECSCanary10Percent5Minutes",
        "is_correct": false
      },
      {
        "content": "CodeDeployDefault.ECSLinear10PercentEvery3Minutes",
        "is_correct": true
      },
      {
        "content": "CodeDeployDefault.OneAtATime",
        "is_correct": false
      },
      {
        "content": "CodeDeployDefault.LambdaCanary10Percent5Minutes",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "CodeDeployDefault.ECSLinear10PercentEvery3Minutes - Shifts 10 percent of traffic every three minutes until all traffic is shifted.<><>CodeDeployDefault.ECSCanary10Percent5Minutes - Shifts 10 percent of traffic in the first increment. The remaining 90 percent is deployed five minutes later."
    }
  },
  {
    "content": "A company has point-of-sale devices across thousands of retail shops that synchronize sales transactions with a centralized system. The system includes an Amazon API Gateway API that exposes an AWS Lambda function. The Lambda function processes the transactions and stores the transactions in Amazon RDS for MySQL. The number of transactions increases rapidly during the day and is near zero at night.<><>How can a developer increase the elasticity of the system MOST cost-effectively?",
    "widget": "CI",
    "answers": [
      {
        "content": "Migrate from Amazon RDS to Amazon Aurora MySQL. Use an Aurora Auto Scaling policy to scale road replicas based on CPU consumption.",
        "is_correct": false
      },
      {
        "content": "Migrate from Amazon RDS to Amazon Aurora MySQL. Use an Aurora Auto Scaling policy to scale read replicas based on the number of database connections.",
        "is_correct": false
      },
      {
        "content": "Create an Amazon Simple Queue Service (Amazon SQS) queue. Publish transactions to the queue. Set the queue to invoke the Lambda function. Turn on enhanced fanout for the Lambda function.",
        "is_correct": false
      },
      {
        "content": "Create an Amazon Simple Queue Service (Amazon SQS) queue. Publish transactions to the queue. Set the queue to invoke the Lambda function. Set the reserved concurrency of the Lambda function to be less than the number of database connections.",
        "is_correct": true
      }
    ],
    "explanation": {
      "content": "Read replicas are for read... transactions means that something have to be written.<><>There is no such thing of fanout for lambda.<><>SQS + Lambda Reserved Concurrency ensures we don't saturate database connections by configuring reserved concurrency - this protects against data loss by preventing an overload of the database during peak POS system usage."
    }
  },
  {
    "content": "A developer is writing an AWS Lambda function. The Lambda function needs to access items that are stored in an Amazon DynamoDB table.<><>What is the MOST secure way to configure this access for the Lambda function?",
    "widget": "CI",
    "answers": [
      {
        "content": "Create an IAM user that has permissions to access the DynamoDB table. Create an access key for this user. Store the access key ID and secret access key in the Lambda function environment variables.",
        "is_correct": false
      },
      {
        "content": "Add a resource-based policy to the DynamoDB table to allow access from the Lambda function's IAM role.",
        "is_correct": false
      },
      {
        "content": "Create an IAM policy that allows access to the DynamoDB table. Attach this policy to the Lambda function's IAM role.",
        "is_correct": true
      },
      {
        "content": "Create a DynamoDB Accelerator (DAX) cluster. Configure the Lambda function to use the DAX duster to access the DynamoDB table.",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "Using IAM Role is the most secure way.<><>Only Identity based policies can be used for DynamoDB, you can't add a resource-based policy to the DynamoDB table."
    }
  },
  {
    "content": "A developer is implementing user authentication and authorization for a web application that is hosted on an Amazon EC2 instance. The developer needs to ensure that the user credentials are encrypted and secure when they are stored and transmitted.<><>Which solution will meet these requirements?",
    "widget": "CI",
    "answers": [
      {
        "content": "Activate web server modules for authentication and authorization on the instance. Use HTTP basic authentication for the user login.",
        "is_correct": false
      },
      {
        "content": "Deploy a custom authentication and authorization API over HTTP. Store the user credentials on Amazon ElastiCache for Redis.",
        "is_correct": false
      },
      {
        "content": "Use Amazon Cognito to configure a user pool. Use the Amazon Cognito API to authenticate and authorize the users.",
        "is_correct": true
      },
      {
        "content": "Create IAM users. Assign the users to different IAM groups. Use AWS Single Sign-On to authenticate and authorize each user.",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "End User side authentication - Cognito.<><>SSO and IAM are for managing AWS resources. And, doesn’t make sense storing credentials in in-memory storages…<><>To assure in-transit encryption you can't use HTTP."
    }
  },
  {
    "content": "A company that has multiple offices uses an Amazon DynamoDB table to store employee payroll information. Item attributes consist of employee names, office identifiers, and cumulative daily hours worked The most frequently used query extracts a report of an alphabetical subset of employees for a specific office.<><>Which design of the DynamoDB table primary key will have the MINIMUM performance impact?",
    "widget": "CI",
    "answers": [
      {
        "content": "Partition key on the office identifier and sort key on the employee name",
        "is_correct": true
      },
      {
        "content": "Partition key on the employee name and sort key on the office identifier",
        "is_correct": false
      },
      {
        "content": "Partition key on the employee name",
        "is_correct": false
      },
      {
        "content": "Partition key on the office identifier",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "By having the office identifier as the partition key, DynamoDB will distribute the data evenly across multiple partitions, allowing for efficient querying of data. Additionally, by having the employee name as the sort key, you can use the sort key to efficiently retrieve the alphabetical subset of employees for a specific office. This way, the query only needs to scan a smaller portion of the data, reducing the number of read operations and improving performance.<><>Partition key on the employee name and sort key on the office identifier - having a composite primary key with a partition key on the employee name and a sort key on the office identifier, would be an inefficient approach as the queries would need to scan a large portion of the data to retrieve the required subset of employees."
    }
  },
  {
    "content": "A company hosts a microservices application that uses Amazon API Gateway. AWS Lambda, Amazon Simple Queue Service (Amazon SQS), and Amazon DynamoDB. One of the Lambda functions adds messages to an SQS FIFO queue.<><>When a developer checks the application logs, the developer finds a few duplicated items in a DynamoDB table. The items were inserted by another polling function that processes messages from the queue.<><>What is the MOST likely cause of this issue?",
    "widget": "CI",
    "answers": [
      {
        "content": "Write operations on the DynamoDB table are being throttled.",
        "is_correct": false
      },
      {
        "content": "The SQS queue delivered the message to the function more than once.",
        "is_correct": false
      },
      {
        "content": "API Gateway duplicated the message in the SQS queue.",
        "is_correct": false
      },
      {
        "content": "The polling function timeout is greater than the queue visibility timeout.",
        "is_correct": true
      }
    ],
    "explanation": {
      "content": "It is very important to set the visibility timeout to a longer time because if the request does not get processed faster than the timeout, then it becomes visible again in the queue and its picked up by another service and reprocessed. This will result in duplicate processes.<><>Unlike standard queues, FIFO queues don't introduce duplicate messages. FIFO queues help you avoid sending duplicates to a queue.<><>This can happen because the message remains in the queue for a longer time, and it can be delivered to the same or another instance of the polling function again, which can result in duplicate items in the DynamoDB table. To resolve this issue, the developer can adjust the timeout settings and ensure that the queue visibility timeout is lower than the polling function's timeout. This will ensure that the message remains invisible to the queue for the desired time and is not delivered again during that period."
    }
  },
  {
    "content": "A development team has been using a builder server that is hosted on an Amazon EC2 instance to perform builds and deployments for the last 3 months. The EC2 instance's instance profile uses an IAM role that contains the Administrator Access managed policy. The development team must replace that policy with a policy that provides only the required permissions.<><>What is the FASTEST way to create a custom 1AM policy for the EC2 instance to meet this requirement?",
    "widget": "CI",
    "answers": [
      {
        "content": "Create a new IAM policy based on services that the build server deployed or updated in the last 3 months.",
        "is_correct": false
      },
      {
        "content": "Create a new IAM policy that includes all actions that AWS CloudTrail recorded for the IAM role in the last 3 months.",
        "is_correct": true
      },
      {
        "content": "Create a new permissions boundary policy that denies all access. Associate the permissions boundaries with the IAM role.",
        "is_correct": false
      },
      {
        "content": "Create a new IAM policy by using Amazon Athena to query an Amazon S3 bucket that contains AWS CloudTrail events that the IAM role performed in the last 3 months.",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "As an administrator or developer, you might grant permissions to IAM entities (users or roles) beyond what they require. IAM provides several options to help you refine the permissions that you grant. One option is to generate an IAM policy that is based on access activity for an entity. IAM Access Analyzer reviews your AWS CloudTrail logs and generates a policy template that contains the permissions that the entity used in your specified date range. You can use the template to create a policy with fine-grained permissions that grant only the permissions that are required to support your specific use case.<><>A. Create a new IAM policy based on services that the build server deployed or updated in the last 3 months. - Services isn’t very specific, there are multiple different IAM actions that can be performed on each service. Additionally there is no method to achieve what is required.<><>B. Create a new IAM policy that includes all actions that AWS CloudTrail recorded for the IAM role in the last 3 months. - Cloudtrail will record these requests. Cloudtrail has 90 day retention by default, so this is an accurate answer.<><>C. Create a new permissions boundary policy that denies all access. Associate the permissions boundaries with the IAM role. - IAM permissions boundaries have no relevance here, permissions boundaries don’t grant access.<><>D. Create a new IAM policy by using Amazon Athena to query an Amazon S3 bucket that contains AWS CloudTrail events that the IAM role performed in the last 3 months - This is a the best architected solution, but the question is FASTEST, and this is not the fastest method."
    }
  },
  {
    "content": "A developer needs to write an AWS CloudFormation template on a local machine and deploy a CloudFormation stack to AWS.<><>What must the developer do to complete these tasks?",
    "widget": "CI",
    "answers": [
      {
        "content": "Install the AWS CLI. Configure the AWS CLI by using an IAM user name and password.",
        "is_correct": false
      },
      {
        "content": "Install the AWS CLI. Configure the AWS CLI by using an SSH key.",
        "is_correct": false
      },
      {
        "content": "Install the AWS CLI. Configure the AWS CLI by using an IAM user access key and secret key.",
        "is_correct": true
      },
      {
        "content": "Install an AWS software development kit (SDK). Configure the SDK by using an X.509 certificate.",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "For Aws CLI Command, we need access key and secret access key.<><>In summary, to complete the task of writing an AWS CloudFormation template on a local machine and deploying a CloudFormation stack to AWS, the developer must install the AWS CLI and configure it by using their IAM user access key and secret key."
    }
  },
  {
    "content": "A developer is working on a web application that runs on Amazon Elastic Container Service (Amazon ECS) and uses an Amazon DynamoDB table to store data.<><>The application performs a large number of read requests against a small set of the table data.<><>How can the developer improve the performance of these requests? (Choose two.)",
    "widget": "CM",
    "answers": [
      {
        "content": "Create an Amazon ElastiCache cluster. Configure the application to cache data in the cluster.",
        "is_correct": true
      },
      {
        "content": "Create a DynamoDB Accelerator (DAX) cluster. Configure the application to use the DAX cluster for DynamoDB requests.",
        "is_correct": true
      },
      {
        "content": "Configure the application to make strongly consistent read requests against the DynamoDB table.",
        "is_correct": false
      },
      {
        "content": "Increase the read capacity of the DynamoDB table.",
        "is_correct": false
      },
      {
        "content": "Enable DynamoDB adaptive capacity.",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "Increasing the read capacity is more suitable for situations where the entire table is experiencing high read demand, rather than focusing on a small set of frequently accessed data.<><>Strongly consistent is only useful for race conditions - if not mission critical then eventual consitent will do Although increasing Read Capacity units (RCUs) is fastest way to increase performance, but it’s more for large amount of resultset than for quicker retrieval. Adaptive capacity? Unless you meant autoscaling. DynamoDB have provisioned RCU/WCU auto-scaling (same as previous issue on read capacity)."
    }
  },
  {
    "content": "A developer needs to use Amazon DynamoDB to store customer orders. The developer's company requires all customer data to be encrypted at rest with a key that the company generates.<><>What should the developer do to meet these requirements?",
    "widget": "CI",
    "answers": [
      {
        "content": "Create the DynamoDB table with encryption set to None. Code the application to use the key to decrypt the data when the application reads from the table. Code the application to use the key to encrypt the data when the application writes to the table.",
        "is_correct": false
      },
      {
        "content": "Store the key by using AWS Key Management Service (AWS KMS). Choose an AWS KMS customer managed key during creation of the DynamoDB table. Provide the Amazon Resource Name (ARN) of the AWS KMS key.",
        "is_correct": true
      },
      {
        "content": "Store the key by using AWS Key Management Service (AWS KMS). Create the DynamoDB table with default encryption. Include the kms:Encrypt parameter with the Amazon Resource Name (ARN) of the AWS KMS key when using the DynamoDB software development kit (SDK).",
        "is_correct": false
      },
      {
        "content": "Store the key by using AWS Key Management Service (AWS KMS). Choose an AWS KMS AWS managed key during creation of the DynamoDB table. Provide the Amazon Resource Name (ARN) of the AWS KMS key.",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "Unless you opted to encrypt your data by using an AWS managed CMK, DynamoDB formerly encrypted all table data at rest by using the AWS owned CMK. However, now you can opt to encrypt your data by using a customer managed CMK. [See docs](https://aws.amazon.com/blogs/database/bring-your-own-encryption-keys-to-amazon-dynamodb/)."
    }
  },
  {
    "content": "A developer is creating a solution to track an account's Amazon S3 buckets over time. The developer has created an AWS Lambda function that will run on a schedule. The function will list the account's S3 buckets and will store the list in an Amazon DynamoDB table. The developer receives a permissions error when the developer runs the function with the AWSLambdaBasicExecutionRole AWS managed policy.<><>Which combination of permissions should the developer use to resolve this error? (Choose two.)",
    "widget": "CM",
    "answers": [
      {
        "content": "Cross-account IAM role",
        "is_correct": false
      },
      {
        "content": "Permission for the Lambda function to list buckets in Amazon S3",
        "is_correct": true
      },
      {
        "content": "Permission for the Lambda function to write in DynamoDB",
        "is_correct": true
      },
      {
        "content": "Permission for Amazon S3 to invoke the Lambda function",
        "is_correct": false
      },
      {
        "content": "Permission for DynamoDB to invoke the Lambda function",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "According to the question, Lambda function only needs to write to the dynamodb table and dynamodb wont need to invoke the function(doesn't make sense). rather, the function needs access to the s3 list."
    }
  },
  {
    "content": "A company is adding items to an Amazon DynamoDB table from an AWS Lambda function that is written in Python. A developer needs to implement a solution that inserts records in the DynamoDB table and performs automatic retry when the insert fails.<><>Which solution meets these requirements with MINIMUM code changes?",
    "widget": "CI",
    "answers": [
      {
        "content": "Configure the Python code to run the AWS CLI through shell to call the PutItem operation",
        "is_correct": false
      },
      {
        "content": "Call the PutItem operation from Python by using the DynamoDB HTTP API",
        "is_correct": false
      },
      {
        "content": "Queue the items in AWS Glue, which will put them into the DynamoDB table",
        "is_correct": false
      },
      {
        "content": "Use the AWS software development kit (SDK) for Python (boto3) to call the PutItem operation",
        "is_correct": true
      }
    ],
    "explanation": {
      "content": "The AWS SDKs for DynamoDB automatically retry requests that receive this exception. Your request is eventually successful, unless your retry queue is too large to finish. Reduce the frequency of requests using Error retries and exponential backoff.<><>AWS SDKs implements exponential backoff which will retry the requests until they are successful."
    }
  },
  {
    "content": "A developer is writing an AWS Lambda function. The developer wants to log key events that occur during the Lambda function and include a unique identifier to associate the events with a specific function invocation.<><>Which of the following will help the developer accomplish this objective?",
    "widget": "CI",
    "answers": [
      {
        "content": "Obtain the request identifier from the Lambda context object. Architect the application to write logs to the console.",
        "is_correct": false
      },
      {
        "content": "Obtain the request identifier from the Lambda event object. Architect the application to write logs to a file.",
        "is_correct": false
      },
      {
        "content": "Obtain the request identifier from the Lambda event object. Architect the application to write logs to the console.",
        "is_correct": false
      },
      {
        "content": "Obtain the request identifier from the Lambda context object. Architect the application to write logs to a file.",
        "is_correct": true
      }
    ],
    "explanation": {
      "content": "The AWS Lambda context object provides information about the Lambda function invocation, including a unique identifier for the request. This identifier can be used to associate log events with a specific function invocation. Writing logs to a file is a better practice as it allows developers to store, archive, or analyze log data later. Writing logs to the console is generally not recommended for production-ready applications as it can cause issues like scrolling latency when there is a high volume of log events."
    }
  },
  {
    "content": "A company experienced partial downtime during the last deployment of a new application. AWS Elastic Beanstalk split the environment's Amazon EC2 instances into batches and deployed a new version one batch at a time after taking them out of service. Therefore, full capacity was not maintained during deployment.<><>The developer plans to release a new version of the application, and is looking for a policy that will maintain full capacity and minimize the impact of the failed deployment.<><>Which deployment policy should the developer use?",
    "widget": "CI",
    "answers": [
      {
        "content": "Immutable",
        "is_correct": true
      },
      {
        "content": "All at Once",
        "is_correct": false
      },
      {
        "content": "Rolling",
        "is_correct": false
      },
      {
        "content": "Rolling with an Additional Batch",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "Immutable infrastructure has become a new norm in IT operations.<><>Immutable Deployment is one of those approaches, and it simply means: Immutable: the “staging” environment, once ready to become production, doesn't change. If we need to change something, we then deploy new code on completely new infrastructure.<><>The benefits of an immutable infrastructure include more consistency and reliability in your infrastructure and a simpler, more predictable deployment process"
    }
  },
  {
    "content": "A company is providing services to many downstream consumers. Each consumer may connect to one or more services. This has resulted in a complex architecture that is difficult to manage and does not scale well. The company needs a single interface to manage these services to consumers.<><>Which AWS service should be used to refactor this architecture?",
    "widget": "CI",
    "answers": [
      {
        "content": "AWS Lambda",
        "is_correct": false
      },
      {
        "content": "AWS X-Ray",
        "is_correct": false
      },
      {
        "content": "Amazon SQS",
        "is_correct": false
      },
      {
        "content": "Amazon API Gateway",
        "is_correct": true
      }
    ],
    "explanation": {
      "content": ""
    }
  },
  {
    "content": "When a Developer tries to run an AWS CodeBuild project, it raises an error because the length of all environment variables exceeds the limit for the combined maximum of characters.<><>What is the recommended solution?",
    "widget": "CI",
    "answers": [
      {
        "content": "Add the export LC_ALL=ג€en_US.utf8ג€ command to the pre_build section to ensure POSIX localization.",
        "is_correct": false
      },
      {
        "content": "Use Amazon Cognito to store key-value pairs for large numbers of environment variables.",
        "is_correct": false
      },
      {
        "content": "Update the settings for the build project to use an Amazon S3 bucket for large numbers of environment variables.",
        "is_correct": false
      },
      {
        "content": "Use AWS Systems Manager Parameter Store to store large numbers of environment variables.",
        "is_correct": true
      }
    ],
    "explanation": {
      "content": "Use AWS Systems Manager Parameter Store to store large numbers of environment variables.<><>Possible cause: Your build is using environment variables that are too large for AWS CodeBuild. CodeBuild can raise errors once the length of all environment variables (all names and values added together) reach a combined maximum of around 5,500 characters.<><>Recommended solution: Use Amazon EC2 Systems Manager Parameter Store to store large environment variables and then retrieve them from your buildspec file."
    }
  },
  {
    "content": "A Development team decides to adopt a continuous integration/continuous delivery (CI/CD) process using AWS CodePipeline and AWS CodeCommit for a new application. However, management wants a person to review and approve the code before it is deployed to production.<><>How can the Development team add a manual approver to the CI/CD pipeline?",
    "widget": "CI",
    "answers": [
      {
        "content": "Use AWS SES to send an email to approvers when their action is required. Develop a simple application that allows approvers to accept or reject a build. Invoke an AWS Lambda function to advance the pipeline when a build is accepted.",
        "is_correct": false
      },
      {
        "content": "If approved, add an approved tag when pushing changes to the CodeCommit repository. CodePipeline will proceed to build and deploy approved commits without interruption.",
        "is_correct": false
      },
      {
        "content": "Add an approval step to CodeCommit. Commits will not be saved until approved.",
        "is_correct": false
      },
      {
        "content": "Add an approval action to the pipeline. Configure the approval action to publish to an Amazon SNS topic when approval is required. The pipeline execution will stop and wait for an approval.",
        "is_correct": true
      }
    ],
    "explanation": {
      "content": "Add an approval action to the pipeline. Configure the approval action to publish to an Amazon SNS topic when approval is required. The pipeline execution will stop and wait for an approval."
    }
  },
  {
    "content": "A Developer is migrating an on-premises application to AWS. The application currently takes user uploads and saves them to a local directory on the server. All uploads must be saved and made immediately available to all instances in an Auto Scaling group.<><>Which approach will meet these requirements?",
    "widget": "CI",
    "answers": [
      {
        "content": "Use Amazon EBS and configure the application AMI to use a snapshot of the same EBS instance on boot.",
        "is_correct": false
      },
      {
        "content": "Use Amazon S3 and rearchitect the application so all uploads are placed in S3.",
        "is_correct": true
      },
      {
        "content": "Use instance storage and share it between instances launched from the same Amazon Machine Image (AMI).",
        "is_correct": false
      },
      {
        "content": "Use Amazon EBS and file synchronization software to achieve eventual consistency among the Auto Scaling group.",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "The best solution should be EFS. But there is no EFS in the option. B is the next best.<><>With S3, there are efforts to modify the code, for new files, S3 has strong consistence. But for update/delete files, S3 has eventually consistence only.<><>For EBS + sync software data sync application could not make sure strong consistence. And there are a lot efforts for configuration. Advantage is that, there is no need to modify the code."
    }
  },
  {
    "content": "A developer has built a market application that stores pricing data in Amazon DynamoDB with Amazon ElastiCache in front. The prices of items in the market change frequently. Sellers have begun complaining that, after they update the price of an item, the price does not actually change in the product listing.<><>What could be causing this issue?",
    "widget": "CI",
    "answers": [
      {
        "content": "The cache is not being invalidated when the price of the item is changed",
        "is_correct": true
      },
      {
        "content": "The price of the item is being retrieved using a write-through ElastiCache cluster",
        "is_correct": false
      },
      {
        "content": "The DynamoDB table was provisioned with insufficient read capacity",
        "is_correct": false
      },
      {
        "content": "The DynamoDB table was provisioned with insufficient write capacity",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "Cache is not being invalidated after table write and this causes inconsistency."
    }
  },
  {
    "content": "The developer is creating a web application that collects highly regulated and confidential user data through a POST request. The web application is served through Amazon CloudFront. User names and phone numbers must be encrypted at the edge and must remain encrypted throughout the entire application stack.<><>What is the MOST secure way to meet these requirements?",
    "widget": "CI",
    "answers": [
      {
        "content": "Enforce Match Viewer with HTTPS Only on CloudFront.",
        "is_correct": false
      },
      {
        "content": "Use only the newest TLS security policy on CloudFront.",
        "is_correct": false
      },
      {
        "content": "Enforce a signed URL on CloudFront on the front end.",
        "is_correct": false
      },
      {
        "content": "Use field-level encryption on CloudFront.",
        "is_correct": true
      }
    ],
    "explanation": {
      "content": "Field-level encryption allows you to enable your users to securely upload sensitive information to your web servers. The sensitive information provided by your users is encrypted at the edge, close to the user, and remains encrypted throughout your entire application stack."
    }
  },
  {
    "content": "A Developer has been asked to create an AWS Lambda function that is triggered any time updates are made to items in an Amazon DynamoDB table. The function has been created, and appropriate permissions have been added to the Lambda execution role. Amazon DynamoDB streams have been enabled for the table, but the function is still not being triggered.<><>Which option would enable DynamoDB table updates to trigger the Lambda function?",
    "widget": "CI",
    "answers": [
      {
        "content": "Change the StreamViewType parameter value to NEW_AND_OLD_IMAGES for the DynamoDB table",
        "is_correct": false
      },
      {
        "content": "Configure event source mapping for the Lambda function",
        "is_correct": true
      },
      {
        "content": "Map an Amazon SNS topic to the DynamoDB streams",
        "is_correct": false
      },
      {
        "content": "Increase the maximum execution time (timeout) setting of the Lambda function",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "To process stream records in a DynamoDB table, you can create an AWS Lambda function and configure an event source mapping to the stream. This allows your Lambda function to be invoked whenever there are changes to the table. Enabling DynamoDB streams alone is not enough to trigger the Lambda function. You also need to create an event source mapping.<><>Change the StreamViewType param - is incorrect because the StreamViewType parameter is used to determine the information included in the stream records.<><>SNS topic to the DynamoDB streams - is incorrect because mapping an SNS topic to DynamoDB streams would not trigger the Lambda function.<><>Lambda timeout - is incorrect because increasing the maximum execution time (timeout) setting of the Lambda function would not enable DynamoDB table updates to trigger the Lambda function."
    }
  },
  {
    "content": "A company maintains a REST service using Amazon API Gateway and the API Gateway native API key validation. The company recently launched a new registration page, which allows users to sign up for the service. The registration page creates a new API key using CreateApiKey and sends the new key to the user. When the user attempts to call the API using this key, the user receives a 403 Forbidden error. Existing users are unaffected and can still call the API.<><>What code updates will grant these new users access to the API?",
    "widget": "CI",
    "answers": [
      {
        "content": "The createDeployment method must be called so the API can be redeployed to include the newly created API key.",
        "is_correct": false
      },
      {
        "content": "The updateAuthorizer method must be called to update the API's authorizer to include the newly created API key.",
        "is_correct": false
      },
      {
        "content": "The importApiKeys method must be called to import all newly created API keys into the current stage of the API.",
        "is_correct": false
      },
      {
        "content": "The createUsagePlanKey method must be called to associate the newly created API key with the correct usage plan.",
        "is_correct": true
      }
    ],
    "explanation": {
      "content": "To grant access to the API for newly registered users, the developer needs to associate the newly created API key with the API usage plan. To do this, the createUsagePlanKey method should be called to add the API key to the correct usage plan.<><>By default, API Gateway only grants access to API keys that are associated with a usage plan, and new API keys are not automatically associated with a usage plan. The usage plan specifies the throttling and quota limits for the API. Once the API key is added to the usage plan, the user should be able to access the API."
    }
  },
  {
    "content": "An application uploads photos to an Amazon S3 bucket. Each photo that is uploaded to the S3 bucket must be resized to a thumbnail image by the application.<><>Each thumbnail image is uploaded with a new name in the same S3 bucket.<><>Which AWS service can a developer configure to directly process each single S3 event for each S3 object upload?",
    "widget": "CI",
    "answers": [
      {
        "content": "Amazon EC2",
        "is_correct": false
      },
      {
        "content": "Amazon Elastic Container Service (Amazon ECS)",
        "is_correct": false
      },
      {
        "content": "AWS Elastic Beanstalk",
        "is_correct": false
      },
      {
        "content": "AWS Lambda",
        "is_correct": true
      }
    ],
    "explanation": {
      "content": "A developer can configure AWS Lambda to directly process each S3 event for each S3 object upload. AWS Lambda can be triggered by S3 events and can execute code that can resize the uploaded photo to a thumbnail and upload the thumbnail with a new name to the same S3 bucket."
    }
  },
  {
    "content": "A company is running a Docker application on Amazon ECS. The application must scale based on user load in the last 15 seconds.<><>How should a Developer instrument the code so that the requirement can be met?",
    "widget": "CI",
    "answers": [
      {
        "content": "Create a high-resolution custom Amazon CloudWatch metric for user activity data, then publish data every 30 seconds",
        "is_correct": false
      },
      {
        "content": "Create a high-resolution custom Amazon CloudWatch metric for user activity data, then publish data every 5 seconds",
        "is_correct": true
      },
      {
        "content": "Create a standard-resolution custom Amazon CloudWatch metric for user activity data, then publish data every 30 seconds",
        "is_correct": false
      },
      {
        "content": "Create a standard-resolution custom Amazon CloudWatch metric for user activity data, then publish data every 5 seconds",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "To scale the Docker application on Amazon ECS based on user load in the last 15 seconds, a high-resolution custom Amazon CloudWatch metric for user activity data should be created, and data should be published every 5 seconds.<><>high-res + 5 sec intervals is the correct answer because high-resolution custom metrics have a finer resolution than standard-resolution custom metrics, with data points that can be as frequent as 1 second. The user activity data needs to be measured in near-real-time to accurately reflect the current user load, and publishing the data every 5 seconds will provide the necessary frequency. This will allow Amazon ECS to scale the Docker application appropriately based on the user load.<><>Other options do not provide the required frequency for the user activity data, and therefore may not be accurate enough to scale the Docker application in a timely manner."
    }
  },
  {
    "content": "Where should the appspec.yml file be placed in order for AWS CodeDeploy to work?",
    "widget": "CI",
    "answers": [
      {
        "content": "In the root of the application source code directory structure",
        "is_correct": true
      },
      {
        "content": "In the bin folder along with all the complied code",
        "is_correct": false
      },
      {
        "content": "In an S3 bucket",
        "is_correct": false
      },
      {
        "content": "In the same folder as the application configuration files",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "If your application uses the EC2/On-Premises compute platform, the AppSpec file must be a YAML-formatted file named appspec.yml and it must be placed in the root of the directory structure of an application's source code.<><>The appspec.yml file should be placed in the root of the application source code directory structure for AWS CodeDeploy to work. This file is a configuration file that specifies the deployment process, including details such as the source files to be copied to the deployment target, the location where these files should be copied, the scripts to be run on the target instance before and after the deployment, and more. When CodeDeploy initiates a deployment, it looks for the appspec.yml file in the root directory of the application source code to determine how the deployment should be performed."
    }
  },
  {
    "content": "A Developer is working on an application that handles 10MB documents that contain highly-sensitive data. The application will use AWS KMS to perform client- side encryption.<><>What steps must be followed?",
    "widget": "CI",
    "answers": [
      {
        "content": "Invoke the Encrypt API passing the plaintext data that must be encrypted, then reference the customer managed key ARN in the KeyId parameter",
        "is_correct": false
      },
      {
        "content": "Invoke the GenerateRandom API to get a data encryption key, then use the data encryption key to encrypt the data",
        "is_correct": false
      },
      {
        "content": "Invoke the GenerateDataKey API to retrieve the encrypted version of the data encryption key to encrypt the data",
        "is_correct": false
      },
      {
        "content": "Invoke the GenerateDataKey API to retrieve the plaintext version of the data encryption key to encrypt the data",
        "is_correct": true
      }
    ],
    "explanation": {
      "content": "When uploading an object—Using the customer master key (CMK) ID, the client first sends a request to AWS KMS for a CMK that it can use to encrypt your object data. AWS KMS returns two versions of a randomly generated data key:<><>1. A plaintext version of the data key that the client uses to encrypt the object data.<><>2. A cipher blob of the same data key that the client uploads to Amazon S3 as object metadata.<><>Note: The client obtains a unique data key for each object that it uploads."
    }
  },
  {
    "content": "An application uses Amazon Kinesis Data Streams to ingest and process large streams of data records in real time. Amazon EC2 instances consume and process the data from the shards of the Kinesis data stream by using Amazon Kinesis Client Library (KCL). The application handles the failure scenarios and does not require standby workers. The application reports that a specific shard is receiving more data than expected. To adapt to the changes in the rate of data flow, the `hot` shard is resharded.<><>Assuming that the initial number of shards in the Kinesis data stream is 4, and after resharding the number of shards increased to 6, what is the maximum number of EC2 instances that can be deployed to process data from all the shards?",
    "widget": "CI",
    "answers": [
      {
        "content": "12",
        "is_correct": false
      },
      {
        "content": "6",
        "is_correct": true
      },
      {
        "content": "4",
        "is_correct": false
      },
      {
        "content": "1",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "Typically, when you use the KCL, you should ensure that the number of instances does not exceed the number of shards (except for failure standby purposes). Each shard is processed by exactly one KCL worker and has exactly one corresponding record processor, so you never need multiple instances to process one shard. However, one worker can process any number of shards, so it's fine if the number of shards exceeds the number of instances."
    }
  },
  {
    "content": "A Company runs continuous integration/continuous delivery (CI/CD) pipelines for its application on AWS CodePipeline. A Developer must write unit tests and run them as part of the pipelines before staging the artifacts for testing.<><>How should the Developer incorporate unit tests as part of CI/CD pipelines?",
    "widget": "CI",
    "answers": [
      {
        "content": "Create a separate CodePipeline pipeline to run unit tests",
        "is_correct": false
      },
      {
        "content": "Update the AWS CodeBuild specification to include a phase for running unit tests",
        "is_correct": true
      },
      {
        "content": "Install the AWS CodeDeploy agent on an Amazon EC2 instance to run unit tests",
        "is_correct": false
      },
      {
        "content": "Create a testing branch in AWS CodeCommit to run unit tests",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "The recommended way to incorporate unit tests as part of CI/CD pipelines in AWS CodePipeline is to update the AWS CodeBuild specification to include a phase for running unit tests. The AWS CodeBuild specification allows developers to define build commands and output artifacts for their build projects. By including a phase to run unit tests in the build specification, developers can ensure that their code passes automated tests before it moves to the next stage of the pipeline. This approach ensures that only working code is deployed, reducing the risk of errors or issues in production."
    }
  },
  {
    "content": "A Developer has written an application that runs on Amazon EC2 instances and generates a value every minute. The Developer wants to monitor and graph the values generated over time without logging in to the instance each time.<><>Which approach should the Developer use to achieve this goal?",
    "widget": "CI",
    "answers": [
      {
        "content": "Use the Amazon CloudWatch metrics reported by default for all EC2 instances. View each value from the CloudWatch console.",
        "is_correct": false
      },
      {
        "content": "Develop the application to store each value in a file on Amazon S3 every minute with the timestamp as the name.",
        "is_correct": false
      },
      {
        "content": "Publish each generated value as a custom metric to Amazon CloudWatch using available AWS SDKs.",
        "is_correct": true
      },
      {
        "content": "Store each value as a variable and add the variable to the list of EC2 metrics that should be reported to the Amazon CloudWatch console.",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "Use the Amazon CloudWatch metrics reported by default for all EC2 instances. View each value from the CloudWatch console - by default EC2 instances don't send report to CloudWatch.<><>Develop the application to store each value in a file on Amazon S3 every minute with the timestamp as the name - S3 is not for monitoring values.<><>Store each value as a variable and add the variable to the list of EC2 metrics that should be reported to the Amazon CloudWatch console - EC2 instances don't send report to CloudWatch by default."
    }
  },
  {
    "content": "A developer is trying to get data from an Amazon DynamoDB table called demoman-table. The developer configured the AWS CLI to use a specific IAM user's credentials and executed the following command:`aws dynamodb get-item --table-name demoman-table --key {'id': {'N': '1993'}}`<><>The command returned errors and no rows were returned.<><>What is the MOST likely cause of these issues?",
    "widget": "CI",
    "answers": [
      {
        "content": "The command is incorrect; it should be rewritten to use put-item with a string argument.",
        "is_correct": false
      },
      {
        "content": "The developer needs to log a ticket with AWS Support to enable access to the demoman-table.",
        "is_correct": false
      },
      {
        "content": "Amazon DynamoDB cannot be accessed from the AWS CLI and needs to be called via the REST API.",
        "is_correct": false
      },
      {
        "content": "The IAM user needs an associated policy with read access to demoman-table.",
        "is_correct": true
      }
    ],
    "explanation": {
      "content": "That's a likely cause when the user executing the CLI command has no read access granted.<><>It has right syntax. it just need the read access permission."
    }
  },
  {
    "content": "A Development team is working on a case management solution that allows medical claims to be processed and reviewed. Users log in to provide information related to their medical and financial situations.<><>As part of the application, sensitive documents such as medical records, medical imaging, bank statements, and receipts are uploaded to Amazon S3. All documents must be securely transmitted and stored. All access to the documents must be recorded for auditing.<><>What is the MOST secure approach?",
    "widget": "CI",
    "answers": [
      {
        "content": "Use S3 default encryption using Advanced Encryption Standard-256 (AES-256) on the destination bucket.",
        "is_correct": false
      },
      {
        "content": "Use Amazon Cognito for authorization and authentication to ensure the security of the application and documents.",
        "is_correct": false
      },
      {
        "content": "Use AWS Lambda to encrypt and decrypt objects as they are placed into the S3 bucket.",
        "is_correct": false
      },
      {
        "content": "Use client-side encryption/decryption with Amazon S3 and AWS KMS.",
        "is_correct": true
      }
    ],
    "explanation": {
      "content": "Use client-side encryption/decryption with Amazon S3 and AWS KMS. Keywords: Auditing is required, AWS KMS has auditing capability. And has to be Client Side Encryption for securely transmitting.<><>Which technique is the MOST SECURE? Double encryption: at the client side AND at Amazon S3 using KMS."
    }
  },
  {
    "content": "A developer is planning to use an Amazon API Gateway and AWS Lambda to provide a REST API. The developer will have three distinct environments to manage: development, test, and production.<><>How should the application be deployed while minimizing the number of resources to manage?",
    "widget": "CI",
    "answers": [
      {
        "content": "Create a separate API Gateway and separate Lambda function for each environment in the same Region.",
        "is_correct": false
      },
      {
        "content": "Assign a Region for each environment and deploy API Gateway and Lambda to each Region.",
        "is_correct": false
      },
      {
        "content": "Create one API Gateway with multiple stages with one Lambda function with multiple aliases.",
        "is_correct": true
      },
      {
        "content": "Create one API Gateway and one Lambda function, and use a REST parameter to identify the environment.",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": ""
    }
  },
  {
    "content": "An application needs to use the IP address of the client in its processing. The application has been moved into AWS and has been placed behind an Application Load Balancer (ALB). However, all the client IP addresses now appear to be the same. The application must maintain the ability to scale horizontally.<><>Based on this scenario, what is the MOST cost-effective solution to this problem?",
    "widget": "CI",
    "answers": [
      {
        "content": "Remove the application from the ALB. Delete the ALB and change Amazon Route 53 to direct traffic to the instance running the application.",
        "is_correct": false
      },
      {
        "content": "Remove the application from the ALB. Create a Classic Load Balancer in its place. Direct traffic to the application using the HTTP protocol.",
        "is_correct": false
      },
      {
        "content": "Alter the application code to inspect the X-Forwarded-For header. Ensure that the code can work properly if a list of IP addresses is passed in the header.",
        "is_correct": true
      },
      {
        "content": "Alter the application code to inspect a custom header. Alter the client code to pass the IP address in the custom header.",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "Alter the application code to inspect the X-Forwarded-For header. Ensure that the code can work properly if a list of IP addresses is passed in the header.<><>The X-Forwarded-For request header helps you identify the IP address of a client when you use an HTTP or HTTPS load balancer. Because load balancers intercept traffic between clients and servers, your server access logs contain only the IP address of the load balancer. To see the IP address of the client, use the X-Forwarded-For request header. Elastic Load Balancing stores the IP address of the client in the X-Forwarded-For request header and passes the header to your server."
    }
  },
  {
    "content": "A developer tested an application locally and then deployed it to AWS Lambda. While testing the application remotely, the Lambda function fails with an access denied message.<><>How can this issue be addressed?",
    "widget": "CI",
    "answers": [
      {
        "content": "Update the Lambda function's execution role to include the missing permissions.",
        "is_correct": true
      },
      {
        "content": "Update the Lambda function's resource policy to include the missing permissions.",
        "is_correct": false
      },
      {
        "content": "Include an IAM policy document at the root of the deployment package and redeploy the Lambda function.",
        "is_correct": false
      },
      {
        "content": "Redeploy the Lambda function using an account with access to the AdministratorAccess policy.",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "The execution role which should contain the necessary permissions so lambda can perform."
    }
  },
  {
    "content": "A Developer must analyze performance issues with production-distributed applications written as AWS Lambda functions. These distributed Lambda applications invoke other components that make up the applications.<><>How should the Developer identify and troubleshoot the root cause of the performance issues in production?",
    "widget": "CI",
    "answers": [
      {
        "content": "Add logging statements to the Lambda functions, then use Amazon CloudWatch to view the logs.",
        "is_correct": false
      },
      {
        "content": "Use AWS CloudTrail and then examine the logs.",
        "is_correct": false
      },
      {
        "content": "Use AWS X-Ray, then examine the segments and errors.",
        "is_correct": true
      },
      {
        "content": "Run Amazon Inspector agents and then analyze performance.",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "X-Ray helps you analyze and debug distributed applications, such as those built using a microservices architecture. Using X-Ray, you can understand how your application and its underlying services are performing to identify and troubleshoot the root causes of performance issues and errors. It helps you debug and triage distributed applications wherever those applications are running, whether the architecture is serverless, containers, Amazon EC2, on-premises, or a mixture of all of these."
    }
  },
  {
    "content": "A company is building a compute-intensive application that will run on a fleet of Amazon EC2 instances. The application uses attached Amazon EBS disks for storing data. The application will process sensitive information and all the data must be encrypted.<><>What should a Developer do to ensure the data is encrypted on disk without impacting performance?",
    "widget": "CI",
    "answers": [
      {
        "content": "Configure the Amazon EC2 instance fleet to use encrypted EBS volumes for storing data.",
        "is_correct": true
      },
      {
        "content": "Add logic to write all data to an encrypted Amazon S3 bucket.",
        "is_correct": false
      },
      {
        "content": "Add a custom encryption algorithm to the application that will encrypt and decrypt all data.",
        "is_correct": false
      },
      {
        "content": "Create a new Amazon Machine Image (AMI) with an encrypted root volume and store the data to ephemeral disks.",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "Configure the Amazon EC2 instance fleet to use encrypted EBS volumes for storing data is the best option. This will ensure that all the data stored on the attached Amazon EBS disks is encrypted at rest. The encryption is performed transparently to the application and does not impact performance significantly. Creating an encrypted AMI with an encrypted root volume is an option, but it only encrypts the root volume, not the attached EBS disks. Writing data to an encrypted Amazon S3 bucket or adding a custom encryption algorithm to the application may also work, but they are not specific to the attached EBS disks and could potentially impact performance."
    }
  },
  {
    "content": "A Developer is working on a serverless project based in Java. Initial testing shows a cold start takes about 8 seconds on average for AWS Lambda functions.<><>What should the Developer do to reduce the cold start time? (Choose two.)",
    "widget": "CM",
    "answers": [
      {
        "content": "Add the Spring Framework to the project and enable dependency injection.",
        "is_correct": false
      },
      {
        "content": "Reduce the deployment package by including only needed modules from the AWS SDK for Java.",
        "is_correct": true
      },
      {
        "content": "Increase the memory allocation setting for the Lambda function.",
        "is_correct": true
      },
      {
        "content": "Increase the timeout setting for the Lambda function.",
        "is_correct": false
      },
      {
        "content": "Change the Lambda invocation mode from synchronous to asynchronous.",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "Add the Spring Framework to the project and enable dependency injection. - This does not reduce start up time (in fact it can aggravate the problem)<><>Reduce the deployment package by including only needed modules from the AWS SDK for Java. - This is true only if the question assumes that the Java code is using AWS SDK (not remarked).<><>Increase the memory allocation setting for the Lambda function. - This is true as increasing the memory will increase the assigned CPU and will reduce the start up time.<><>Increase the timeout setting for the Lambda function. - This can mitigate the problem but it doesn´t reduce the start up time as requested.<><>Change the Lambda invocation mode from synchronous to asynchronous. - This can mitigate the problem but it doesn´t reduce the start up as requested.<><><><>There are a number of factors that contribute to cold start times, including the languages used, number of dependencies, function chains, and virtual private clouds. To reduce the impact caused by Lambda cold starts, you can allocate more memory, use a basic HTTP client, and preload dependencies."
    }
  },
  {
    "content": "A company runs an e-commerce website that uses Amazon DynamoDB where pricing for items is dynamically updated in real time. At any given time, multiple updates may occur simultaneously for pricing information on a particular product. This is causing the original editor's changes to be overwritten without a proper review process.<><>Which DynamoDB write option should be selected to prevent this overwriting?",
    "widget": "CI",
    "answers": [
      {
        "content": "Concurrent writes",
        "is_correct": false
      },
      {
        "content": "Conditional writes",
        "is_correct": true
      },
      {
        "content": "Atomic writes",
        "is_correct": false
      },
      {
        "content": "Batch writes",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "DynamoDB optionally supports conditional writes for these operations. A conditional write succeeds only if the item attributes meet one or more expected conditions. Otherwise, it returns an error. Conditional writes are helpful in many situations. For example, you might want a PutItem operation to succeed only if there is not already an item with the same primary key. Or you could prevent an UpdateItem operation from modifying an item if one of its attributes has a certain value.<><>Conditional writes are helpful in cases where multiple users attempt to modify the same item."
    }
  },
  {
    "content": "A developer is storing JSON files in an Amazon S3 bucket. The developer wants to securely share an object with a specific group of people.<><>How can the developer securely provide temporary access to the objects that are stored in the S3 bucket?",
    "widget": "CI",
    "answers": [
      {
        "content": "Set object retention on the files. Use the AWS software development kit (SDK) to restore the object before subsequent requests. Provide the bucket's S3 URL.",
        "is_correct": false
      },
      {
        "content": "Use the AWS software development kit (SDK) to generate a presigned URL. Provide the presigned URL.",
        "is_correct": true
      },
      {
        "content": "Set a bucket policy that restricts access after a period of time. Provide the bucket's S3 URL.",
        "is_correct": false
      },
      {
        "content": "Configure static web hosting on the S3 bucket. Provide the bucket's web URL.",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "All objects by default are private. Only the object owner has permission to access these objects. However, the object owner can optionally share objects with others by creating a presigned URL, using their own security credentials, to grant time-limited permission to download the objects."
    }
  },
  {
    "content": "A front-end web application is using Amazon Cognito user pools to handle the user authentication flow. A developer is integrating Amazon DynamoDB into the application using the AWS SDK for JavaScript.<><>How would the developer securely call the API without exposing the access or secret keys?",
    "widget": "CI",
    "answers": [
      {
        "content": "Configure Amazon Cognito identity pools and exchange the JSON Web Token (JWT) for temporary credentials.",
        "is_correct": true
      },
      {
        "content": "Run the web application in an Amazon EC2 instance with the instance profile configured.",
        "is_correct": false
      },
      {
        "content": "Hardcore the credentials, use Amazon S3 to host the web application, and enable server-side encryption.",
        "is_correct": false
      },
      {
        "content": "Use Amazon Cognito user pool JSON Web Tokens (JWITs) to access the DynamoDB APIs.",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "The two main components of Amazon Cognito are user pools and identity pools. Identity pools provide AWS credentials to grant your users access to other AWS services. To enable users in your user pool to access AWS resources, you can configure an identity pool to exchange user pool tokens for AWS credentials. For more information see Accessing AWS Services Using an Identity Pool After Sign-in and Getting Started with Amazon Cognito Identity Pools (Federated Identities)."
    }
  },
  {
    "content": "A Developer must build an application that uses Amazon DynamoDB. The requirements state that the items being stored in the DynamoDB table will be 7KB in size and that reads must be strongly consistent. The maximum read rate is 3 items per second, and the maximum write rate is 10 items per second.<><>How should the Developer size the DynamoDB table to meet these requirements?",
    "widget": "CI",
    "answers": [
      {
        "content": "Read: 3 read capacity units Write: 70 write capacity units",
        "is_correct": false
      },
      {
        "content": "Read: 6 read capacity units Write: 70 write capacity units",
        "is_correct": true
      },
      {
        "content": "Read: 6 read capacity units Write: 10 write capacity units",
        "is_correct": false
      },
      {
        "content": "Read: 3 read capacity units Write: 10 write capacity units",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "We are talking about 3 strong reads of 7KB. 7KB requires 2 RCUs, so 3 x 2 = 6. Regarding the writes, we are talking about 10 itens per second. 7KB requires 7 WCUs. So 10 x 7 = 70."
    }
  },
  {
    "content": "A company needs to ingest terabytes of data each hour from thousands of sources that are delivered almost continually throughout the day. The volume of messages generated varies over the course of the day. Messages must be delivered in real time for fraud detection and live operational dashboards.<><>Which approach will meet these requirements?",
    "widget": "CI",
    "answers": [
      {
        "content": "Send the messages to an Amazon SQS queue, then process the messages by using a fleet of Amazon EC2 instances",
        "is_correct": false
      },
      {
        "content": "Use the Amazon S3 API to write messages to an S3 bucket, then process the messages by using Amazon Redshift",
        "is_correct": false
      },
      {
        "content": "Use AWS Data Pipeline to automate the movement and transformation of data",
        "is_correct": false
      },
      {
        "content": "Use Amazon Kinesis Data Streams with Kinesis Client Library to ingest and deliver messages",
        "is_correct": true
      }
    ],
    "explanation": {
      "content": "The key word is real-time.<><>Use Amazon Kinesis Data Streams for Real time data ingestion from multiple sources."
    }
  },
  {
    "content": "A developer is debugging an AWS Lambda function behind an Amazon API Gateway. Whenever the API Gateway endpoint is called, HTTP status code 200 is returned even though AWS Lambda is recording a 4xx error.<><>What change needs to be made to return a proper error code through the API Gateway?",
    "widget": "CI",
    "answers": [
      {
        "content": "Enable CORS in the API Gateway method settings.",
        "is_correct": false
      },
      {
        "content": "Use a Lambda proxy integration to return HTTP codes and headers.",
        "is_correct": true
      },
      {
        "content": "Enable API Gateway error pass-through.",
        "is_correct": false
      },
      {
        "content": "Return the value in the header x-Amzn-ErrorType.",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "With the Lambda proxy integration, Lambda is required to return an output of the following format ...<><>In this output, statusCode is typically 4XX for a client error and 5XX for a server error. API Gateway handles these errors by mapping the Lambda error to an HTTP error response, according to the specified statusCode. For API Gateway to pass the error type (for example, InvalidParameterException), as part of the response to the client, the Lambda function must include a header (for example, 'X-Amzn-ErrorType':'InvalidParameterException') in the headers property.<><>When using a Lambda proxy integration, the Lambda function can return an HTTP status code and headers directly to the API Gateway, which then passes it on to the client. This is in contrast to a standard integration, where the Lambda function returns only the body of the response, and the API Gateway itself handles the HTTP status code and headers.<><>Therefore, using a Lambda proxy integration can help resolve this issue and ensure that the appropriate HTTP status code is returned through the API Gateway."
    }
  },
  {
    "content": "For a deployment using AWS CodeDeploy, what is the run order of the hooks for in-place deployments?",
    "widget": "CI",
    "answers": [
      {
        "content": "Before Install -> Application Stop -> Application Start -> After Install",
        "is_correct": false
      },
      {
        "content": "Application Stop -> Before Install -> After Install -> Application Start",
        "is_correct": true
      },
      {
        "content": "Before Install -> Application Stop -> Validate Service -> Application Start",
        "is_correct": false
      },
      {
        "content": "Application Stop -> Before Install -> Validate Service -> Application Start",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "Run order of hooks in a deployment (without classic load balancer in deployment group)<><>Start -> Application Stop -> Download Bundle -> Before Install -> Install -> After Install -> Application Start -> Validate Service -> End"
    }
  },
  {
    "content": "A developer is using Amazon S3 as the event source that invokes a Lambda function when new objects are created in the bucket. The event source mapping information is stored in the bucket notification configuration. The developer is working with different versions of the Lambda function, and has a constant need to update notification configuration so that Amazon S3 invokes the correct version.<><>What is the MOST efficient and effective way to achieve mapping between the S3 event and Lambda?",
    "widget": "CI",
    "answers": [
      {
        "content": "Use a different Lambda trigger.",
        "is_correct": false
      },
      {
        "content": "Use Lambda environment variables.",
        "is_correct": false
      },
      {
        "content": "Use a Lambda alias.",
        "is_correct": true
      },
      {
        "content": "Use Lambda tags.",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "Each alias has a unique ARN. You can assign an alias to a specific version and link the S3 trigger to that alias. If you want to change the version of the Lambda triggered by S3, you just need to edit the alias. - A Lambda alias is like a pointer to a specific function version. Users can access the function version using the alias Amazon Resource Name (ARN)."
    }
  },
  {
    "content": "A company has a multi-tier application that uses Amazon API Gateway, AWS Lambda, and Amazon RDS. The company wants to investigate a slow response time to calls that come from the API Gateway API.<><>What is the MOST operationally efficient way for the company to determine which internal call is causing the slow response times?",
    "widget": "CI",
    "answers": [
      {
        "content": "Use Amazon CloudWatch.",
        "is_correct": false
      },
      {
        "content": "Use AWS X-Ray.",
        "is_correct": true
      },
      {
        "content": "Use AWS CloudTrail.",
        "is_correct": false
      },
      {
        "content": "Use VPC Flow Logs.",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "AWS X-Ray is a service that collects data about requests that your application serves, and provides tools that you can use to view, filter, and gain insights into that data to identify issues and opportunities for optimization. For any traced request to your application, you can see detailed information not only about the request and response, but also about calls that your application makes to downstream AWS resources, microservices, databases, and web APIs."
    }
  },
  {
    "content": "A developer is deploying an application that will store files in an Amazon S3 bucket. The files must be encrypted at rest. The developer wants to automatically replicate the files to an S3 bucket in a different AWS Region for disaster recovery.<><>How can the developer accomplish this task with the LEAST amount of configuration?",
    "widget": "CI",
    "answers": [
      {
        "content": "Encrypt the files by using server-side encryption with S3 managed encryption keys (SSE-S3). Enable S3 bucket replication.  ",
        "is_correct": true
      },
      {
        "content": "Encrypt the files by using server-side encryption (SSE) with an AWS Key Management Service (AWS KMS) customer master key (CMK). Enable S3 bucket replication.",
        "is_correct": false
      },
      {
        "content": "Use the s3 sync command to sync the files to the S3 bucket in the other Region.",
        "is_correct": false
      },
      {
        "content": "Configure an S3 Lifecycle configuration to automatically transfer files to the S3 bucket in the other Region.",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "The question is about LEAST amount of configuration. Creating a CMK would add a additional task.<><>By default, Amazon S3 doesn't replicate objects that are stored at rest using server-side encryption with customer managed keys stored in AWS KMS. Additional configuration is needed to direct Amazon S3 to replicate these objects."
    }
  },
  {
    "content": "A serverless application is using AWS Step Functions to process data and save it to a database. The application needs to validate some data with an external service before saving the data. The application will call the external service from an AWS Lambda function, and the external service will take a few hours to validate the data. The external service will respond to a webhook when the validation is complete.<><>A developer needs to pause the Step Functions workflow and wait for the response from the external service.<><>What should the developer do to meet this requirement?",
    "widget": "CI",
    "answers": [
      {
        "content": "Use the .wait ForTaskToken option in the Lambda function task state. Pass the token in the body.",
        "is_correct": true
      },
      {
        "content": "Use the .waitForTaskToken option in the Lambda function task state. Pass the invocation request.",
        "is_correct": false
      },
      {
        "content": "Call the Lambda function in synchronous mode. Wait for the external service to complete the processing.",
        "is_correct": false
      },
      {
        "content": "Call the Lambda function in asynchronous mode. Use the Wait state until the external service completes the processing.",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "[See docs](https://docs.aws.amazon.com/step-functions/latest/dg/connect-to-resource.html#connect-wait-example)<><>Call the Lambda function in synchronous mode. Wait for the external service to complete the processing - lambda will pass request to third party service and return to step. step will continue before that third party service finish.<><>Call the Lambda function in asynchronous mode. Use the Wait state until the external service completes the processing - step will send request to lambda in asynchronous and will wait. but it can not know for haw much time it will wait because processing time is unknown so we can not use Wait state."
    }
  },
  {
    "content": "A developer must use AWS X-Ray to monitor an application that is running on an Amazon EC2 instance. The developer has prepared the application by using the X-Ray SDK.<><>What should the developer do to perform the monitoring?",
    "widget": "CI",
    "answers": [
      {
        "content": "Configure the X-Ray SDK sampling rule and target. Activate the X-Ray daemon from the EC2 console or the AWS CLI with the modify-instance-attribute command to set the XRayEnabled flag.",
        "is_correct": false
      },
      {
        "content": "Install the X-Ray daemon. Assign an IAM role to the EC2 instance with a policy that allows writes to X-Ray.  ",
        "is_correct": true
      },
      {
        "content": "Install the X-Ray daemon. Configure it to forward data to Amazon EventBridge (Amazon CloudWatch Events). Grant the EC2 instance permission to write to Event Bridge (CloudWatch Events).",
        "is_correct": false
      },
      {
        "content": "Deploy the X-Ray SDK with the application, and instrument the application code. Use the SDK logger to capture and send the events.",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "To monitor an application running on an Amazon EC2 instance using AWS X-Ray, the following steps must be performed:<><>1. Install the X-Ray daemon on the EC2 instance.<><>2. Assign an IAM role to the EC2 instance that has a policy that allows writes to X-Ray. This will allow the X-Ray daemon to send tracing data to X-Ray.<><>3. Prepare the application by using the X-Ray SDK and instrumenting the application code.<><>Once these steps are completed, X-Ray will be able to collect and visualize the tracing data from the application. The X-Ray daemon will automatically collect tracing data from the application and send it to X-Ray for analysis.<><>Configuring the X-Ray SDK sampling rule and target is not necessary to perform the monitoring. Similarly, configuring the X-Ray daemon to forward data to Event Bridge (CloudWatch Events) is not necessary as it can directly send data to X-Ray."
    }
  },
  {
    "content": "A developer is designing a full-stack serverless application. Files for the website are stored in an Amazon S3 bucket. AWS Lambda functions that use Amazon API Gateway endpoints return results from an Amazon DynamoDB table. The developer must create a solution that securely provides registration and authentication for the application while minimizing the amount of configuration.<><>Which solution meets these requirements?",
    "widget": "CI",
    "answers": [
      {
        "content": "Create an Amazon Cognito user pool and an app client. Configure the app client to use the user pool and provide the hosted web UI provided for sign-up and sign-in.",
        "is_correct": true
      },
      {
        "content": "Configure an Amazon Cognito identity pool. Map the users with IAM roles that are configured to access the S3 bucket that stores the website.",
        "is_correct": false
      },
      {
        "content": "Configure and launch an Amazon EC2 instance to set up an identity provider with an Amazon Cognito user pool. Configure the user pool to provide the hosted web UI for sign-up and sign-in.",
        "is_correct": false
      },
      {
        "content": "Create an IAM policy that allows access to the website that is stored in the S3 bucket. Attach the policy to an IAM group. Add IAM users to the group.",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "Everything related to S3 and DynDb is already done, the question is tricky. The only requirement is signup and authentication, which is a Cognito User Pool side.<><>Cognito identity pool - Identity pool are used to request permissions to access aws resources not for login.<><>EC2 + Cognito user pool - I am using S3 with a serverless application and also only 1 EC2 instance is not enough to serve multiple users.<><>IAM permissions are more for internal aws accoun users not end users"
    }
  },
  {
    "content": "A developer needs to use the AWS CLI on an on-premises development server temporarily to access AWS services while performing maintenance. The developer needs to authenticate to AWS with their identity for several hours.<><>What is the MOST secure way to call AWS CLI commands with the developer's IAM identity?",
    "widget": "CI",
    "answers": [
      {
        "content": "Specify the developer's IAM access key ID and secret access key as parameters for each CLI command",
        "is_correct": false
      },
      {
        "content": "Run the aws configure CLI command. Provide the developer's IAM access key ID and secret access key.",
        "is_correct": false
      },
      {
        "content": "Specify the developer's IAM profile as a parameter for each CLI command.",
        "is_correct": false
      },
      {
        "content": "Run the get-session-token CLI command with the developer's IAM user. Use the returned credentials to call the CLI",
        "is_correct": true
      }
    ],
    "explanation": {
      "content": "get-session-token allows for temporary credentials with a set time duration.<><>Not IAM access key ID and secret access key doens't fit, because the requirement is for temporary access."
    }
  },
  {
    "content": "An AWS Lambda function accesses two Amazon DynamoDB tables. A developer wants to improve the performance of the Lambda function by identifying bottlenecks in the function.<><>How can the developer inspect the timing of the DynamoDB API calls?",
    "widget": "CI",
    "answers": [
      {
        "content": "Add DynamoDB as an event source to the Lambda function. View the performance with Amazon CloudWatch metrics",
        "is_correct": false
      },
      {
        "content": "Place an Application Load Balancer (ALB) in front of the two DynamoDB tables. Inspect the ALB logs",
        "is_correct": false
      },
      {
        "content": "Limit Lambda to no more than five concurrent invocations. Monitor from the Lambda console.",
        "is_correct": false
      },
      {
        "content": "Enable AWS X-Ray tracing for the function. View the traces from the X-Ray service.",
        "is_correct": true
      }
    ],
    "explanation": {
      "content": "Enable X-ray for Serverless application to view the map and find the traces for each segments.<><>You can use AWS X-Ray to visualize the components of your application, identify performance bottlenecks, and troubleshoot requests that resulted in an error."
    }
  },
  {
    "content": "A developer deployed an application to an Amazon EC2 instance. The application needs to know the public IPv4 address of the instance.<><>How can the application find this information?",
    "widget": "CI",
    "answers": [
      {
        "content": "Query the instance metadata from http://169.254.169.254/latest/meta-data/.",
        "is_correct": true
      },
      {
        "content": "Query the instance user data from http://169.254.169.254/latest/user-data/.",
        "is_correct": false
      },
      {
        "content": "Query the Amazon Machine Image (AMI) information from http://169.254 169.254/latest/meta-data/ami/.",
        "is_correct": false
      },
      {
        "content": "Check the hosts file of the operating system.",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": ""
    }
  },
  {
    "content": "A developer is designing an AWS Lambda function to perform a maintenance activity. The developer will use Amazon EventBridge (Amazon CloudWatch Events) to invoke the function on an hourly schedule. The developer wants the function to log information at different levels of detail according to the value of a log level variable. The developer must design the function so that the log level can be set without requiring a change to the function code.<><>Which solution will meet these requirements?",
    "widget": "CI",
    "answers": [
      {
        "content": "Add a custom log level parameter for the Lambda function. Set the parameter by using the Lambda console",
        "is_correct": false
      },
      {
        "content": "Set the log level in a Lambda environment variable",
        "is_correct": true
      },
      {
        "content": "Set the log level in the Amazon CloudWatch Logs console.",
        "is_correct": false
      },
      {
        "content": "Add a custom log level parameter for the Lambda function. Set the parameter by using the AWS CLI.",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "Each Lambda function has a LogLevel environment variable. To change the value, you can change only specific microservice Lambda function’s log level.<><>You can use environment variables to adjust your function's behavior without updating code. An environment variable is a pair of strings that is stored in a function's version-specific configuration. The Lambda runtime makes environment variables available to your code and sets additional environment variables that contain information about the function and invocation request."
    }
  },
  {
    "content": "A company is running an application on Amazon Elastic Container Service (Amazon ECS). When the company deploys a new version of the application, the company initially needs to expose 10% of live traffic to the new version. After a period of time, the company needs to immediately route all the remaining live traffic to the new version.<><>Which ECS deployment should the company use to meet these requirements?",
    "widget": "CI",
    "answers": [
      {
        "content": "Rolling update",
        "is_correct": false
      },
      {
        "content": "Blue/green with canary",
        "is_correct": true
      },
      {
        "content": "Blue/green with all at once",
        "is_correct": false
      },
      {
        "content": "Blue/green with linear",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "[See docs](https://d1.awsstatic.com/whitepapers/AWS_Blue_Green_Deployments.pdf)<><>• Canary – Traffic is shifted in two increments.<><>• Linear – Traffic is shifted in equal increments.<><>• All-at-once – All traffic is shifted to the updated tasks"
    }
  },
  {
    "content": "A microservices application is deployed across multiple containers in Amazon Elastic Container Service (Amazon ECS). To improve performance, a developer wants to capture trace information between the microservices and visualize the microservices architecture.<><>Which solution will meet these requirements?",
    "widget": "CI",
    "answers": [
      {
        "content": "Build the container from the amazon/aws-xray-daemon base image. Use the AWS X-Ray SDK to instrument the application.",
        "is_correct": true
      },
      {
        "content": "Install the Amazon CloudWatch agent on the container image. Use the CloudWatch SDK to publish custom metrics from each of the microservices.",
        "is_correct": false
      },
      {
        "content": "Install the AWS X-Ray daemon on each of the ECS instances.",
        "is_correct": false
      },
      {
        "content": "Configure AWS CloudTrail data events to capture the traffic between the microservices.",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "[See docs](https://docs.aws.amazon.com/xray/latest/devguide/xray-daemon-ecs.html#xray-daemon-ecs-build)"
    }
  },
  {
    "content": "A company is planning to use AWS CodeDeploy to deploy an application to Amazon Elastic Container Service (Amazon ECS). During the deployment of a new version of the application, the company initially must expose only 10% of live traffic to the new version of the deployed application. Then, after 15 minutes elapse, the company must route all the remaining live traffic to the new version of the deployed application.<><>Which CodeDeploy predefined configuration will meet these requirements?",
    "widget": "CI",
    "answers": [
      {
        "content": "CodeDeployDefault.ECSCanary10Percent15Minutes",
        "is_correct": true
      },
      {
        "content": "CodeDeployDefault.LambdaCanary10Percent5Minutes",
        "is_correct": false
      },
      {
        "content": "CodeDeployDefault.LambdaCanary10Percent15Minutes",
        "is_correct": false
      },
      {
        "content": "CodeDeployDefault.ECSLinear10PercentEvery1 Minutes",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "[See docs](https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-configurations.html)"
    }
  },
  {
    "content": "A developer notices timeouts from the AWS CLI when the developer runs list commands.<><>What should the developer do to avoid these timeouts?",
    "widget": "CI",
    "answers": [
      {
        "content": "Use the --page-size parameter to request a smaller number of items.",
        "is_correct": true
      },
      {
        "content": "Use shorthand syntax to separate the list by a single space.",
        "is_correct": false
      },
      {
        "content": "Use the yaml-stream output for faster viewing of large datasets.",
        "is_correct": false
      },
      {
        "content": "Use quotation marks around strings to enclose data structure.",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "[See docs](https://docs.aws.amazon.com/cli/latest/userguide/cli-usage-pagination.html)"
    }
  },
  {
    "content": "A developer deploys an AWS Lambda function that runs each time a new Amazon S3 bucket is created. The Lambda function is supposed to attach an S3 Lifecycle policy to each new S3 bucket. The developer discovers that newly created S3 buckets have no S3 Lifecycle policy attached.<><>Which AWS service should the developer use to find a possible error in the Lambda function?",
    "widget": "CI",
    "answers": [
      {
        "content": "AWS CloudTrail",
        "is_correct": false
      },
      {
        "content": "Amazon S3",
        "is_correct": false
      },
      {
        "content": "AWS CloudFormation",
        "is_correct": false
      },
      {
        "content": "Amazon CloudWatch",
        "is_correct": true
      }
    ],
    "explanation": {
      "content": "CloudTrail, you can get a history of AWS API calls for your account, including API calls made via the AWS Management Console, AWS SDKs, command line tools, and higher-level AWS services (such as AWS CloudFormation).<><>So CloudWatch it is."
    }
  },
  {
    "content": "A developer has created a web API that uses Amazon Elastic Container Service (Amazon ECS) and an Application Load Balancer (ALB). An Amazon CloudFront distribution uses the API as an origin for web clients. The application has received millions of requests with a JSON Web Token (JWT) that is not valid in the authorization header. The developer has scaled out the application to handle the unauthenticated requests.<><>What should the developer do to reduce the number of unauthenticated requests to the API?",
    "widget": "CI",
    "answers": [
      {
        "content": "Add a request routing rule to the ALB to return a 401 status code if the authorization header is missing.",
        "is_correct": false
      },
      {
        "content": "Add a container to the ECS task definition to validate JWTs Set the new container as a dependency of the application container.",
        "is_correct": false
      },
      {
        "content": "Create a CloudFront function for the distribution Use the crypto module in the function to validate the JWT. Most Voted",
        "is_correct": true
      },
      {
        "content": "Add a custom authorizer for AWS Lambda to the CloudFront distribution to validate the JWT.",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "The following example function validates a JSON web token (JWT) in the query string of a request. If the token is valid, the function returns the original, unmodified request to CloudFront. If the token is not valid, the function generates an error response. This function uses the crypto module.<><>This function assumes that requests contain a JWT value in a query string parameter named jwt. Also, for this function to work, you must configure CloudFront to cache based on the jwt query string parameter.<><>[See docs](https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/example-function-validate-token.html)."
    }
  },
  {
    "content": "A developer is creating a mobile application that will not require users to log in.<><>What is the MOST efficient method to grant users access to AWS resources?",
    "widget": "CI",
    "answers": [
      {
        "content": "Use an identity provider to securely authenticate with the application.",
        "is_correct": false
      },
      {
        "content": "Create an AWS Lambda function to create an IAM user when a user accesses the application.",
        "is_correct": false
      },
      {
        "content": "Create credentials using AWS KMS and apply these credentials to users when using the application.",
        "is_correct": false
      },
      {
        "content": "Use Amazon Cognito to associate unauthenticated users with an IAM role that has limited access to resources.",
        "is_correct": true
      }
    ],
    "explanation": {
      "content": "Keyword is 'mobile'.<><>This is Cognito Identity pools functionality aka granting users access to limited list of AWS resources."
    }
  },
  {
    "content": "A developer has created on AWS Lambda function tool uses 15 MB of memory. When the developer runs the code natively on a laptop that has 4 cores, the function runs within 100 ms. When the developer deploys the code as a Lambda function with 128 MB of memory, the first run takes 3 seconds. Subsequent runs take more than 500 ms to finish.<><>The developer needs to improve the performance of the Lambda function so that the function runs consistently in less than 100 ms, excluding the initial startup time.<><>Which solution will meet this requirement?",
    "widget": "CI",
    "answers": [
      {
        "content": "Increase the reserved concurrency of the Lambda function.",
        "is_correct": false
      },
      {
        "content": "Increase the provisioned concurrency of the Lambda function.",
        "is_correct": false
      },
      {
        "content": "Increase the memory of the Lambda function.",
        "is_correct": true
      },
      {
        "content": "Repackage the Lambda function as a container. Redeploy the function.",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "It mentions locally using 4 cores, and by increasing memory of a lambda, you can increase CPU. Plus provisioned concurrency would only help on a cold start, and it's still taking too long on subsequent runs.<><>Increasing memory on the lambda will help with performance, because it's like increasing the CPU of the function. Adding provisioned concurrency will help eliminate the init time, but the question says to ignore that."
    }
  },
  {
    "content": "A company is planning to use AWS CodeDeploy to deploy an application to AWS Lambda. During the deployment of a new version of the application, the company initially must expose only 10% of live traffic to the new version of the deployed application. Then, every 10 minutes, the company must route another 10% of live traffic to the new version of the deployed application until all live traffic is routed to the new version.<><>Which CodeDeploy predefined configuration will meet these requirements?",
    "widget": "CI",
    "answers": [
      {
        "content": "CodeDeployDefault.OnceAtATime",
        "is_correct": false
      },
      {
        "content": "CodeDeployDefault.LambdaCanary10Percent10Minutes",
        "is_correct": false
      },
      {
        "content": "CodeDeployDefault.LambdaLinear10PercentEvery10Minutes",
        "is_correct": true
      },
      {
        "content": "CodeDeployDefault.ECSLinear10PercentEvery3Minutes",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "CodeDeployDefault.LambdaLinear10PercentEvery10Minutes, because Linear deployment is increase X% every 10 minutes.<><>[See docs](https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-configurations.html)"
    }
  },
  {
    "content": "A developer wants to use AWS Elastic Beanstalk to test a new version of on application in a test environment.<><>Which deployment method offers the FASTEST deployment?",
    "widget": "CI",
    "answers": [
      {
        "content": "Immutable",
        "is_correct": false
      },
      {
        "content": "Rolling",
        "is_correct": false
      },
      {
        "content": "Rolling with additional batch",
        "is_correct": false
      },
      {
        "content": "All at once",
        "is_correct": true
      }
    ],
    "explanation": {
      "content": "All at once deployment method updates all instances at the same time, and as a result, the deployment process is faster than other methods such as Immutable, Rolling, and Rolling with additional batch, which can take longer due to the fact that they update instances in batches or gradually.<><>[See docs](https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html#deployments-scenarios)"
    }
  },
  {
    "content": "A developer has built an application that inserts data into an Amazon DynamoDB table. The table is configured to use provisioned capacity. The application is deployed on a burstable nano Amazon EC2 Instance. The application logs show that the application has been failing because of a ProvisionedThroughputExceedException error.<><>Which actions should the developer take to resolve this issue? (Choose two.)",
    "widget": "CM",
    "answers": [
      {
        "content": "Move The application to a larger EC instance.",
        "is_correct": false
      },
      {
        "content": "Increase the number or read capacity units (RCUs) that are provisioned for the DynamoDB table.",
        "is_correct": false
      },
      {
        "content": "Reduce the frequency of requests to DynamoDB by implement ng exponential backoff.",
        "is_correct": true
      },
      {
        "content": "Increase the frequency of requests to DynamoDB by decreasing the retry delay.",
        "is_correct": false
      },
      {
        "content": "Change the capacity mode of the DynamoDB table from provisioned to on-demand.",
        "is_correct": true
      }
    ],
    "explanation": {
      "content": "Increase the number or read capacity units (RCUs) - increases RCU, but the application inserts data, so WCU are needed, => wrong.<><>Increase the frequency of requests to DynamoDB by decreasing the retry delay - you application will make more requests per minute, the problem with lack of resources will not disappear, application will have more errors in logs. => wrong<><>Exponential algorithm will lead to fewer requests per minute => application will start working properly. => correct<><>Capacity on demand means that your database will get more required WCU automatically. => correct"
    }
  },
  {
    "content": "A developer is deploying on application on Amazon EC2 instances that run in Account A. In certain cases, this application needs to read data from a private Amazon S3 bucket in Account B. The developer must provide the application access to the S3 bucket without exposing the S3 bucket to anyone else.<><>Which combination of actions should the developer take to meet these requirements? (Choose two.)",
    "widget": "CM",
    "answers": [
      {
        "content": "Create an IAM role with S3 read permissions in Account B.",
        "is_correct": false
      },
      {
        "content": "Update the instance profile IAM role in Account A with S3 read permissions.",
        "is_correct": true
      },
      {
        "content": "Make the S3 bucket public with limited access for Account A.",
        "is_correct": false
      },
      {
        "content": "Configure the bucket policy in Account B to grant permissions to the instance profile role.",
        "is_correct": true
      },
      {
        "content": "Add a trust policy that allows s3:Get* permissions to the IAM rote in Account B.",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "[See docs](https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_evaluation-logic-cross-account.html)."
    }
  },
  {
    "content": "A developer at a company recently created a serverless application to process and show data from business reports. The application's user interface (UI) allows users to select and start processing the flies. The UI displays a message when the result is available to view. The application uses AWS Step Functions with AWS Lambda functions to process the files. The developer used Amazon API Gateway and Lambda functions to create an API to support the UI.<><>The company's UI team reports that the request to process a file s often returning timeout errors because of the size or complexity of the files. The UI team wants the API to provide an immediate response so that the UI can display a message while the files are being processed. The backend process that is invoked by the API needs to send an email message when the report processing is complete.<><>What should the developer do to configure the API to meet these requirements?",
    "widget": "CI",
    "answers": [
      {
        "content": "Change the API Gateway route to add an X-Amz-Invocation-Type header with a static value of 'Event' in the integration request. Deploy the API Gateway stage to apply the changes.",
        "is_correct": true
      },
      {
        "content": "Change the configuration of the Lambda function that implements the request to process a file. Configure the maximum age of the event so that the Lambda function will run asynchronously.",
        "is_correct": false
      },
      {
        "content": "Change the API Gateway timeout value to match the Lambda function timeout value. Deploy the API Gateway stage to apply the changes.",
        "is_correct": false
      },
      {
        "content": "Change the API Gateway route to add an X-Amz-Target header with a static value of 'Async' in the integration request. Deploy the API Gateway stage to apply the changes.",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "[See docs](https://docs.aws.amazon.com/apigateway/latest/developerguide/set-up-lambda-integration-async.html)."
    }
  },
  {
    "content": "An ecommerce application is running behind an Application Load Balancer. A developer observes some unexpected load on the application during non-peak hours. The developer wants to analyze patterns for the client IP addresses that use the application.<><>Which HTTP header should the developer use for this analysis?",
    "widget": "CI",
    "answers": [
      {
        "content": "The X-Forwarded-Proto header",
        "is_correct": false
      },
      {
        "content": "The X-Forwarded-Host header",
        "is_correct": false
      },
      {
        "content": "The X-Forwarded-For header",
        "is_correct": true
      },
      {
        "content": "The X-Forwarded-Port header",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "[See docs](https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/x-forwarded-headers.html#x-forwarded-for)."
    }
  },
  {
    "content": "A developer needs to create an application that supports Security Assertion Markup Language (SAML) and authentication with social media providers. It must also allow access to AWS services, such as Amazon DynamoDB.<><>Which AWS service or feature will meet these requirements with the LEAST amount of additional coding?",
    "widget": "CI",
    "answers": [
      {
        "content": "AWS AppSync",
        "is_correct": false
      },
      {
        "content": "Amazon Cognito identity pools",
        "is_correct": true
      },
      {
        "content": "Amazon Cognito user pools",
        "is_correct": false
      },
      {
        "content": "Amazon Lambda@Edge",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "access to resources = identity pools.<><>Amazon Cognito supports authentication with identity providers (IdPs) through Security Assertion Markup Language 2.0 (SAML 2.0). You can use an IdP that supports SAML with Amazon Cognito to provide a simple onboarding flow for your users. Your SAML-supporting IdP specifies the IAM roles that your users can assume. This way, different users can receive different sets of permissions."
    }
  },
  {
    "content": "A developer is designing a serverless application for an ecommerce website. An Amazon API Gateway API exposes AWS Lambda functions for billing, payment, and user operations. The website features shopping carts for the users. The shopping carts must be stored for extended periods of time and will be retrieved frequently by the front-end application.<><>The load on the application will vary significantly based on the time of day and the promotional sales that are offered on the website. The application must be able to scale automatically to meet these changing demands.<><>Which solution will meet these requirements?",
    "widget": "CI",
    "answers": [
      {
        "content": "Store the data objects on an Amazon RDS DB instance. Cache the data objects in memory by using Amazon ElastiCache.",
        "is_correct": false
      },
      {
        "content": "Store the data objects on Amazon EC2 instances behind an Application Load Balancer. Use session affinity (sticky sessions) for each user's shopping cart.",
        "is_correct": false
      },
      {
        "content": "Store the data objects in Amazon S3 buckets. Cache the data objects by using Amazon CloudFront with the maximum TTL.",
        "is_correct": false
      },
      {
        "content": "Store the data objects in Amazon DynamoDB tables. Cache the data objects by using DynamoDB Accelerator (DAX).",
        "is_correct": true
      }
    ],
    "explanation": {
      "content": "scale automatically to meet these changing demands. = means DynamoDB<><>Amazon DynamoDB Accelerator (DAX) is a fully managed, highly available, in-memory cache for Amazon DynamoDB that delivers up to a 10 times performance improvement—from milliseconds to microseconds—even at millions of requests per second."
    }
  },
  {
    "content": "A company is migrating its on-premises database to Amazon RDS for MySQL. The company has read-heavy workloads, and wants to make sure it re-factors its code to achieve optimum read performance for its queries.<><>How can this objective be met?",
    "widget": "CI",
    "answers": [
      {
        "content": "Add database retries to effectively use RDS with vertical scaling.",
        "is_correct": false
      },
      {
        "content": "Use RDS with multi-AZ deployment.",
        "is_correct": false
      },
      {
        "content": "Add a connection string to use an RDS read replica for read queries.",
        "is_correct": true
      },
      {
        "content": "Add a connection string to use a read replica on an EC2 instance.",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "keyword: re-factors its code<><>Amazon RDS Read Replicas provide enhanced performance and durability for RDS database (DB) instances. They make it easy to elastically scale out beyond the capacity constraints of a single DB instance for read-heavy database workloads."
    }
  },
  {
    "content": "An application running on Amazon EC2 opens connections to an Amazon RDS SQL Server database. The developer does not want to store the user name and password for the database in the code. The developer would also like to automatically rotate the credentials.<><>What is the MOST secure way to store and access the database credentials?",
    "widget": "CI",
    "answers": [
      {
        "content": "Create an IAM role that has permissions to access the database. Attach the role to the EC2 instance.",
        "is_correct": false
      },
      {
        "content": "Use AWS Secrets Manager to store the credentials. Retrieve the credentials from Secrets Manager as needed.",
        "is_correct": true
      },
      {
        "content": "Store the credentials in an encrypted text file in an Amazon S3 bucket. Configure the EC2 instance's user data to download the credentials from Amazon S3 as the instance boots.",
        "is_correct": false
      },
      {
        "content": "Store the user name and password credentials directly in the source code. No further action is needed because the source code is stored in a private repository.",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "Store DB credentials at secrets manager.<><>IAM Role is not correct because SQL Server does not support IAM credentials. The question asks specifically about storing username and password and rotation, so Secret manager is the way to go."
    }
  },
  {
    "content": "A developer received the following error message during an AWS CloudFormation deployment:<><>DELETE_FAILED (The following resource(s) failed to delete: [ASGInstanceRolel2345678].)<><>Which action should the developer take to resolve this error?",
    "widget": "CI",
    "answers": [
      {
        "content": "Contact AWS Support to report an issue with the Auto Scaling Groups (ASG) service.",
        "is_correct": false
      },
      {
        "content": "Add a DependsOn attribute to the ASGInstanceRole12345678 resource in the CloudFormation template. Then delete the stack.",
        "is_correct": false
      },
      {
        "content": "Modify the CloudFormation template to retain the ASGInstanceRolel2345678 resource. Then manually delete the resource after deployment.",
        "is_correct": true
      },
      {
        "content": "Add a force parameter when calling CloudFormation with the role-arn of ASGInstanceRolel2345678.",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "It is not mentioned anywhere that deletion failed due to dependency on something. For example CloudFormation cannot delete S3 buckets or ECR repositories if they are not empty. In this case only manual deletion helps."
    }
  },
  {
    "content": "An application runs on multiple EC2 instances behind an ELB.<><>Where is the session data best written so that it can be served reliably across multiple requests?",
    "widget": "CI",
    "answers": [
      {
        "content": "Write data to Amazon ElastiCache.",
        "is_correct": true
      },
      {
        "content": "Write data to Amazon Elastic Block Store.",
        "is_correct": false
      },
      {
        "content": "Write data to Amazon EC2 Instance Store.",
        "is_correct": false
      },
      {
        "content": "Write data to the root filesystem.",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "For session data always remember elastic cache or dynamodb is used.<><>Instance store is ideal for temporary storage of information that changes frequently, such as buffers, caches, scratch data, and other temporary content, or for data that is replicated across a fleet of instances, such as a load-balanced pool of web servers."
    }
  },
  {
    "content": "A company is using continuous integration/continuous delivery (CI/CD) systems. A developer must automate the deployment of an application software package to Amazon EC2 instances and virtual servers that run on premises.<><>Which AWS service should the developer use to meet these requirements?",
    "widget": "CI",
    "answers": [
      {
        "content": "AWS Cloud9",
        "is_correct": false
      },
      {
        "content": "AWS CodeBuild",
        "is_correct": false
      },
      {
        "content": "AWS Elastic Beanstalk",
        "is_correct": false
      },
      {
        "content": "AWS CodeDeploy",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": ""
    }
  },
  {
    "content": "A software company is using AWS CodeBuild to build an application. The buildspec runs the application build and creates a Docker image that contains the application. The company needs to push the Docker image to Amazon Elastic Container Registry (Amazon ECR) only upon the completion of each successful build.<><>Which solution meets these requirements?",
    "widget": "CI",
    "answers": [
      {
        "content": "Change the buildspec by adding a post_build phase that uses the commands block to push the Docker image.",
        "is_correct": true
      },
      {
        "content": "Change the buildspec by adding a post_build phase that uses the finally block to push the Docker image.",
        "is_correct": false
      },
      {
        "content": "Specify the Docker image in the buildspec's artifacts sequence with an action to push the image.",
        "is_correct": false
      },
      {
        "content": "Use a batch build to define a build matrix. Use the batch build to push the Docker image.",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "[See docs](https://docs.aws.amazon.com/codebuild/latest/userguide/sample-docker.html#sample-docker-files)."
    }
  },
  {
    "content": "A company is using Amazon RDS as the backend database for its application. After a recent marketing campaign, a surge of read requests to the database increased the latency of data retrieval from the database.<><>The company has decided to implement a caching layer in front of the database. The cached content must be encrypted and must be highly available.<><>Which solution will meet these requirements?",
    "widget": "CI",
    "answers": [
      {
        "content": "Amazon CloudFront",
        "is_correct": false
      },
      {
        "content": "Amazon ElastiCache for Memcached",
        "is_correct": false
      },
      {
        "content": "Amazon ElastiCache for Redis in cluster mode",
        "is_correct": true
      },
      {
        "content": "Amazon DynamoDB Accelerator (DAX)",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "Redis with Cluster Mode. Memcached is designed for simplicity. Whereas as Redis with Cluster Mode will provide high availability."
    }
  },
  {
    "content": "A company is developing a report implemented using AWS Step Functions. Amazon CloudWatch shows errors in the Step Functions task state machine. To troubleshoot each task, the state input needs to be included along with the error message in the state output.<><>Which coding practice can preserve both the original input and the error for the state?",
    "widget": "CI",
    "answers": [
      {
        "content": "Use ResultPath in a Catch statement to include the error with the original input.",
        "is_correct": true
      },
      {
        "content": "Use InputPath in a Catch statement and set the value to null.",
        "is_correct": false
      },
      {
        "content": "Use ErrorEquals in a Retry statement to include the error with the original input.",
        "is_correct": false
      },
      {
        "content": "Use OutputPath in a Retry statement and set the value to $.",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "In some cases, you might want to preserve the original input with the error. Use ResultPath in a Catch to include the error with the original input, instead of replacing it.<><>The output of a state can be a copy of its input, the result it produces (for example, output from a Task state’s Lambda function), or a combination of its input and result. Use ResultPath to control which combination of these is passed to the state output."
    }
  },
  {
    "content": "A developer is receiving HTTP 400: ThrottlingException errors intermittently when calling the Amazon CloudWatch API. When a call fails, no data is retrieved.<><>What best practice should first be applied to address this issue?",
    "widget": "CI",
    "answers": [
      {
        "content": "Contact AWS Support for a limit increase.",
        "is_correct": false
      },
      {
        "content": "Use the AWS CLI to get the metrics.",
        "is_correct": false
      },
      {
        "content": "Analyze the applications and remove the API call.",
        "is_correct": false
      },
      {
        "content": "Retry the call with exponential backoff.",
        "is_correct": true
      }
    ],
    "explanation": {
      "content": "It's a best practice to use the following methods to reduce your call rate and avoid API throttling:<><>Distribute your API calls evenly over time rather than making several API calls in a short time span. If you require data to be available with a one-minute resolution, you have an entire minute to emit that metric. Use jitter (randomized delay) to send data points at various times.<><>Combine as many metrics as possible into a single API call. For example, a single PutMetricData call can include 20 metrics and 150 data points. You can also use pre-aggregated data sets, such as StatisticSet, to publish aggregated data points, thus reducing the number of PutMetricData calls per second.<><>Retry your call with exponential backoff and jitter."
    }
  },
  {
    "content": "A company has an online order website that uses Amazon DynamoDB to store item inventory. A sample of the inventory object is as follows:<><>`{'Id': {'N': '456'}, 'Price': {'N': '650'}, 'ProductCategory': {'S': 'Sporting Goods'}}`<><>A developer needs to reduce all inventory prices by 100 as long as the resulting price would not be less than 500.<><>What should the developer do to make this change with the LEAST number of calls to DynamoDB?",
    "widget": "CI",
    "answers": [
      {
        "content": "Perform a DynamoDB Query operation with the Id. If the price is >= 600, perform an UpdateItem operation to update the price.",
        "is_correct": false
      },
      {
        "content": "Perform a DynamoDB UpdateItem operation with a condition expression of 'Price >= 600'.",
        "is_correct": true
      },
      {
        "content": "Perform a DynamoDB UpdateItem operation with a condition expression of 'ProductCategory IN ({'S': 'Sporting Goods'}) and Price 600'.",
        "is_correct": false
      },
      {
        "content": "Perform a DynamoDB UpdateItem operation with a condition expression of 'MIN Price = 500'.",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "aws dynamodb update-item --table-name ProductCatalog --key `{'Id': {'N': '456'}}` --update-expression 'SET Price = Price - :discount' --condition-expression 'Price > :limit' --expression-attribute-values file://values.json"
    }
  },
  {
    "content": "A company is using an AWS Lambda function to process records from an Amazon Kinesis data stream. The company recently observed slow processing of the records. A developer notices that the iterator age metric for the function is increasing and that the Lambda run duration is constantly above normal.<><>Which actions should the developer take to increase the processing speed? (Choose two.)",
    "widget": "CI",
    "answers": [
      {
        "content": "Increase the number of shards of the Kinesis data stream.",
        "is_correct": true
      },
      {
        "content": "Decrease the timeout of the Lambda function.",
        "is_correct": false
      },
      {
        "content": "Increase the memory that is allocated to the Lambda function.",
        "is_correct": true
      },
      {
        "content": "Decrease the number of shards of the Kinesis data stream.",
        "is_correct": false
      },
      {
        "content": "Increase the timeout of the Lambda function.",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "1.Decrease your function's runtime duration is achievable by: Increase the amount of memory allocated to the function.<><>2. Optimize your function code: 2.Increase your stream's shard count<><>[See docs](https://aws.amazon.com/premiumsupport/knowledge-center/lambda-iterator-age/)."
    }
  },
  {
    "content": "A developer is making changes to a custom application that uses AWS Elastic Beanstalk.<><>Which solutions will update the Elastic Beanstalk environment with the new application version after the developer completes the changes? (Choose two.)",
    "widget": "CM",
    "answers": [
      {
        "content": "Package the application code into a .zip file. Use the AWS Management Console to upload the zip file and deploy the packaged application.",
        "is_correct": true
      },
      {
        "content": "Package the application code into a .tar file. Use the AWS Management Console to create a new application version from the .tar file. Update the environment by using the AWS CLI.",
        "is_correct": false
      },
      {
        "content": "Package the application code into a .tar file. Use the AWS Management Console to upload the .tar file and deploy the packaged application.",
        "is_correct": false
      },
      {
        "content": "Package the application code into a .zip file. Use the AWS CLI to create a new application version from the .zip file and to update the environment.",
        "is_correct": true
      },
      {
        "content": "Package the application code into a .zip file. Use the AWS Management Console to create a new application version from the .zip file. Rebuild the environment by using the AWS CLI.",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "Elastic beanstalk won't take .tar files and E seems strange to use AWS Management console and then CLI to rebuild.<><>When you go to your EB console and any application ; rightaway on the console you can see a BIG option to upload and deploy ;thats where I lean upload zip and deploy. You can do the same with AWS EB CLI, by creating application new version and updating environment fot the target app.<><>[See docs](https://docs.aws.amazon.com/cli/latest/reference/elasticbeanstalk/update-environment.html)."
    }
  },
  {
    "content": "A company has an application where reading objects from Amazon S3 is based on the type of user. The user types are registered user and guest user. The company has 25,000 users and is growing. Information is pulled from an S3 bucket depending on the user type.<><>Which approaches are recommended to provide access to both user types? (Choose two.)",
    "widget": "CM",
    "answers": [
      {
        "content": "Provide a different access key and secret access key in the application code for registered users and guest users to provide read access to the objects.",
        "is_correct": false
      },
      {
        "content": "Use S3 bucket policies to restrict read access to specific IAM users.",
        "is_correct": false
      },
      {
        "content": "Use Amazon Cognito to provide access using authenticated and unauthenticated roles.",
        "is_correct": true
      },
      {
        "content": "Create a new IAM user for each user and grant read access.",
        "is_correct": false
      },
      {
        "content": "Use the AWS IAM service and let the application assume the different roles using the AWS Security Token Service (AWS STS) AssumeRole action depending on the type of user and provide read access to Amazon S3 using the assumed role.",
        "is_correct": true
      }
    ],
    "explanation": {
      "content": "S3 bucket policies - would be possible if you had a few users and not expecting to get more, so not a real world scenario<><>Cognito - Cognito can handle authorized and unauthorized access so C is correct<><>IAM user - Why would you create so many users and assign them permissions when you just need to distinguish between two roles<><>STS AssumeRole - Perfectly fine for the scenario to create Authorized and Unauthorized role, let the app Assume the role and call the services (if the role has the necessary permissions then the call would be successful)."
    }
  },
  {
    "content": "A developer is writing an application to analyze the traffic to a fleet of Amazon EC2 instances. The EC2 instances run behind a public Application Load Balancer (ALB). An HTTP server runs on each of the EC2 instances, logging all requests to a log file.<><>The developer wants to capture the client public IP addresses. The developer analyzes the log files and notices only the IP address of the ALB.<><>What must the developer do to capture the client public IP addresses in the log file?",
    "widget": "CI",
    "answers": [
      {
        "content": "Add a Host header to the HTTP server log configuration file.",
        "is_correct": false
      },
      {
        "content": "Install the Amazon CloudWatch Logs agent on each EC2 instance. Configure the agent to write to the log file.",
        "is_correct": false
      },
      {
        "content": "Install the AWS X-Ray daemon on each EC2 instance. Configure the daemon to write to the log file.",
        "is_correct": false
      },
      {
        "content": "Add an X-Forwarded-For header to the HTTP server log configuration file.",
        "is_correct": true
      }
    ],
    "explanation": {
      "content": "The X-Forwarded-For request header helps you identify the IP address of a client when you use an HTTP or HTTPS load balancer. Because load balancers intercept traffic between clients and servers, your server access logs only contain the IP address of the load balancer."
    }
  },
  {
    "content": "A developer is writing a new AWS Serverless Application Model (AWS SAM) template with a new AWS Lambda function. The Lambda function runs complex code. The developer wants to test the Lambda function with more CPU power.<><>What should the developer do to meet this requirement?",
    "widget": "CI",
    "answers": [
      {
        "content": "Increase the runtime engine version.",
        "is_correct": false
      },
      {
        "content": "Increase the timeout.",
        "is_correct": false
      },
      {
        "content": "Increase the number of Lambda layers.",
        "is_correct": false
      },
      {
        "content": "Increase the memory.",
        "is_correct": true
      }
    ],
    "explanation": {
      "content": "With allocating more memory, more CPU will be available."
    }
  },
  {
    "content": "A developer is creating a serverless web application and maintains different branches of code. The developer wants to avoid updating the Amazon API Gateway target endpoint each time a new code push is performed.<><>What solution would allow the developer to perform a code push efficiently, without the need to update the API Gateway?",
    "widget": "CI",
    "answers": [
      {
        "content": "Associate different AWS Lambda functions to an API Gateway target endpoint.",
        "is_correct": false
      },
      {
        "content": "Create different stages in API Gateway. then associate API Gateway with AWS Lambda.",
        "is_correct": false
      },
      {
        "content": "Create aliases and versions in AWS Lambda.",
        "is_correct": true
      },
      {
        "content": "Tag the AWS Lambda functions with different names.",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "I think to 'allow the developer to perform a code push efficiently, without the need to update the API Gateway', this will require creating an alias referencing latest lambda version abd which can be referred to using API Gateway Stage Variable.<><>code branches are used to make a local copy of a repo for a dev to work on it on their local machine before merging changes back into the main branch - which may then be promoted to staging, QA, prod, etc as appropriate. Making an entirely new stage every time a dev pushes code makes no sense. Instead you would push the code into a new Lambda version and update the alias to point to it, with no changes on the API gateway."
    }
  },
  {
    "content": "A developer needs to deploy an application running on AWS Fargate using Amazon ECS. The application has environment variables that must be passed to a container for the application to initialize.<><>How should the environment variables be passed to the container?",
    "widget": "CI",
    "answers": [
      {
        "content": "Define an array that includes the environment variables under the environment parameter within the service definition.",
        "is_correct": false
      },
      {
        "content": "Define an array that includes the environment variables under the environment parameter within the task definition.",
        "is_correct": true
      },
      {
        "content": "Define an array that includes the environment variables under the entryPoint parameter within the task definition.",
        "is_correct": false
      },
      {
        "content": "Define an array that includes the environment variables under the entryPoint parameter within the service definition.",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": ""
    }
  },
  {
    "content": "A developer must extend an existing application that is based on the AWS Serverless Application Model (AWS SAM). The developer has used the AWS SAM CLI to create the project. The project contains different AWS Lambda functions.<><>Which combination of commands must the developer use to redeploy the AWS SAM application? (Choose two.)",
    "widget": "CM",
    "answers": [
      {
        "content": "sam init",
        "is_correct": false
      },
      {
        "content": "sam validate",
        "is_correct": false
      },
      {
        "content": "sam build",
        "is_correct": true
      },
      {
        "content": "sam deploy",
        "is_correct": true
      },
      {
        "content": "sam publish",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": ""
    }
  },
  {
    "content": "A developer manages an application that interacts with Amazon RDS. After observing slow performance with read queries, the developer implements Amazon ElastiCache to update the cache immediately following the primary database update.<><>What will be the result of this approach to caching?",
    "widget": "CI",
    "answers": [
      {
        "content": "Caching will increase the load on the database instance because the cache is updated for every database update.",
        "is_correct": false
      },
      {
        "content": "Caching will slow performance of the read queries because the cache is updated when the cache cannot find the requested data.",
        "is_correct": false
      },
      {
        "content": "The cache will become large and expensive because the infrequently requested data is also written to the cache.",
        "is_correct": true
      },
      {
        "content": "Overhead will be added to the initial response time because the cache is updated only after a cache miss.",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": ""
    }
  },
  {
    "content": "A developer has a stateful web server on-premises that is being migrated to AWS. The developer must have greater elasticity in the new design.<><>How should the developer re-factor the application to make it more elastic? (Choose two.)",
    "widget": "CM",
    "answers": [
      {
        "content": "Use pessimistic concurrency on Amazon DynamoDB.",
        "is_correct": false
      },
      {
        "content": "Use Amazon CloudFront with an Auto Scaling group.",
        "is_correct": false
      },
      {
        "content": "Use Amazon CloudFront with an AWS Web Application Firewall.",
        "is_correct": false
      },
      {
        "content": "Store session state data in an Amazon DynamoDB table.",
        "is_correct": true
      },
      {
        "content": "Use an ELB with an Auto Scaling group.",
        "is_correct": true
      }
    ],
    "explanation": {
      "content": "The most elastic and scalable solution for storing session state data in a web application is to use an Amazon DynamoDB table. This is because DynamoDB is a highly scalable and managed NoSQL database that can handle large amounts of read and write requests, making it well-suited for storing session state data.<><>Using an ELB with an Auto Scaling group is another way to make the application more elastic. An ELB (Elastic Load Balancer) is a managed load balancing service that can automatically distribute incoming application traffic across multiple Amazon EC2 instances in an Auto Scaling group, improving the application's availability and scalability."
    }
  },
  {
    "content": "A developer has a legacy application that is hosted on-premises. Other applications hosted on AWS depend on the on-premises application for proper functioning.<><>In case of any application errors, the developer wants to be able to use Amazon CloudWatch to monitor and troubleshoot all applications from one place.<><>How can the developer accomplish this?",
    "widget": "CI",
    "answers": [
      {
        "content": "Install an AWS SDK on the on-premises server to automatically send logs to CloudWatch.",
        "is_correct": false
      },
      {
        "content": "Download the CloudWatch agent to the on-premises server. Configure the agent to use IAM user credentials with permissions for CloudWatch.",
        "is_correct": true
      },
      {
        "content": "Upload log files from the on-premises server to Amazon S3 and have CloudWatch read the files.",
        "is_correct": false
      },
      {
        "content": "Upload log files from the on-premises server to an Amazon EC2 instance and have the instance forward the logs to CloudWatch.",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "KEY: monitoring on premise = CloudWatch Agent"
    }
  },
  {
    "content": "A developer is designing a serverless application with two AWS Lambda functions to process photos. One Lambda function stores objects in an Amazon S3 bucket and stores the associated metadata in an Amazon DynamoDB table. The other Lambda function fetches the objects from the S3 bucket by using the metadata from the DynamoDB table. Both Lambda functions use the same Python library to perform complex computations and are approaching the quota for the maximum size of zipped deployment packages.<><>What should the developer do to reduce the size of the Lambda deployment packages with the LEAST operational overhead?",
    "widget": "CI",
    "answers": [
      {
        "content": "Package each Python library in its own .zip file archive. Deploy each Lambda function with its own copy of the library.",
        "is_correct": false
      },
      {
        "content": "Create a Lambda layer with the required Python library. Use the Lambda layer in both Lambda functions.",
        "is_correct": true
      },
      {
        "content": "Combine the two Lambda functions into one Lambda function. Deploy the Lambda function as a single .zip file archive.",
        "is_correct": false
      },
      {
        "content": "Download the Python library to an S3 bucket. Program the Lambda functions to reference the object URLs.",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "[See docs](https://docs.aws.amazon.com/lambda/latest/dg/configuration-layers.html)."
    }
  },
  {
    "content": "A developer is adding a feature to a client-side application so that users can upload videos to an Amazon S3 bucket.<><>What is the MOST secure way to give the application the ability to write files to the S3 bucket?",
    "widget": "CI",
    "answers": [
      {
        "content": "Update the S3 bucket policy to allow public write access. Allow any user to upload videos by removing the need to handle user authentication within the client- side application.",
        "is_correct": false
      },
      {
        "content": "Create a new IAM policy and a corresponding IAM user with permissions to write to the S3 bucket. Store the key and the secret for the user in the application code. Use the key to authenticate the video uploads.",
        "is_correct": false
      },
      {
        "content": "Configure the API layer of the application to have a new endpoint that creates signed URLs that allow an object to be put into the S3 bucket. Generate a presigned URL through this API call in the client application. Upload the video by using the signed URL.",
        "is_correct": true
      },
      {
        "content": "Generate a new IAM key and a corresponding secret by using the AWS account root user credentials. Store the key and the secret for the user in the application code. Use the key to authenticate the video uploads.",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "[See docs](https://docs.aws.amazon.com/AmazonS3/latest/userguide/PresignedUrlUploadObject.html)."
    }
  },
  {
    "content": "A company is building an application for stock trading. The application needs sub-millisecond latency for processing trade requests. The company uses Amazon DynamoDB to store all the trading data that is used to process each trading request.<><>A development team performs load testing on the application and finds that the data retrieval time is higher than expected. The development team needs a solution that reduces the data retrieval time with the least possible effort.<><>Which solution meets these requirements?",
    "widget": "CI",
    "answers": [
      {
        "content": "Add local secondary indexes (LSIs) for the trading data.",
        "is_correct": false
      },
      {
        "content": "Store the trading data in Amazon S3, and use S3 Transfer Acceleration.",
        "is_correct": false
      },
      {
        "content": "Add retries with exponential backoff for DynamoDB queries.",
        "is_correct": false
      },
      {
        "content": "Use DynamoDB Accelerator (DAX) to cache the trading data.",
        "is_correct": true
      }
    ],
    "explanation": {
      "content": "keywords are ‘time required to get data’, ‘significantly improves data retrieval time’ and ‘least amount of work’.<><>Amazon DynamoDB is designed for scale and performance. In most cases, the DynamoDB response times can be measured in single-digit milliseconds. However, there are certain use cases that require response times in microseconds. For these use cases, DynamoDB Accelerator (DAX) delivers fast response times for accessing eventually consistent data."
    }
  },
  {
    "content": "A developer needs to build and deploy a serverless application that has an API that mobile clients will use. The API will use Amazon DynamoDB and Amazon OpenSearch Service (Amazon Elasticsearch Service) as data sources. Responses that are sent to the clients will contain aggregated data from both data sources.<><>The developer must minimize the number of API endpoints and must minimize the number of API calls that are required to retrieve the necessary data.<><>Which solution should the developer use to meet these requirements?",
    "widget": "CI",
    "answers": [
      {
        "content": "GraphQL API on AWS AppSync",
        "is_correct": true
      },
      {
        "content": "REST API on Amazon API Gateway",
        "is_correct": false
      },
      {
        "content": "GraphQL API on an Amazon EC2 instance",
        "is_correct": false
      },
      {
        "content": "REST API on AWS Elastic Beanstalk",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "AppSync has direct integration with DynamoDB and OpenSearch, and it talks about minimizing number of API endpoints. With AppSync you only have one endpoint and the client asks for the data it wants.<><>AppSync can combine data from one or more sources:<><>• NoSQL data stores, Relational databases, HTTP APIs…,<><>• Integrates with DynamoDB, Aurora, OpenSearch & others,<><>• Custom sources with AWS Lambda."
    }
  },
  {
    "content": "A Lambda function processes data before sending it to a downstream service. Each piece of data is approximately 1MB in size. After a security audit, the function is now required to encrypt the data before sending it downstream.<><>Which API call is required to perform the encryption?",
    "widget": "CI",
    "answers": [
      {
        "content": "Pass the data to the KMS ReEncrypt API for encryption.",
        "is_correct": false
      },
      {
        "content": "Use the KMS GenerateDataKey API to get an encryption key.",
        "is_correct": true
      },
      {
        "content": "Use the KMS GenerateDataKeyWithoutPlainText API to get an encryption key.",
        "is_correct": false
      },
      {
        "content": "Pass the data to KMS as part of the Encrypt API for encryption.",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "Encrypt API can encrypt plaintext of up to 4 KB using a KMS key."
    }
  },
  {
    "content": "A company has a web application that runs on Amazon EC2 instances with a custom Amazon Machine Image (AMI). The company uses AWS CloudFormation to provision the application. The application runs in the us-east-1 Region, and the company needs to deploy the application to the us-west-1 Region.<><>An attempt to create the AWS CloudFormation stack in us-west-1 fails. An error message states that the AMI ID does not exist. A developer must resolve this error with a solution that uses the least amount of operational overhead.<><>Which solution meets these requirements?",
    "widget": "CI",
    "answers": [
      {
        "content": "Change the AWS CloudFormation templates for us-east-1 and us-west-1 to use an AWS AMI. Relaunch the stack for both Regions.",
        "is_correct": false
      },
      {
        "content": "Copy the custom AMI from us-east-1 to us-west-1. Update the AWS CloudFormation template for us-west-1 to refer to AMI ID for the copied AMI. Relaunch the stack.",
        "is_correct": true
      },
      {
        "content": "Build the custom AMI in us-west-1. Create a new AWS CloudFormation template to launch the stack in us-west-1 with the new AMI ID.",
        "is_correct": false
      },
      {
        "content": "Manually deploy the application outside AWS CloudFormation in us-west-1.",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "AMI is region specific. Create New AMIs for each individual region."
    }
  },
  {
    "content": "A developer wants to run a PHP website with an NGINX proxy and package them as Docker containers in one environment. The developer wants a managed environment with automated provisioning and load balancing. The developer cannot change the configuration and must minimize operational overhead.<><>How should the developer build the website to meet these requirements?",
    "widget": "CI",
    "answers": [
      {
        "content": "Create a new application in AWS Elastic Beanstalk that is preconfigured for a multicontainer Docker environment. Upload the code, and deploy it to a web server environment.",
        "is_correct": true
      },
      {
        "content": "Deploy the code on Amazon EC2 instances in an Auto Scaling group behind an Application Load Balancer.",
        "is_correct": false
      },
      {
        "content": "Construct an AWS CloudFormation template that launches Amazon EC2 instances. Install and configure the PHP code by using cfn helper scripts.",
        "is_correct": false
      },
      {
        "content": "Upload the code for the PHP website into an Amazon S3 bucket. Host the website from the S3 bucket.",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "Automatic provisioning points to EB."
    }
  },
  {
    "content": "A company has a website that displays a daily newsletter. When a user visits the website, an AWS Lambda function processes the browser's request and queries the company's on-premises database to obtain the current newsletter. The newsletters are stored in English. The Lambda function uses the Amazon Translate TranslateText API operation to translate the newsletters, and the translation is displayed to the user.<><>Due to an increase in popularity, the website's response time has slowed. The database is overloaded. The company cannot change the database and needs a solution that improves the response time of the Lambda function.<><>Which solution meets these requirements?",
    "widget": "CI",
    "answers": [
      {
        "content": "Change to asynchronous Lambda function invocation.",
        "is_correct": false
      },
      {
        "content": "Cache the translated newsletters in the Lambda /tmp directory.",
        "is_correct": true
      },
      {
        "content": "Enable TranslateText API caching.",
        "is_correct": false
      },
      {
        "content": "Change the Lambda function to use parallel processing.",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "We can use /tmp as temp cache for Lambda. [See docs](https://aws.amazon.com/blogs/compute/choosing-between-aws-lambda-data-storage-options-in-web-apps/)."
    }
  },
  {
    "content": "A developer is creating an AWS CloudFormation template for an application. The application includes an Amazon RDS database. The password to be set for the resource's MasterUserPassword property is already stored in AWS Secrets Manager.<><>How can the developer reference the value of the password in the CloudFormation template?",
    "widget": "CI",
    "answers": [
      {
        "content": "Use a parameter in the CloudFormation template with the same name of the secret.",
        "is_correct": false
      },
      {
        "content": "Use the ssm dynamic reference by specifying the name of the secret and its version.",
        "is_correct": false
      },
      {
        "content": "Use the secretsmanager dynamic reference by specifying the appropriate reference-key segment.",
        "is_correct": true
      },
      {
        "content": "Use the ssm-secure dynamic reference by specifying the name of the secret and its version.",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "Use the secretsmanager dynamic reference to retrieve entire secrets or secret values that are stored in Secrets Manager for use in your templates. Secrets can be database credentials, passwords, third-party API keys, or arbitrary text. [See docs](https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/dynamic-references.html#dynamic-references-secretsmanager)."
    }
  },
  {
    "content": "An application is using Amazon DynamoDB as its data store, and should be able to read 100 items per second as strongly consistent reads. Each item is 5 KB in size.<><>To what value should the table's provisioned read throughput be set?",
    "widget": "CI",
    "answers": [
      {
        "content": "A. 50 read capacity units",
        "is_correct": false
      },
      {
        "content": "B. 100 read capacity units",
        "is_correct": false
      },
      {
        "content": "C. 200 read capacity units",
        "is_correct": true
      },
      {
        "content": "D. 500 read capacity units",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "1 RCU = 1 strong consistent read for 4k. 5K = 2RCU. x 100 = 200RCU"
    }
  },
  {
    "content": "A developer created a Lambda function for a web application backend. When testing the Lambda function from the AWS Lambda console, the developer can see that the function is being run, but there is no log data being generated in Amazon CloudWatch Logs, even after several minutes.<><>What could cause this situation?",
    "widget": "CI",
    "answers": [
      {
        "content": "The Lambda function does not have any explicit log statements for the log data to send it to CloudWatch Logs.",
        "is_correct": false
      },
      {
        "content": "The Lambda function is missing CloudWatch Logs as a source trigger to send log data.",
        "is_correct": false
      },
      {
        "content": "The execution role for the Lambda function is missing permissions to write log data to the CloudWatch Logs.",
        "is_correct": true
      },
      {
        "content": "The Lambda function is missing a target CloudWatch Log group.",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "The Lambda function does not have any explicit log statements for the log data to send it to CloudWatch Logs. -- If lambda was executed, it will always have some logs to send. For example execution time and memory usage will be always send after execution is done. [Incorrect]<><>The Lambda function is missing CloudWatch Logs as a source trigger to send log data. -- Lambda is connected to CloudWatch by default and you cannot disable it. [Incorrect]<><>The execution role for the Lambda function is missing permissions to write log data to the CloudWatch Logs. -- You have to explicitly define a IAM role with IAM policy which allows Lambda to send logs to CloudWatch. [Correct]<><>The Lambda function is missing a target CloudWatch Log group. -- Lambda is connected to CloudWatch by default and you cannot disable it. [Incorrect]"
    }
  },
  {
    "content": "A developer has written code for an application and wants to share it with other developers on the team to receive feedback. The shared application code needs to be stored long-term with multiple versions and batch change tracking.<><>Which AWS service should the developer use?",
    "widget": "CI",
    "answers": [
      {
        "content": "AWS CodeBuild",
        "is_correct": false
      },
      {
        "content": "Amazon S3",
        "is_correct": false
      },
      {
        "content": "AWS CodeCommit",
        "is_correct": true
      },
      {
        "content": "AWS Cloud9",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": ""
    }
  },
  {
    "content": "A developer has created a new IAM user that has the s3:PutObject permission to write to a specific Amazon S3 bucket. The S3 bucket uses server-side encryption with AWS KMS managed keys (SSE-KMS) as the default encryption. When an application uses the access key and secret key of the IAM user to call the PutObject API operation, the application receives an access denied error.<><>What should the developer do to resolve this error?",
    "widget": "CI",
    "answers": [
      {
        "content": "Update the policy of the IAM user to allow the s3:EncryptionConfiguration action.",
        "is_correct": false
      },
      {
        "content": "Update the bucket policy of the S3 bucket to allow the IAM user to upload objects.",
        "is_correct": false
      },
      {
        "content": "Update the policy of the IAM user to allow the kms:GenerateDataKey action.",
        "is_correct": true
      },
      {
        "content": "Update the ACL of the S3 bucket to allow the IAM user to upload objects.",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "[See docs](https://aws.amazon.com/premiumsupport/knowledge-center/s3-access-denied-error-kms/)."
    }
  },
  {
    "content": "A static website is hosted in an Amazon S3 bucket. Several HTML pages on the site use JavaScript to download images from another Amazon S3 bucket. These images are not displayed when users browse the site.<><>What is the possible cause for the issue?",
    "widget": "CI",
    "answers": [
      {
        "content": "The referenced Amazon S3 bucket is in another region.",
        "is_correct": false
      },
      {
        "content": "The images must be stored in the same Amazon S3 bucket.",
        "is_correct": false
      },
      {
        "content": "Port 80 must be opened on the security group in which the Amazon S3 bucket is located.",
        "is_correct": false
      },
      {
        "content": "Cross Origin Resource Sharing must be enabled on the Amazon S3 bucket.",
        "is_correct": true
      }
    ],
    "explanation": {
      "content": "CORS should be enabled on a bucket, if another bucket wants to access resources from it."
    }
  },
  {
    "content": "An application needs to encrypt data that is written to Amazon S3 where the keys are managed in an on-premises data center, and the encryption is handled by S3.<><>Which type of encryption should be used?",
    "widget": "CI",
    "answers": [
      {
        "content": "Use server-side encryption with Amazon S3-managed keys.",
        "is_correct": false
      },
      {
        "content": "Use server-side encryption with AWS KMS-managed keys.",
        "is_correct": false
      },
      {
        "content": "Use client-side encryption with AWS KMS-managed keys.",
        "is_correct": false
      },
      {
        "content": "Use server-side encryption with customer-provided keys.",
        "is_correct": true
      }
    ],
    "explanation": {
      "content": "With Server-Side Encryption with Customer-Provided Keys (SSE-C), you manage the encryption keys and Amazon S3 manages the encryption, as it writes to disks, and decryption, when you access your objects.<><>With Server-Side Encryption with Customer-Provided Keys (SSE-C), you manage the encryption keys and Amazon S3 manages the encryption, as it writes to disks, and decryption, when you access your objects."
    }
  },
  {
    "content": "A developer is building a serverless application that is based on AWS Lambda. The developer initializes the AWS software development kit (SDK) outside of the Lambda handler function.<><>What is the PRIMARY benefit of this action?",
    "widget": "CI",
    "answers": [
      {
        "content": "Improves legibility and stylistic convention",
        "is_correct": false
      },
      {
        "content": "Takes advantage of runtime environment reuse",
        "is_correct": true
      },
      {
        "content": "Provides better error handling",
        "is_correct": false
      },
      {
        "content": "Creates a new SDK instance for each invocation",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "Initialize SDK clients and database connections outside of the function handler, and cache static assets locally in the /tmp directory. Subsequent invocations processed by the same instance of your function can reuse these resources. This saves cost by reducing function run time."
    }
  },
  {
    "content": "A developer is testing a Docker-based application that uses the AWS SDK to interact with Amazon DynamoDB. In the local development environment, the application has used IAM access keys. The application is now ready for deployment onto an ECS cluster.<><>How should the application authenticate with AWS services in production?",
    "widget": "CI",
    "answers": [
      {
        "content": "A. Configure an ECS task IAM role for the application to use.",
        "is_correct": true
      },
      {
        "content": "B. Refactor the application to call AWS STS AssumeRole based on an instance role.",
        "is_correct": false
      },
      {
        "content": "C. Configure AWS access key/secret access key environment variables with new credentials.",
        "is_correct": false
      },
      {
        "content": "D. Configure the credentials file with a new access key/secret access key.",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "With IAM roles for Amazon ECS tasks, you can specify an IAM role that can be used by the containers in a task. [See docs](https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-iam-roles.html)."
    }
  },
  {
    "content": "A developer has created an AWS Lambda function to provide notification through Amazon Simple Notification Service (Amazon SNS) whenever a file is uploaded to Amazon S3 that is larger than 50 MB. The developer has deployed and tested the Lambda function by using the CLI. However, when the event notification is added to the S3 bucket and a 3,000 MB file is uploaded, the Lambda function does not launch.<><>Which of the following is a possible reason for the Lambda function's inability to launch?",
    "widget": "CI",
    "answers": [
      {
        "content": "The S3 event notification does not activate for files that are larger than 1,000 MB.",
        "is_correct": false
      },
      {
        "content": "The resource-based policy for the Lambda function does not have the required permissions to be invoked by Amazon S3.",
        "is_correct": true
      },
      {
        "content": "Lambda functions cannot be invoked directly from an S3 event.",
        "is_correct": false
      },
      {
        "content": "The S3 bucket needs to be made public.",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "S3 event notification does not activate for files that are larger than 1,000 MB -- No such limits<><>The resource-based policy for the Lambda function does not have the required permissions to be invoked by Amazon S3 -- Yes, resource based policies allows principals to assume it.<><>Lambda functions cannot be invoked directly from an S3 event. -- S3 has direct integration with lambda. Amazon S3 can send an event to a Lambda function when an object is created or deleted.<><>The S3 bucket needs to be made public. -- definitely not a secure choice"
    }
  },
  {
    "content": "A company stores documents in Amazon S3 with default settings. A new regulation requires the company to encrypt the documents at rest, rotate the encryption keys annually, and keep a record of when the encryption keys were rotated. The company does not want to manage the encryption keys outside of AWS.<><>Which solution will meet these requirements?",
    "widget": "CI",
    "answers": [
      {
        "content": "Use server-side encryption with Amazon S3 managed encryption keys (SSE-S3).",
        "is_correct": false
      },
      {
        "content": "Use server-side encryption with AWS KMS managed encryption keys (SSE-KMS).",
        "is_correct": true
      },
      {
        "content": "Use server-side encryption with customer-provided encryption keys (SSE-C).",
        "is_correct": false
      },
      {
        "content": "Use client-side encryption before sending the data to Amazon S3.",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "Server-Side Encryption with AWS KMS keys (SSE-KMS) is similar to SSE-S3, but with some additional benefits and charges for using this service. There are separate permissions for the use of a KMS key that provides added protection against unauthorized access of your objects in Amazon S3. SSE-KMS also provides you with an audit trail that shows when your KMS key was used and by whom. Additionally, you can create and manage customer managed keys or use AWS managed keys that are unique to you, your service, and your Region. [See docs](https://docs.aws.amazon.com/AmazonS3/latest/userguide/serv-side-encryption.html)."
    }
  },
  {
    "content": "A developer has discovered that an application responsible for processing messages in an Amazon SQS queue is routinely falling behind. The application is capable of processing multiple messages in one invocation, but is only receiving one message at a time.<><>What should the developer do to increase the number of messages the application receives?",
    "widget": "CI",
    "answers": [
      {
        "content": "Call the ChangeMessageVisibility API for the queue and set MaxNumberOfMessages to a value greater than the default of 1.",
        "is_correct": false
      },
      {
        "content": "Call the AddPermission API to set MaxNumberOfMessages for the ReceiveMessage action to a value greater than the default of 1.",
        "is_correct": false
      },
      {
        "content": "Call the ReceiveMessage API to set MaxNumberOfMessages to a value greater than the default of 1.",
        "is_correct": true
      },
      {
        "content": "Call the SetQueueAttributes API for the queue and set MaxNumberOfMessages to a value greater than the default of 1.",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "One can change the number of messages received in a queue from 1 to 10 messages ate a time, by calling the ReceiveMessage API. Use ReceiveMessage API to retrieve up to 10 messages at a time, from the specified queue. [See docs](https://docs.aws.amazon.com/AWSSimpleQueueService/latest/APIReference/API_ReceiveMessage.html)."
    }
  },
  {
    "content": "A developer is using an Amazon Kinesis Data Firehose delivery stream to store data in Amazon S3. Before storing the data in Amazon S3, the developer wants to enrich the data by combining the data with data from an Amazon DynamoDB table.<><>How can the developer implement the data enrichment?",
    "widget": "CI",
    "answers": [
      {
        "content": "Create a Kinesis Data Firehose data transformation by using an Amazon EC2 instance.",
        "is_correct": false
      },
      {
        "content": "Configure the Kinesis Data Firehose delivery stream to send data to a Kinesis data stream. Enrich the data by using an AWS Lambda function.",
        "is_correct": false
      },
      {
        "content": "Configure the Kinesis Data Firehose delivery stream to store data in the DynamoDB table. Export the table to Amazon S3.",
        "is_correct": false
      },
      {
        "content": "Create a Kinesis Data Firehose data transformation by using an AWS Lambda function.",
        "is_correct": true
      }
    ],
    "explanation": {
      "content": "Kinesis Data Firehose can invoke your Lambda function to transform incoming source data and deliver the transformed data to destinations. You can enable Kinesis Data Firehose data transformation when you create your delivery stream."
    }
  },
  {
    "content": "A company created an application to consume and process data. The application uses Amazon Simple Queue Service (Amazon SQS) and AWS Lambda functions. The application is currently working as expected, but it occasionally receives several messages that it cannot process properly. The company needs to clear these messages to prevent the queue from becoming blocked.<><>A developer must implement a solution that makes queue processing always operational. The solution must give the company the ability to defer the messages with errors and save these messages for further analysis.<><>What is the MOST operationally efficient solution that meets these requirements?",
    "widget": "CI",
    "answers": [
      {
        "content": "Configure Amazon CloudWatch Logs to save the error messages to a separate log stream.",
        "is_correct": false
      },
      {
        "content": "Create a new SQS queue. Set the new queue as a dead-letter queue for the application queue. Configure the Maximum Receives setting.",
        "is_correct": true
      },
      {
        "content": "Change the SQS queue to a FIFO queue. Configure the message retention period to 0 seconds.",
        "is_correct": false
      },
      {
        "content": "Configure an Amazon CloudWatch alarm for Lambda function errors. Publish messages to an Amazon Simple Notification Service (Amazon SNS) topic to notify administrator users.",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "The main keywords here are 'save these messages for further analysis' so DLQ is always a good option in this kind of question"
    }
  },
  {
    "content": "A company has a website that is developed in PHP and is launched using AWS Elastic Beanstalk. There is a new version of the website that needs to be deployed in the Elastic Beanstalk environment. The company cannot tolerate having the website offline if an update fails. Deployments must have minimal impact and rollback as soon as possible.<><>What deployment method should be used?",
    "widget": "CI",
    "answers": [
      {
        "content": "All at once",
        "is_correct": false
      },
      {
        "content": "Rolling",
        "is_correct": false
      },
      {
        "content": "Snapshots",
        "is_correct": false
      },
      {
        "content": "Immutable",
        "is_correct": true
      }
    ],
    "explanation": {
      "content": "[See docs](https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/environmentmgmt-updates-immutable.html)."
    }
  },
  {
    "content": "An application running on multiple Amazon EC2 instances pulls messages from a standard Amazon SQS queue. A requirement for the application is that all messages must be encrypted at rest.<><>Developers are instructed to use methods that allow for centralized key management and minimize possible support requirements whenever possible.<><>Which of the following solutions supports these requirements?",
    "widget": "CI",
    "answers": [
      {
        "content": "Encrypt individual messages by using client-side encryption with customer managed keys, then write to the SQS queue.",
        "is_correct": false
      },
      {
        "content": "Encrypt individual messages by using SQS Extended Client and the Amazon S3 encryption client.",
        "is_correct": false
      },
      {
        "content": "Create an SQS queue, and encrypt the queue by using server-side encryption with AWS KMS.",
        "is_correct": true
      },
      {
        "content": "Create an SQS queue, and encrypt the queue by using client-side encryption.",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "SSE encrypts messages as soon as Amazon SQS receives them. The messages are stored in encrypted form and Amazon SQS decrypts messages only when they are sent to an authorized consumer.<><>Encrypt individual messages by using client-side encryption with customer managed keys, then write to the SQS queue. -- We want AWS to manage Key for us.<><>Encrypt individual messages by using SQS Extended Client and the Amazon S3 encryption client. -- SQS Extended Client enables you to manage Amazon SQS message payloads with Amazon S3 and haven't any relation with encryption.<><>Create an SQS queue, and encrypt the queue by using client-side encryption. -- We need a server side encryption."
    }
  },
  {
    "content": "A company is running a web application that is using Amazon Cognito for authentication. The company does not want to use multi-factor authentication (MFA) for all the visitors every time, but the company's security team has concerns about compromised credentials. The development team needs to configure mandatory MFA only when suspicious sign-in attempts are detected.<><>Which Amazon Cognito feature will meet these requirements?",
    "widget": "CI",
    "answers": [
      {
        "content": "Short message service (SMS) text message MFA",
        "is_correct": false
      },
      {
        "content": "Advanced security metrics",
        "is_correct": false
      },
      {
        "content": "Time-based one-time password (TOTP) software token MFA",
        "is_correct": false
      },
      {
        "content": "Adaptive authentication",
        "is_correct": true
      }
    ],
    "explanation": {
      "content": "With adaptive authentication, you can configure your user pool to block suspicious sign-ins or add second factor authentication in response to an increased risk level. [See docs](https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-user-pool-settings-adaptive-authentication.html)."
    }
  },
  {
    "content": "A company has a two-tier application running on an Amazon EC2 server that handles all of its AWS based e-commerce activity. During peak times, the backend servers that process orders are overloaded with requests. This results in some orders failing to process. A developer needs to create a solution that will re-factor the application.<><>Which steps will allow for more flexibility during peak times, while still remaining cost-effective? (Choose two.)",
    "widget": "CM",
    "answers": [
      {
        "content": "Increase the backend T2 EC2 instance sizes to x1 to handle the largest possible load throughout the year.",
        "is_correct": false
      },
      {
        "content": "Implement an Amazon SQS queue to decouple the front-end and backend servers.",
        "is_correct": true
      },
      {
        "content": "Use an Amazon SNS queue to decouple the front-end and backend servers.",
        "is_correct": false
      },
      {
        "content": "Migrate the backend servers to on-premises and pull from an Amazon SNS queue.",
        "is_correct": false
      },
      {
        "content": "Modify the backend servers to pull from an Amazon SQS queue.",
        "is_correct": true
      }
    ],
    "explanation": {
      "content": "For those who don't know what a two-tier application is: it is called two tier when front end is connected directly to the data source (like a database). So there is no EC2 or SQS queue in between the front end and the data source.<><>Although this architecture is cost-effective because you only pay running the data source, but not something in between. But the disadvantage is, just like the question states, the back end cannot respond to all requests coming at the same time."
    }
  },
  {
    "content": "A developer is creating an AWS Lambda function that generates a new file each time it runs. Each new file must be checked into an AWS CodeCommit repository hosted in the same AWS account.<><>How should the developer accomplish this?",
    "widget": "CI",
    "answers": [
      {
        "content": "When the Lambda function starts, use the Git CLI to clone the repository. Check the new file into the cloned repository and push the change.",
        "is_correct": false
      },
      {
        "content": "After the new file is created in Lambda, use cURL to invoke the CodeCommit API. Send the file to the repository.",
        "is_correct": false
      },
      {
        "content": "Use an AWS SDK to instantiate a CodeCommit client. Invoke the putjile method to add the file to the repository.",
        "is_correct": true
      },
      {
        "content": "Upload the new file to an Amazon S3 bucket. Create an AWS Step Function to accept S3 events. In the Step Function, add the new file to the repository.",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "[See docs](https://docs.aws.amazon.com/cli/latest/reference/codecommit/index.html)."
    }
  },
  {
    "content": "A developer is designing an Amazon DynamoDB table for an application. The application will store user information that includes a unique identifier and an email address for each user. The application must be able to query the table by using either the unique identifier or the email address.<><>How should the developer design the DynamoDB table to meet these requirements?",
    "widget": "CI",
    "answers": [
      {
        "content": "For the primary key of the table, specify the unique identifier as the partition key and specify the email address as the sort key.",
        "is_correct": false
      },
      {
        "content": "For the primary key of the table, specify the unique identifier as the partition key. Create a local secondary index (LSI) based on the email address.",
        "is_correct": false
      },
      {
        "content": "For the primary key of the table, specify the email address as the partition key and specify the unique identifier as the sort key.",
        "is_correct": false
      },
      {
        "content": "For the primary key of the table, specify the unique identifier as the partition key. Create a global secondary index (GSI) based on the email address.",
        "is_correct": true
      }
    ],
    "explanation": {
      "content": "the GSI can have different partition key and sort key from base table. So we can create partition key is ‘email address’ and sort key is ‘unique identifier’ for GSI. Then we can use only ‘email address’ for query.<><>For the primary key of the table, specify the unique identifier as the partition key and specify the email address as the sort key. -- For the primary key of the table, specify the unique identifier as the partition key and specify the email address as the sort key.)<><>For the primary key of the table, specify the unique identifier as the partition key. Create a local secondary index (LSI) based on the email address. -- the LSI index must have the same partition key with base table. So in case the assumption above, you cannot query only email address."
    }
  },
  {
    "content": "A developer has an application that asynchronously invokes an AWS Lambda function. The developer wants to store messages that resulted in failed invocations of the Lambda function so that the application can retry the call later.<><>What should the developer do to accomplish this goal with the LEAST operational overhead?",
    "widget": "CI",
    "answers": [
      {
        "content": "Set up Amazon CloudWatch Logs log groups to filter and store the messages in an Amazon S3 bucket. Import the messages in Lambda. Run the Lambda function again.",
        "is_correct": false
      },
      {
        "content": "Configure Amazon EventBridge (Amazon CloudWatch Events) to send the messages to Amazon Simple Notification Service (Amazon SNS) to initiate the Lambda function again.",
        "is_correct": false
      },
      {
        "content": "Implement a dead-letter queue for discarded messages. Set the dead-letter queue as an event source for the Lambda function.",
        "is_correct": true
      },
      {
        "content": "Send Amazon EventBridge (Amazon CloudWatch Events) events to an Amazon Simple Queue Service (Amazon SQS) queue. Configure the Lambda function to pull messages from the SQS queue. Run the Lambda function again.",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "Asynchronous invocation allow you to configure SQS queue, park all error in DLQ. Asynchronous invocation – You can configure a dead-letter queue on the function to capture events that weren't successfully processed.<><>Event source mappings – Event source mappings that read from streams retry the entire batch of items.<><>For event source mappings that read from a queue, you determine the length of time between retries and destination for failed events by configuring the visibility timeout and redrive policy on the source queue.<><>[See docs](https://docs.aws.amazon.com/lambda/latest/dg/invocation-retries.html)."
    }
  },
  {
    "content": "A developer is writing an application in Python. The application runs on AWS Lambda. The application generates a file and needs to upload this file to Amazon S3.<><>The developer must implement this upload functionality with the least possible change to the application code.<><>Which solution meets these requirements?",
    "widget": "CI",
    "answers": [
      {
        "content": "Make an HTTP request directly to the S3 API to upload the file.",
        "is_correct": false
      },
      {
        "content": "Include the AWS SDK for Python in the Lambda function. Use the SDK to upload the file.",
        "is_correct": false
      },
      {
        "content": "Use the AWS SDK for Python that is installed in the Lambda environment to upload the file.",
        "is_correct": true
      },
      {
        "content": "Use the AWS CLI that is installed in the Lambda environment to upload the file.",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "boto3 module is already available in the AWS Lambda Python runtimes, don’t bother including boto3 and its dependency botocore in your Lambda deployment zip file."
    }
  },
  {
    "content": "An application that is hosted on an Amazon EC2 instance needs access to files that are stored in an Amazon S3 bucket. The application lists the objects that are stored in the S3 bucket and displays a table to the user. During testing, a developer discovers that the application does not show any objects in the list.<><>What is the MOST secure way to resolve this issue?",
    "widget": "CI",
    "answers": [
      {
        "content": "Update the IAM instance profile that is attached to the EC2 instance to include the S3:' permission for the S3 bucket.",
        "is_correct": false
      },
      {
        "content": "Update the IAM instance profile that is attached to the EC2 instance to include the S3:ListBucket permission for the S3 bucket.",
        "is_correct": true
      },
      {
        "content": "Update the developer's user permissions to include the S3:ListBucket permission for the S3 bucket.",
        "is_correct": false
      },
      {
        "content": "Update the S3 bucket policy by including the S3:ListBucket permission and by setting the Principal element to specify the account number of the EC2 instance.",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "least principle of privilege, so no s3:*. to view all files in bucket, s3:listbucket is sufficient. s3:ListBucket permission allows the user to use the Amazon S3 GET Bucket (List Objects) operation."
    }
  },
  {
    "content": "A developer needs to implement a cache to store data that an application frequently queries from an Amazon RDS for MySQL database. The data structures that will be cached include sets and sorted sets.<><>How should the developer implement the cache to achieve the LOWEST latency?",
    "widget": "CI",
    "answers": [
      {
        "content": "Create an Amazon ElastiCache for Memcached instance. Serialize the data as JSON before caching the data.",
        "is_correct": false
      },
      {
        "content": "Create an Amazon ElastiCache for Redis instance. Use a Redis client library to cache the data.",
        "is_correct": true
      },
      {
        "content": "Create an Amazon DynamoDB table. Serialize the data as JSON before caching the data.",
        "is_correct": false
      },
      {
        "content": "Create an Amazon ElastiCache for Memcached instance. Use a Memcached client library to cache the data.",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "Memcache has no data type. Redis supports storing objects. Memcache has no support for sorted set"
    }
  },
  {
    "content": "A developer creates an AWS Lambda function to publish a message to an Amazon Simple Notification Service (Amazon SNS) topic. All message content must be encrypted in transit and at rest between Lambda and Amazon SNS. A part of the Lambda execution role is as follows:<><>`'Effect': 'Allow', 'Action': 'SNS:Publish', 'Resource': 'arn:aws:sns:us-east-1:1234567890:secure-topic'`<><>Which combination of steps should the developer take to meet these requirements? (Choose two.)",
    "widget": "CM",
    "answers": [
      {
        "content": "Enable server-side encryption on the SNS topic.",
        "is_correct": true
      },
      {
        "content": "Add a Deny statement to the Lambda execution role. Specify the SNS topic ARN as the resource. Specify 'aws:SecureTransport': 'true' as the condition.",
        "is_correct": false
      },
      {
        "content": "Create a VPC endpoint for Amazon SNS.",
        "is_correct": false
      },
      {
        "content": "Add a StringEquals condition of 'sns:Protocol': 'https' to the Lambda execution role.",
        "is_correct": false
      },
      {
        "content": "Add a Deny statement to the Lambda execution role. Specify the SNS topic ARN as the resource. Specify 'aws:SecureTransport': 'false' as the condition.",
        "is_correct": true
      }
    ],
    "explanation": {
      "content": "Trick question. You SHOULD deny policy when protocol is not HTTPS. [See docs](https://docs.aws.amazon.com/sns/latest/dg/sns-security-best-practices.html)."
    }
  },
  {
    "content": "A developer is using an AWS Key Management Service (AWS KMS) customer master key (CMK) with imported key material to encrypt data in Amazon S3. The developer accidentally deletes the key material of the CMK and is unable to decrypt the data.<><>How can the developer decrypt the data that was encrypted by the CMK?",
    "widget": "CI",
    "answers": [
      {
        "content": "Request support from AWS to recover the deleted key material.",
        "is_correct": false
      },
      {
        "content": "Create a new CMK. Use the new CMK to decrypt the data.",
        "is_correct": false
      },
      {
        "content": "Use the CMK without the key material.",
        "is_correct": false
      },
      {
        "content": "Reimport the same key material to the CMK.",
        "is_correct": true
      }
    ],
    "explanation": {
      "content": "To use the KMS key again, you must reimport the same key material into the KMS key. In contrast, deleting a KMS key is irreversible. If you schedule key deletion and the required waiting period expires, AWS KMS deletes the key material and all metadata associated with the KMS key."
    }
  },
  {
    "content": "A developer needs to launch a new Amazon EC2 instance by using the AWS CLI.<><>Which AWS CLI command should the developer use to meet this requirement?",
    "widget": "CI",
    "answers": [
      {
        "content": "aws ec2 bundle-instance",
        "is_correct": false
      },
      {
        "content": "aws ec2 start-instances",
        "is_correct": false
      },
      {
        "content": "aws ec2 confirm-product-instance",
        "is_correct": false
      },
      {
        "content": "aws ec2 run instances",
        "is_correct": true
      }
    ],
    "explanation": {
      "content": "[See docs](https://docs.aws.amazon.com/cli/latest/userguide/cli-services-ec2-instances.html)."
    }
  },
  {
    "content": "A development team uses AWS Elastic Beanstalk for application deployment. The development team has configured the application version lifecycle policy to limit the number of application versions to 25. However, even with the application version lifecycle policy, the source bundle is deleted from the Amazon S3 source bucket.<><>What should the development team do in the Elastic Beanstalk application version lifecycle settings to retain the source code in the S3 bucket?",
    "widget": "CI",
    "answers": [
      {
        "content": "Enable versioning on the source bundle S3 bucket.",
        "is_correct": false
      },
      {
        "content": "Disable the S3 bucket lifecycle policy to avoid the archiving of the source bundle.",
        "is_correct": false
      },
      {
        "content": "Update the Elastic Beanstalk application version lifecycle policy to increase the version quota to 50.",
        "is_correct": false
      },
      {
        "content": "Update the Elastic Beanstalk application version lifecycle policy to retain the source bundle in Amazon S3.",
        "is_correct": true
      }
    ],
    "explanation": {
      "content": "Set Retention to Retain source bundle in S3. [See docs](https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/applications-lifecycle.html)"
    }
  },
  {
    "content": "A development team is building a new application that will run on Amazon EC2 and use Amazon DynamoDB as a storage layer. The developers all have assigned IAM user accounts in the same IAM group. The developers currently can launch EC2 instances, and they need to be able to launch EC2 instances with an instance role allowing access to Amazon DynamoDB.<><>Which AWS IAM changes are needed when creating an instance role to provide this functionality?",
    "widget": "CI",
    "answers": [
      {
        "content": "Create an IAM permission policy attached to the role that allows access to DynamoDB. Add a trust policy to the role that allows DynamoDB to assume the role. Attach a permissions policy to the development group in AWS IAM that allows developers to use the iam:GetRole and iam:PassRole permissions for the role.",
        "is_correct": false
      },
      {
        "content": "Create an IAM permissions policy attached to the role that allows access to DynamoDAdd a trust policy to the role that allows Amazon EC2 to assume the role. Attach a permissions policy to the development group in AWS IAM that allows developers to use the iam:PassRole permission for the role.",
        "is_correct": true
      },
      {
        "content": "Create an IAM permission policy attached to the role that allows access to Amazon EC2. Add a trust policy to the role that allows DynamoDB to assume the role. Attach a permissions policy to the development group in AWS IAM that allows developers to use the iam:PassRole permission for the role.",
        "is_correct": false
      },
      {
        "content": "Create an IAM permissions policy attached to the role that allows access to DynamoDB. Add a trust policy to the role that allows Amazon EC2 to assume the role. Attach a permissions policy to the development group in AWS IAM that allows developers to use the iam:GetRole permission for the role.",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "Policy attached to a Role for EC2 instance profile. Policy gives Role access to Dynamo and allows developer to passrole into EC2 during launch."
    }
  },
  {
    "content": "A banking application processes thousands of transactions each second. Each transaction payload must have end-to-end encryption. The application encrypts each transaction locally by using the AWS Key Management Service (AWS KMS) GenerateDataKey operation. A developer is testing the application and receives a ThrottlingException error.<><>Which actions are best practices to resolve this error? (Choose two.)",
    "widget": "CM",
    "answers": [
      {
        "content": "Use the LocalCryptoMatenalsCache feature of the AWS Encryption SDK encryption library.",
        "is_correct": true
      },
      {
        "content": "Call the AWS KMS Encrypt operation directly to allow AWS KMS to encrypt the data.",
        "is_correct": false
      },
      {
        "content": "Create a case in the AWS Support Center to increase the quota for the account.",
        "is_correct": true
      },
      {
        "content": "Use Amazon Simple Queue Service (Amazon SQS) to queue the requests to AWS KMS.",
        "is_correct": false
      },
      {
        "content": "Switch to an AWS KMS custom key store.",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "[See docs](https://aws.amazon.com/premiumsupport/knowledge-center/kms-throttlingexception-error/)."
    }
  },
  {
    "content": "A developer has code that is stored in an Amazon S3 bucket. The code must be deployed as an AWS Lambda function across multiple accounts in the same AWS Region as the S3 bucket. An AWS CloudFormation template that runs for each account will deploy the Lambda function.<><>What is the MOST secure way to allow CloudFormation to access the Lambda code in the S3 bucket?",
    "widget": "CI",
    "answers": [
      {
        "content": "Grant the CloudFormation service role the S3 ListBucket and GetObject permissions. Add a bucket policy to Amazon S3 with the pnncipal of 'AWS': [account numbers].",
        "is_correct": true
      },
      {
        "content": "Grant the CloudFormation service role the S3 GetObject permission. Add a bucket policy to Amazon S3 with the principal of '*'.",
        "is_correct": false
      },
      {
        "content": "Use a service-based link to grant the Lambda function the S3 ListBucket and GetObject permissions by explicitly adding the S3 bucket’s account number in the resource.",
        "is_correct": false
      },
      {
        "content": "Use a service-based link to grant the Lambda function the S3 GetObject permission. Add a resource of '*' to allow access to the S3 bucket.",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": ""
    }
  },
  {
    "content": "A company is migrating a legacy application to a serverless application on AWS. The legacy application consists of a set of web services that are exposed by an Amazon API Gateway API. A developer needs to replace the existing implementation of web services with AWS Lambda functions. The developer needs to test a new version of the API that uses the functions in production. The developer must minimize the impact of the testing on the application's users.<><>Which solution will meet these requirements?",
    "widget": "CI",
    "answers": [
      {
        "content": "Create a beta stage for the new version of the API. Send the updated endpoint to the users.",
        "is_correct": false
      },
      {
        "content": "Create a development stage for the new version of the API. Use a canary deployment.",
        "is_correct": true
      },
      {
        "content": "Create a development stage for the new version of the API. Promote a canary release.",
        "is_correct": false
      },
      {
        "content": "Create a deployment stage. Enable mutual TLS for the new version of the API.",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "Canary deployment. The question is asking for a solution that will have the minimum impact on users.<><>Create a beta stage for the new version of the API. Send the updated endpoint to the users. -- why would you send your beta version to the users? And if you do, what sort of an impact would it have on the users if something is wrong with the beta version?"
    }
  },
  {
    "content": "A developer needs to modify an application architecture to meet new functional requirements. Application data is stored in Amazon DynamoDB and processed for analysis in a nightly batch. The system analysts do not want to wait until the next day to view the processed data and have asked to have it available in near-real time.<><>Which application architecture pattern would enable the data to be processed as it is received?",
    "widget": "CI",
    "answers": [
      {
        "content": "Event driven",
        "is_correct": true
      },
      {
        "content": "Client-server driven",
        "is_correct": false
      },
      {
        "content": "Fan-out driven",
        "is_correct": false
      },
      {
        "content": "Schedule driven",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "Event driven = real time"
    }
  },
  {
    "content": "A company is migrating a web application from on premises to AWS. The company needs to move session storage from the application code to a shared service as part of the migration. The session storage data must be encrypted at rest.<><>Which AWS services meet these requirements? (Choose two.)",
    "widget": "CM",
    "answers": [
      {
        "content": "Amazon ElastiCache for Redis",
        "is_correct": true
      },
      {
        "content": "Amazon ElastiCache for Memcached",
        "is_correct": false
      },
      {
        "content": "Amazon CloudWatch",
        "is_correct": false
      },
      {
        "content": "AWS CloudTrail",
        "is_correct": false
      },
      {
        "content": "Amazon DynamoDB",
        "is_correct": true
      }
    ],
    "explanation": {
      "content": "ElastiCache for Redis is always a good option as a distributed cache for session management. It also supports encrypt at rest. [See](https://aws.amazon.com/getting-started/hands-on/building-fast-session-caching-with-amazon-elasticache-for-redis/)<><>DynamoDB is also common to store session state with TTL support. And all user data stored in Amazon DynamoDB is fully encrypted at rest. [See](https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/EncryptionAtRest.html)"
    }
  },
  {
    "content": "A company is hosting a workshop for external users and wants to share the reference documents with the external users for 7 days. The company stores the reference documents in an Amazon S3 bucket that the company owns.<><>What is the MOST secure way to share the documents with the external users?",
    "widget": "CI",
    "answers": [
      {
        "content": "Use S3 presigned URLs to share the documents with the external users. Set an expiration time of 7 days.",
        "is_correct": true
      },
      {
        "content": "Move the documents to an Amazon WorkDocs folder Share the links of the WorkDocs folder with the external users.",
        "is_correct": false
      },
      {
        "content": "Create temporary IAM users that have read-only access to the S3 bucket. Share the access keys with the external users. Expire the credentials after 7 days.",
        "is_correct": false
      },
      {
        "content": "Create a role that has read-only access to the S3 bucket. Share the Amazon Resource Name (ARN) of this role with the external users.",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "[See docs](https://docs.aws.amazon.com/AmazonS3/latest/userguide/ShareObjectPreSignedURL.html)."
    }
  },
  {
    "content": "A developer is storing many objects in a single Amazon S3 bucket. The developer needs to optimize the S3 bucket for high request rates.<><>How should the developer store the objects to meet this requirement?",
    "widget": "CI",
    "answers": [
      {
        "content": "Store the objects by using S3 Intelligent-Tiering.",
        "is_correct": false
      },
      {
        "content": "Store the objects at the root of the S3 bucket.",
        "is_correct": false
      },
      {
        "content": "Store the objects by using object key names distributed across multiple prefixes.",
        "is_correct": true
      },
      {
        "content": "Store each object with an object tag named 'prefix' that contains a unique value.",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "Prefix should be a correct answer: For example, your application can achieve at least 3,500 PUT/COPY/POST/DELETE or 5,500 GET/HEAD requests per second per partitioned prefix. There are no limits to the number of prefixes in a bucket. You can increase your read or write performance by using parallelization. For example, if you create 10 prefixes in an Amazon S3 bucket to parallelize reads, you could scale your read performance to 55,000 read requests per second."
    }
  },
  {
    "content": "A company has a serverless application that uses AWS Lambda functions and AWS Systems Manager parameters to store configuration data. The company moves the Lambda functions inside the VPC and into private subnets. The Lambda functions are now producing errors in their attempts to access Systems Manager parameters.<><>Which solution will allow the Lambda functions to access Systems Manager parameters inside the VPC?",
    "widget": "CI",
    "answers": [
      {
        "content": "Configure security groups to allow access to Systems Manager.",
        "is_correct": false
      },
      {
        "content": "Create an interface VPC endpoint for Systems Manager.",
        "is_correct": true
      },
      {
        "content": "Use an Internet gateway from inside the VPC.",
        "is_correct": false
      },
      {
        "content": "Create a gateway VPC endpoint for Systems Manager.",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "Gateway endpoints only support s3 and dynamodb. Set up an interface endpoint in your Amazon VPC that allows your function to access Systems Manager."
    }
  },
  {
    "content": "A research company has a website that is used once each day to perform scientific calculations based on inputs that are submitted through a webpage. The calculations are CPU intensive. An AWS Lambda function performs the calculations once each day. Users occasionally receive errors because of Lambda function timeouts.<><>Which change will reduce the Lambda function's runtime duration?",
    "widget": "CI",
    "answers": [
      {
        "content": "Configure Lambda to run the function on an Amazon EC2 burstable instance type.",
        "is_correct": false
      },
      {
        "content": "Configure Lambda to run the function on an Amazon EC2 instance type that is recommended for high performance computing (HPC) workloads.",
        "is_correct": false
      },
      {
        "content": "Configure Lambda to run the function with a larger reserved concurrency value.",
        "is_correct": false
      },
      {
        "content": "Configure Lambda to run the function with a larger memory value.",
        "is_correct": true
      }
    ],
    "explanation": {
      "content": "There are similar questions like this one, the keyword here is CPU intensive -> increase Lamba RAM. [See docs](https://docs.aws.amazon.com/lambda/latest/operatorguide/configurations.html#cpu-bound-config)."
    }
  },
  {
    "content": "A developer is creating an application. New users of the application must be able to create an account and register by using their own social media accounts.<><>Which AWS service or resource should the developer use to meet these requirements?",
    "widget": "CI",
    "answers": [
      {
        "content": "IAM role",
        "is_correct": false
      },
      {
        "content": "Amazon Cognito identity pools",
        "is_correct": false
      },
      {
        "content": "Amazon Cognito user pools",
        "is_correct": true
      },
      {
        "content": "AWS Directory Service",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "Anything related to users creating an account = Cognito user pools"
    }
  },
  {
    "content": "A developer wants to use AWS CodeDeploy to deploy an Amazon Elastic Container Service (Amazon ECS) service.<><>What are the MINIMUM properties required in the 'resources' section of the AppSpec file for CodeDeploy to deploy the ECS service successfully?",
    "widget": "CI",
    "answers": [
      {
        "content": "name, alias currentversion, and targetversion",
        "is_correct": false
      },
      {
        "content": "TaskDefinition, ContainerName, and PlatformVersion",
        "is_correct": false
      },
      {
        "content": "TaskDefimtion, ContainerName, and ContainerPort",
        "is_correct": true
      },
      {
        "content": "name, currentversion, NetworkConfiguration, and PlatformVersion",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "The AppSpec file for an Amazon ECS deployment specifies your task definition, container name, and container port. [See docs](https://docs.aws.amazon.com/codedeploy/latest/userguide/tutorial-ecs-create-appspec-file.html)."
    }
  },
  {
    "content": "A developer is writing an application that stores data in an Amazon DynamoDB table by using the Putltem API operation. The table has a partition key of streamID and has a sort key of seqID. The developer needs to make sure that the Putltem invocation does not overwrite the existing partition key and sort key.<><>Which condition expression will maintain the uniqueness of the partition key and the sort key?",
    "widget": "CI",
    "answers": [
      {
        "content": "condition = 'attribute_not_exists(streamID) AND attribute_not_exists(seqID)'",
        "is_correct": true
      },
      {
        "content": "condition = 'attribute_not_exists(PARTITION) AND attribute_not_exists(SORT)'",
        "is_correct": false
      },
      {
        "content": "condition = 'attribute_exists(streamID) AND attribute_exists(seqID)'",
        "is_correct": false
      },
      {
        "content": "condition = 'attribute_exists(PARTITION) AND attribute_exists(SORT)'",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "aws dynamodb put-item --table-name ProductCatalog --item file://item.json --condition-expression 'attribute_not_exists(Id)' [See docs](https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Expressions.ConditionExpressions.html)"
    }
  },
  {
    "content": "A developer has created an AWS Lambda function that is written in Python. The Lambda function reads data from objects in Amazon S3 and writes data to an Amazon DynamoDB table.<><>The function is successfully invoked from an S3 event notification when an object is created. However, the function fails when it attempts to write to the DynamoDB table.<><>What is the MOST likely cause of this issue?",
    "widget": "CI",
    "answers": [
      {
        "content": "The Lambda function's concurrency limit has been exceeded.",
        "is_correct": false
      },
      {
        "content": "The DynamoDB table requires a global secondary index (GSI) to support writes.",
        "is_correct": false
      },
      {
        "content": "The Lambda function does not have IAM permissions to write to DynamoDB.",
        "is_correct": true
      },
      {
        "content": "The DynamoDB table is not running in the same Availability Zone as the Lambda function.",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "[See docs](https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_examples_lambda-access-dynamodb.html)"
    }
  },
  {
    "content": "A development team wants to build a continuous integration/continuous delivery (CI/CD) pipeline. The team is using AWS CodePipeline to automate the code build and deployment. The team wants to store the program code to prepare for the CI/CD pipeline.<><>Which AWS service should the team use to store the program code?",
    "widget": "CI",
    "answers": [
      {
        "content": "AWS CodeDeploy",
        "is_correct": false
      },
      {
        "content": "AWS CodeArtifact",
        "is_correct": false
      },
      {
        "content": "AWS CodeCommit",
        "is_correct": true
      },
      {
        "content": "Amazon CodeGuru",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "CodeArtifact is not the best choice for storing program code that needs to be version controlled and managed as source code. CodeArtifact is designed to store artifacts and dependencies that are typically produced during the build process, rather than the source code itself. While it is possible to store source code in CodeArtifact, it is not a recommended practice for managing source code repositories.<><>Therefore, if the development team wants to store the program code to prepare for the CI/CD pipeline, AWS CodeCommit is the most suitable AWS service to use for managing and version-controlling the source code repository."
    }
  },
  {
    "content": "A developer supports an application that accesses data in an Amazon DynamoDB table. One of the item attributes is expiration Date in the timestamp format. The application uses this attribute to find items, archive them, and remove them from the table based on the timestamp value.<><>The application will be decommissioned soon, and the developer must find another way to implement this functionality. The developer needs a solution that will require the least amount of code to write.<><>Which solution will meet these requirements?",
    "widget": "CI",
    "answers": [
      {
        "content": "Enable TTL on the expirationDate attribute in the table. Create a DynamoDB stream. Create an AWS Lambda function to process the deleted items. Create a DynamoDB trigger for the Lambda function.",
        "is_correct": true
      },
      {
        "content": "Create two AWS Lambda functions: one to delete the items and one to process the items. Create a DynamoDB stream. Use the DeleteItem API operation to delete the items based on the expirationDate attribute. Use the GetRecords API operation to get the items from the DynamoDB stream and process them.",
        "is_correct": false
      },
      {
        "content": "Create two AWS Lambda functions: one to delete the items and one to process the items. Create an Amazon EventBridge (Amazon CloudWatch Events) scheduled rule to invoke the Lambda functions. Use the DeleteItem API operation to delete the items based on the expirationDate attribute. Use the GetRecords API operation to get the items from the DynamoDB table and process them.",
        "is_correct": false
      },
      {
        "content": "Enable TTL on the expirationDate attribute in the table. Specify an Amazon Simple Queue Service (Amazon SQS) dead-letter queue as the target to delete the items. Create an AWS Lambda function to process the items.",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "TTL and a DynamoDB stream. [See docs](https://aws.amazon.com/blogs/database/automatically-archive-items-to-s3-using-dynamodb-time-to-live-with-aws-lambda-and-amazon-kinesis-firehose/)"
    }
  },
  {
    "content": "A company has developed a new serverless application using AWS Lambda functions that will be deployed using the AWS Serverless Application Model (AWS SAM) CLI.<><>Which step should the developer complete prior to deploying the application?",
    "widget": "CI",
    "answers": [
      {
        "content": "Compress the application to a .zip file and upload it into AWS Lambda.",
        "is_correct": false
      },
      {
        "content": "Test the new AWS Lambda function by first tracing it in AWS X-Ray.",
        "is_correct": false
      },
      {
        "content": "Bundle the serverless application using a SAM package.",
        "is_correct": true
      },
      {
        "content": "Create the application environment using the eb create my-env command.",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "[See docs](https://docs.aws.amazon.com/codedeploy/latest/userguide/tutorial-lambda-sam-deploy-update.html)"
    }
  },
  {
    "content": "A developer is working on an ecommerce website. The developer wants to review server logs without logging in to each of the application servers individually. The website runs on multiple Amazon EC2 instances, is written in Python, and needs to be highly available.<><>How can the developer update the application to meet these requirements with MINIMUM changes?",
    "widget": "CI",
    "answers": [
      {
        "content": "Rewrite the application to be cloud native and to run on AWS Lambda, where the logs can be reviewed in Amazon CloudWatch.",
        "is_correct": false
      },
      {
        "content": "Set up centralized logging by using Amazon OpenSearch Service (Amazon Elasticsearch Service), Logstash, and OpenSearch Dashboards (Kibana).",
        "is_correct": false
      },
      {
        "content": "Scale down the application to one larger EC2 instance where only one instance is recording logs.",
        "is_correct": false
      },
      {
        "content": "Install the unified Amazon CloudWatch agent on the EC2 instances. Configure the agent to push the application logs to CloudWatch.",
        "is_correct": true
      }
    ],
    "explanation": {
      "content": "one approach is to centralize the logs by sending them to a log management and analysis service. AWS offers a service called Amazon CloudWatch Logs that can be used to centralize logs from multiple EC2 instances and provide a unified view of them. [See docs](https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Install-CloudWatch-Agent.html)"
    }
  },
  {
    "content": "How would a developer notify users when a new item is written to a DynamoDB table without affecting the provisioned throughput?",
    "widget": "CI",
    "answers": [
      {
        "content": "Set up a DynamoDB stream to trigger a Lambda function that sends an SNS notification to users.",
        "is_correct": true
      },
      {
        "content": "Schedule an Amazon CloudWatch event to regularly trigger a Lambda function that scans the DynamoDB table.",
        "is_correct": false
      },
      {
        "content": "Run a polling application that queries the DynamoDB table at one-second intervals and send SNS notification to users.",
        "is_correct": false
      },
      {
        "content": "Embed a Lambda notification function in DynamoDB and configure DynamoDB to trigger the embedded Lambda function when changes are made.",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "DynamoDB streams capture a time-ordered sequence of item-level modifications made to a table. By using DynamoDB streams, a developer can enable real-time processing of table data, and trigger an action in response to any change made to the table.<><>In this scenario, the developer can set up a DynamoDB stream on the table and configure a Lambda function to be triggered by new stream records. The Lambda function can then process the data, and send a notification to users via SNS.<><>Using DynamoDB streams and Lambda function does not impact the provisioned throughput of the DynamoDB table, as the stream reads and Lambda function processing are performed asynchronously in the background."
    }
  },
  {
    "content": "A developer is writing a web application that must share secure documents with end users. The documents are stored in a private Amazon S3 bucket. The application must allow only authenticated users to download specific documents when requested, and only for a duration of 15 minutes.<><>How can the developer meet these requirements?",
    "widget": "CI",
    "answers": [
      {
        "content": "Copy the documents to a separate S3 bucket that has a lifecycle policy for deletion after 15 minutes.",
        "is_correct": false
      },
      {
        "content": "Create a presigned S3 URL using the AWS SDK with an expiration time of 15 minutes.",
        "is_correct": true
      },
      {
        "content": "Use server-side encryption with AWS KMS managed keys (SSE-KMS) and download the documents using HTTPS.",
        "is_correct": false
      },
      {
        "content": "Modify the S3 bucket policy to only allow specific users to download the documents. Revert the change after 15 minutes.",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "S3 Presigned URLs for limited time access"
    }
  },
  {
    "content": "A company wants to migrate an existing web application to AWS. The application consists of two web servers and a MySQL database.<><>The company wants the application to automatically scale in response to demand. The company also wants to reduce its operational overhead for database backups and maintenance. The company needs the ability to deploy multiple versions of the application concurrently.<><>What is the MOST operationally efficient solution that meets these requirements?",
    "widget": "CI",
    "answers": [
      {
        "content": "Deploy the application to AWS Elastic Beanstalk. Migrate the database to an Amazon RDS Multi-AZ DB instance.",
        "is_correct": true
      },
      {
        "content": "Create an Amazon Machine Image (AMI) that contains the application code. Create an Auto Scaling group that is based on the AMI. Integrate the Auto Scaling group with an Application Load Balancer for the web servers. Migrate the database to a MySQL instance that runs on an Amazon EC2 instance.",
        "is_correct": false
      },
      {
        "content": "Deploy the application to AWS Elastic Beanstalk. Migrate the database to a MySQL instance that runs on an Amazon EC2 instance.",
        "is_correct": false
      },
      {
        "content": "Create an Amazon Machine Image (AMI) that contains the application code. Create an Auto Scaling group that is based on the AMI. Integrate the Auto Scaling group with an Application Load Balancer for the web servers. Migrate the database to an Amazon RDS Multi-AZ DB instance.",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "ElasticBeanStalk for deploying multiple versions of application concurrently + RDS for database backups and maintenance. [See docs](https://aws.amazon.com/rds/features/backup/)"
    }
  },
  {
    "content": "A financial company must store original customer records for 10 years for legal reasons. A complete record contains personally identifiable information (PII). According to local regulations. PII is available to only certain people in the company and must not be shared with third parties. The company needs to make the records available to third-party organizations for statistical analysis without sharing the PII.<><>A developer wants to store the original immutable record in Amazon S3. Depending on who accesses the S3 document, the document should be returned as is or with all the PII removed. The developer has written an AWS Lambda function to remove the PII from the document. The function is named removePii.<><>What should the developer do so that the company can meet the PII requirements while maintaining only one copy of the document?",
    "widget": "CI",
    "answers": [
      {
        "content": "Set up an S3 event notification that invokes the removePii function when an S3 GET request is made. Call Amazon S3 by using a GET request to access the object without PII.",
        "is_correct": false
      },
      {
        "content": "Set up an S3 event notification that invokes the removePii function when an S3 PUT request is made. Call Amazon S3 by using a PUT request to access the object without PII.",
        "is_correct": false
      },
      {
        "content": "Create an S3 Object Lambda access point from the S3 console. Select the removePii function. Use S3 Access Points to access the object without PII.",
        "is_correct": true
      },
      {
        "content": "Create an S3 access point from the S3 console. Use the access point name to call the GetObjectLegalHold S3 API function. Pass in the removePii function name to access the object without PII.",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "The best solution for the given requirements is to use an S3 Object Lambda function to remove the PII from the document. S3 Object Lambda is a new feature that allows the developer to add custom code to S3 GET requests. The developer can create an S3 Object Lambda function to remove the PII from the document and configure S3 to use the function whenever an object is requested from a specific access point. This way, depending on who accesses the document, the document will either be returned as is or with the PII removed, without having to store multiple copies of the document.<><>The developer should create an S3 Object Lambda access point from the S3 console, select the removePii function, and use S3 Access Points to access the object without PII. The S3 Object Lambda function will automatically remove the PII from the document whenever an object is requested from the access point."
    }
  },
  {
    "content": "A developer is designing an AWS Lambda function that creates temporary files that are less than 10 MB during invocation. The temporary files will be accessed and modified multiple times during invocation. The developer has no need to save or retrieve these files in the future.<><>Where should the temporary files be stored?",
    "widget": "CI",
    "answers": [
      {
        "content": "the /tmp directory",
        "is_correct": true
      },
      {
        "content": "Amazon Elastic File System (Amazon EFS)",
        "is_correct": false
      },
      {
        "content": "Amazon Elastic Block Store (Amazon EBS)",
        "is_correct": false
      },
      {
        "content": "Amazon S3",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "[See docs](https://aws.amazon.com/blogs/compute/choosing-between-aws-lambda-data-storage-options-in-web-apps/)"
    }
  },
  {
    "content": "A developer is building a web and mobile application for two types of users: regular users and guest users. Regular users are required to log in, but guest users do not log in. Users should see only their data, regardless of whether they authenticate. Users need AWS credentials before they can access AWS resources.<><>What is the MOST secure solution that the developer can implement to allow access for guest users?",
    "widget": "CI",
    "answers": [
      {
        "content": "Use an Amazon Cognito credentials provider to issue temporary credentials that are linked to an unauthenticated role that has access to the required resources.",
        "is_correct": true
      },
      {
        "content": "Set up an IAM user that has permissions to the required resources. Hardcode the IAM credentials in the web and mobile application.",
        "is_correct": false
      },
      {
        "content": "Generate temporary keys that are stored in AWS Key Management Service (AWS KMS). Use the temporary keys to access the required resources.",
        "is_correct": false
      },
      {
        "content": "Generate temporary credentials. Store the temporary credentials in AWS Secrets Manager. Use the temporary credentials to access the required resources.",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "The most secure solution that the developer can implement to allow access for guest users is to use an Amazon Cognito credentials provider to issue temporary credentials that are linked to an unauthenticated role that has access to the required resources.<><>Using Amazon Cognito, the developer can create a user pool and an identity pool. The user pool allows for user authentication and management, while the identity pool provides temporary AWS credentials that grant access to AWS resources. By configuring the identity pool to allow unauthenticated users, guest users can obtain temporary AWS credentials without the need for authentication. The developer can also define fine-grained access control policies to ensure that users see only their data.<><>Set up an IAM user that has permissions to the required resources. Hardcode the IAM credentials -- is not a recommended solution as it would require hardcoding of IAM credentials, which is not secure.<><>Generate temporary keys/credentials -- those options do not provide a solution for user authentication and management, which is a requirement for the application."
    }
  },
  {
    "content": "A developer is using AWS Elastic Beanstalk to create a deployment for a web application that supports ecommerce. According to a company requirement. Amazon EC2 instances that host one version of the application must be retired when the deployment of a new version is complete.<><>Which deployment methods can the developer use to meet this requirement? (Choose two.)",
    "widget": "CM",
    "answers": [
      {
        "content": "All-al-once deployment",
        "is_correct": false
      },
      {
        "content": "In-place deployment",
        "is_correct": false
      },
      {
        "content": "Rolling deployment without an additional batch",
        "is_correct": false
      },
      {
        "content": "Blue/green deployment",
        "is_correct": true
      },
      {
        "content": "Immutable deployment",
        "is_correct": true
      }
    ],
    "explanation": {
      "content": "must have 2 at the same time during transition => blue/green + immutable. [See docs](https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html)"
    }
  },
  {
    "content": "A company caches session information for a web application in an Amazon DynamoDB table. The company wants an automated way to delete old items from the table.<><>What is the simplest way to do this?",
    "widget": "CI",
    "answers": [
      {
        "content": "Write a script that deletes old records; schedule the script as a cron job on an Amazon EC2 instance.",
        "is_correct": false
      },
      {
        "content": "Add an attribute with the expiration time; enable the Time To Live feature based on that attribute.",
        "is_correct": true
      },
      {
        "content": "Each day, create a new table to hold session data; delete the previous day’s table.",
        "is_correct": false
      },
      {
        "content": "Add an attribute with the expiration time; name the attribute ItemExpiration.",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "[See docs](https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/howitworks-ttl.html)"
    }
  },
  {
    "content": "A company's new mobile app uses Amazon API Gateway. As the development team completes a new release of its APIs, a developer must safely and transparently roll out the API change.<><>What is the SIMPLEST solution for the developer to use for rolling out the new API version to a limited number of users through API Gateway?",
    "widget": "CI",
    "answers": [
      {
        "content": "Create a new API in API Gateway. Direct a portion of the traffic to the new API using an Amazon Route 53 weighted routing policy.",
        "is_correct": false
      },
      {
        "content": "Validate the new API version and promote it to production during the window of lowest expected utilization.",
        "is_correct": false
      },
      {
        "content": "Implement an Amazon CloudWatch alarm to trigger a rollback if the observed HTTP 500 status code rate exceeds a predetermined threshold.",
        "is_correct": false
      },
      {
        "content": "Use the canary release deployment option in API Gateway. Direct a percentage of the API traffic using the canarySettings setting.",
        "is_correct": true
      }
    ],
    "explanation": {
      "content": "Canary deployment helps to shift the traffic based on percentage. [See docs](https://docs.aws.amazon.com/apigateway/latest/developerguide/canary-release.html)"
    }
  },
  {
    "content": "A developer is designing a serverless application that customers use to select seats for a concert venue. Customers send the ticket requests to an Amazon API Gateway API with an AWS Lambda function that acknowledges the order and generates an order ID. The application includes two additional Lambda functions: one for inventory management and one for payment processing. These two Lambda functions run in parallel and write the order to an Amazon Dynamo DB table.<><>The application must provide seats to customers according to the following requirements. If a seat is accidently sold more than once, the first order that the application received must get the seat. In these cases, the application must process the payment for only the first order. However, if the first order is rejected during payment processing, the second order must get the seat. In these cases, the application must process the payment for the second order.<><>Which solution will meet these requirements?",
    "widget": "CI",
    "answers": [
      {
        "content": "Send the order ID to an Amazon Simple Notification Service (Amazon SNS) FIFO topic that fans out to one Amazon Simple Queue Service (Amazon SQS) FIFO queue for inventory management and another SQS FIFO queue for payment processing.",
        "is_correct": true
      },
      {
        "content": "Change the Lambda function that generates the order ID to initiate the Lambda function for inventory management. Then initiate the Lambda function for payment processing.",
        "is_correct": false
      },
      {
        "content": "Send the order ID to an Amazon Simple Notification Service (Amazon SNS) topic. Subscribe the Lambda functions for inventory management and payment processing to the topic.",
        "is_correct": false
      },
      {
        "content": "Deliver the order ID to an Amazon Simple Queue Service (Amazon SQS) queue. Configure the Lambda functions for inventory management and payment processing to poll the queue.",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "SNS FIFO topics and SQS FIFO queues working together with the fanout functionality is the way to go. These two together to provide strict message ordering and message deduplication."
    }
  },
  {
    "content": "A developer wants to implement authentication using Amazon Cognito user pools for an existing API in Amazon API Gateway. After creating the Amazon Cognito user pool, the developer tests the GET request to the API. Unauthenticated requests to the API return a 200 OK status response.<><>Which combination of additional steps are required to complete the authentication implementation? (Choose two.)",
    "widget": "CM",
    "answers": [
      {
        "content": "Create an Amazon Cognito authorizer in API Gateway and specify the Amazon Cognito user pool.",
        "is_correct": true
      },
      {
        "content": "Create an AWS Lambda authorizer in API Gateway and specify the Amazon Cognito user pool.",
        "is_correct": false
      },
      {
        "content": "Specify the authorizer in the GET method section of API Gateway and redeploy the API",
        "is_correct": true
      },
      {
        "content": "Use Amazon Cognito user pools to make and authenticate the request to API Gateway.",
        "is_correct": false
      },
      {
        "content": "Create an Amazon Cognito authorizer in API Gateway and specify the Amazon Cognito identity pool.",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "After the user pool is created in Cognito, an authorizer will need to be created in API Gateway for Cognito (not Lambda) specifying a user pool. Then, redeploying the API after the changes will allow the changes to take effect.<><>[See docs for ApiGateway Cognito](https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-integrate-with-cognito.html) and [see docs Cognito Authorizer](https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-use-lambda-authorizer.html)"
    }
  },
  {
    "content": "A developer is creating a command line script to launch an Amazon EC2 instance at a preset time with a cron job. The developer will provide a user data script to start a task and then terminate the instance. The task cannot be interrupted and must run to completion.<><>How should the developer launch the EC2 instance?",
    "widget": "CI",
    "answers": [
      {
        "content": "Use the ec2 start-instances command.",
        "is_correct": false
      },
      {
        "content": "Use the ec2 request-spot-instances command.",
        "is_correct": false
      },
      {
        "content": "Use the ec2 run-instances command.",
        "is_correct": true
      },
      {
        "content": "Use the ec2 purchase-scheduled-instances command.",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "The ec2 run-instances command is used to launch a new EC2 instance. It allows you to specify the Amazon Machine Image (AMI) to use, instance type, security group, and user data script. In this case, the developer can specify the user data script that starts the task and terminates the instance after the task is completed.<><>The ec2 start-instances command is used to start a stopped instance, not to launch a new one.<><>The ec2 request-spot-instances command is used to request EC2 instances at a lower price than the on-demand price. This is not relevant to this scenario as there is no mention of needing to save costs.<><>The ec2 purchase-scheduled-instances command is used to purchase EC2 instances that are scheduled to launch in the future. This is also not relevant to this scenario as the instance needs to be launched at a preset time with a cron job."
    }
  },
  {
    "content": "A developer is creating an Amazon DynamoDB table. The entire table must be encrypted at rest.<><>Which solution will meet this requirement MOST cost-effectively?",
    "widget": "CI",
    "answers": [
      {
        "content": "Create the DynamoDB table by using default encryption settings.",
        "is_correct": true
      },
      {
        "content": "Encrypt the data by using the DynamoDB Encryption Client.",
        "is_correct": false
      },
      {
        "content": "During creation of the DynamoDB table, configure encryption at rest with an AWS Key Management Service (AWS KMS) AWS managed key.",
        "is_correct": false
      },
      {
        "content": "During creation of the DynamoDB table, configure encryption at rest with an AWS Key Management Service (AWS KMS) customer managed key.",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "Creating a DynamoDB table by using default encryption settings will automatically encrypt the entire table at rest without any additional configuration. This option is the most cost-effective as there are no additional costs for configuring encryption or using additional services.<><>Using the DynamoDB Encryption Client, provides client-side encryption to encrypt data before it is sent to DynamoDB. While this option also encrypts the entire table at rest, it requires additional effort and complexity to implement and manage.<><>Configuring encryption at rest with an AWS KMS AWS managed key, is a good solution for greater control over encryption keys and to meet specific compliance requirements. However, this option incurs additional costs for using AWS KMS.<><>Configuring encryption at rest with an AWS KMS customer managed key, provides even greater control over encryption keys, but also incurs additional costs for using AWS KMS and managing the customer-managed key."
    }
  },
  {
    "content": "A company configures an Amazon S3 bucket to deliver S3 object events to Amazon EventBridge (Amazon CloudWatch Events). An EventBridge rule invokes an AWS Lambda function for each object event that is received from the S3 bucket.<><>A developer is working on a new version of the Lambda function. To ensure that the new Lambda function works as expected, the developer must run a repeatable test that uses realistic S3 bucket object events. The developer must minimize the amount of code and infrastructure that are required to support the test.<><>Which solution will meet these requirements?",
    "widget": "CI",
    "answers": [
      {
        "content": "Create another S3 bucket that can deliver object events to EventBridge. Add another EventBridge rule to deliver data events from the new S3 bucket to the new Lambda function Develop a tool to update objects in the new S3 bucket to produce the test S3 object events.",
        "is_correct": false
      },
      {
        "content": "Add the new Lambda function as an additional target of the existing EventBridge rule. Deliver the S3 object events to the existing Lambda function and the new Lambda function simultaneously.",
        "is_correct": false
      },
      {
        "content": "Use EventBridge to archive and replay production S3 object events. Set up a new EventBridge rule to deliver replayed S3 object events to the new Lambda function.",
        "is_correct": true
      },
      {
        "content": "Develop a tool that uses the EventBridge PutEvents API operation to publish aws.s3 data events. Add a new EventBridge rule that delivers the aws.s3 events to the new Lambda function.",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "Use EventBridge to archive and replay production S3 object events, allowing the developer to test the new Lambda function with realistic events. This option requires minimal code and infrastructure changes and ensures that the test data is representative of real-world scenarios."
    }
  },
  {
    "content": "A developer has built an application running on AWS Lambda using AWS Serverless Application Model (AWS SAM).<>What is the correct sequence of steps to successfully deploy the application?",
    "widget": "CI",
    "answers": [
      {
        "content": "1. Build the SAM template in Amazon EC2. 2. Package the SAM template to Amazon EBS storage. 3. Deploy the SAM template from Amazon EBS.",
        "is_correct": false
      },
      {
        "content": "1. Build the SAM template locally. 2. Package the SAM template onto Amazon S3. 3. Deploy the SAM template from Amazon S3.",
        "is_correct": true
      },
      {
        "content": "1. Build the SAM template locally. 2. Deploy the SAM template from Amazon S3. 3. Package the SAM template for use.",
        "is_correct": false
      },
      {
        "content": "1. Build the SAM template locally. 2. Package the SAM template from AWS CodeCommit. 3. Deploy the SAM template to CodeCommit.",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": ""
    }
  },
  {
    "content": "A team of developers is using an AWS CodePipeline pipeline as a continuous integration and continuous delivery (CI/CD) mechanism for a web application. A developer has written unit tests to programmatically test the functionality of the application code. The unit tests produce a test report that shows the results of each individual check. The developer now wants to run these tests automatically during the CI/CD process.<>Which solution will meet this requirement with the LEAST operational effort?",
    "widget": "CI",
    "answers": [
      {
        "content": "Write a Git pre-commit hook that runs the tests before every commit. Ensure that each developer who is working on the project has the pre-commit hook installed locally. Review the test report and resolve any issues before pushing changes to AWS CodeCommit.",
        "is_correct": false
      },
      {
        "content": "Add a new stage to the pipeline. Use AWS CodeBuild as the provider. Add the new stage after the stage that deploys code revisions to the test environment. Write a buildspec that fails the CodeBuild stage if any test does not pass. Use the test reports feature of CodeBuild to integrate the report with the CodeBuild console. View the test results in CodeBuild. Resolve any issues.",
        "is_correct": false
      },
      {
        "content": "Add a new stage to the pipeline. Use AWS CodeBuild as the provider. Add the new stage before the stage that deploys code revisions to the test environment. Write a buildspec that fails the CodeBuild stage if any test does not pass. Use the test reports feature of CodeBuild to integrate the report with the CodeBuild console. View the test results in CodeBuild. Resolve any issues.",
        "is_correct": true
      },
      {
        "content": "Add a new stage to the pipeline. Use Jenkins as the provider. Configure CodePipeline to use Jenkins to run the unit tests. Write a Jenkinsfile that fails the stage if any test does not pass. Use the test report plugin for Jenkins to integrate the report with the Jenkins dashboard. View the test results in Jenkins. Resolve any issues.",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "It is generally recommended to run unit tests before deploying code revisions to the test environment. This allows you to catch and fix any issues in the code before they are deployed to the test environment and potentially cause problems for other team members or stakeholders who are relying on that environment.<><>Adding a stage **before** answers to the question 'Which solution will meet this requirement with the LEAST operational effort?' as it adds a new stage to the pipeline using AWS CodeBuild as the provider and runs the unit tests before deploying the code revisions to the test environment. This approach allows you to catch and fix any issues with the code before they are deployed to the test environment and helps to ensure that your test environment remains stable and reliable for other team members or stakeholders to use."
    }
  },
  {
    "content": "A game stores user game data in an Amazon DynamoDB table. Individual users should not have access to other users' game data.<>How can this be accomplished?",
    "widget": "CI",
    "answers": [
      {
        "content": "Encrypt the game data with individual user keys.",
        "is_correct": false
      },
      {
        "content": "Restrict access to specific items based on certain primary key values.",
        "is_correct": true
      },
      {
        "content": "Stage data in SQS queues to inject metadata before accessing DynamoDB.",
        "is_correct": false
      },
      {
        "content": "Read records from DynamoDB and discard irrelevant data client-side.",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": ""
    }
  },
  {
    "content": "A developer is creating an application that will give users the ability to store photos from their cellphones in the cloud. The application needs to support tens of thousands of users. The application uses an Amazon API Gateway REST API that is integrated with AWS Lambda functions to process the photos. The application stores details about the photos in Amazon DynamoDB.<>Users need to create an account to access the application. In the application, users must be able to upload photos and retrieve previously uploaded photos. The photos will range in size from 300 KB to 5 MB.<>Which solution will meet these requirements with the LEAST operational overhead?",
    "widget": "CI",
    "answers": [
      {
        "content": "Use Amazon Cognito user pools to manage user accounts. Create an Amazon Cognito user pool authorizer in API Gateway to control access to the API. Use the Lambda function to store the photos and details in the DynamoDB table. Retrieve previously uploaded photos directly from the DynamoDB table.",
        "is_correct": false
      },
      {
        "content": "Use Amazon Cognito user pools to manage user accounts. Create an Amazon Cognito user pool authorizer in API Gateway to control access to the API. Use the Lambda function to store the photos in Amazon S3. Store the object's S3 key as part of the photo details in the DynamoDB table. Retrieve previously uploaded photos by querying DynamoDB for the S3 key.",
        "is_correct": true
      },
      {
        "content": "Create an IAM user for each user of the application during the sign-up process. Use IAM authentication to access the API Gateway API. Use the Lambda function to store the photos in Amazon S3. Store the object's S3 key as part of the photo details in the DynamoDB table. Retrieve previously uploaded photos by querying DynamoDB for the S3 key.",
        "is_correct": false
      },
      {
        "content": "Create a user’s table in DynamoDB. Use the table to manage user accounts. Create a Lambda authorizer that validates user credentials against the users table. Integrate the Lambda authorizer with API Gateway to control access to the API. Use the Lambda function to store the photos in Amazon S3. Store the object's S3 key as part of the photo details in the DynamoDB table. Retrieve previously uploaded photos by querying DynamoDB for the S3 key.",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "max size of DynamoDB item is 400KB."
    }
  },
  {
    "content": "A developer is building a three-tier web application that should be able to handle a minimum of 5000 requests per minute. Requirements state that the web tier should be completely stateless while the application maintains session state for the users.<>How can session data be externalized, keeping latency at the LOWEST possible value?",
    "widget": "CI",
    "answers": [
      {
        "content": "Create an Amazon RDS instance, then implement session handling at the application level to leverage a database inside the RDS database instance for session data storage.",
        "is_correct": false
      },
      {
        "content": "Implement a shared file system solution across the underlying Amazon EC2 instances, then implement session handling at the application level to leverage the shared file system for session data storage.",
        "is_correct": false
      },
      {
        "content": "Create an Amazon ElastiCache Memcached cluster, then implement session handling at the application level to leverage the cluster for session data storage.",
        "is_correct": true
      },
      {
        "content": "Create an Amazon DynamoDB table, then implement session handling at the application level to leverage the table for session data storage.",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "Which is faster DynamoDB or ElastiCache? Compared to traditional databases, which are usually double-digit milliseconds, ElastiCache is much faster and serves the purpose of an in-memory data store."
    }
  },
  {
    "content": "A company is using an Amazon API Gateway REST API endpoint as a webhook to publish events from an on-premises source control management (SCM) system to Amazon EventBridge (Amazon CloudWatch Events). The company has configured an EventBridge (CloudWatch Events) rule to listen for the events and to control application deployment in a central AWS account. The company needs to receive the same events across multiple receiver AWS accounts.<>How can a developer meet these requirements without changing the configuration of the SCM system?",
    "widget": "CI",
    "answers": [
      {
        "content": "Deploy the API Gateway REST API to all the required AWS accounts. Use the same custom domain name for all the gateway endpoints so that a single SCM webhook can be used for all events from all accounts.",
        "is_correct": false
      },
      {
        "content": "Deploy the API Gateway REST API to all the receiver AWS accounts. Create as many SCM webhooks as the number of AWS accounts.",
        "is_correct": false
      },
      {
        "content": "Grant permission to the central AWS account for EventBridge (CloudWatch Events)to access the receiver AWS accounts. Add an EventBridge (CloudWatch Events) event bus on the receiver AWS accounts as the targets to the existing EventBridge (CloudWatch Events) rule.",
        "is_correct": true
      },
      {
        "content": "Convert the API Gateway type from REST API to HTTP API.",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "[See docs](https://docs.aws.amazon.com/eventbridge/latest/userguide/eb-cross-account.html)"
    }
  },
  {
    "content": "A company is developing an application that will be accessed through the Amazon API Gateway REST API. Registered users should be the only ones who can access certain resources of this API. The token being used should expire automatically and needs to be refreshed periodically.<>How can a developer meet these requirements?",
    "widget": "CI",
    "answers": [
      {
        "content": "Create an Amazon Cognito identity pool, configure the Amazon Cognito Authorizer in API Gateway, and use the temporary credentials generated by the identity pool.",
        "is_correct": false
      },
      {
        "content": "Create and maintain a database record for each user with a corresponding token and use an AWS Lambda authorizer in API Gateway.",
        "is_correct": false
      },
      {
        "content": "Create an Amazon Cognito user pool, configure the Cognito Authorizer in API Gateway, and use the identity or access token.",
        "is_correct": true
      },
      {
        "content": "Create an IAM user for each API user, attach an invoke permissions policy to the API, and use an IAM authorizer in API Gateway.",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "Create an Amazon Cognito user pool, configure the Cognito Authorizer in API Gateway, and use the identity or access token.<><>This option meets all the requirements of the question. By creating an Amazon Cognito User Pool, the developer can manage user authentication and authorization in a scalable and secure way. The Cognito Authorizer in API Gateway allows you to control access to your REST API endpoints based on user authentication, and the access or identity token can be used to authenticate and authorize users.<><>Furthermore, Cognito User Pools support token expiration and refresh, so you can configure token lifetimes and refresh tokens periodically to ensure that access to the API is secure and users are authenticated."
    }
  },
  {
    "content": "A developer maintains a critical business application that uses Amazon DynamoDB as the primary data store. The DynamoDB table contains millions of documents and receives 30-60 requests each minute. The developer needs to perform processing in near-real time on the documents when they are added or updated in the DynamoDB table.<>How can the developer implement this feature with the LEAST amount of change to the existing application code?",
    "widget": "CI",
    "answers": [
      {
        "content": "Set up a cron job on an Amazon EC2 instance. Run a script every hour to query the table for changes and process the documents.",
        "is_correct": false
      },
      {
        "content": "Enable a DynamoDB stream on the table. Invoke an AWS Lambda function to process the documents.",
        "is_correct": true
      },
      {
        "content": "Update the application to send a PutEvents request to Amazon EventBridge (Amazon CloudWatch Events). Create an EventBridge (CloudWatch Events) rule to invoke an AWS Lambda function to process the documents.",
        "is_correct": false
      },
      {
        "content": "Update the application to synchronously process the documents directly after the DynamoDB write.",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "[See docs](https://aws.amazon.com/blogs/database/dynamodb-streams-use-cases-and-design-patterns/)"
    }
  },
  {
    "content": "A web application is using Amazon Kinesis Streams for clickstream data that may not be consumed for up to 12 hours.<>How can the developer implement encryption at rest for data within the Kinesis Streams?",
    "widget": "CI",
    "answers": [
      {
        "content": "Enable SSL connections to Kinesis.",
        "is_correct": false
      },
      {
        "content": "Use Amazon Kinesis Consumer Library.",
        "is_correct": false
      },
      {
        "content": "Encrypt the data once it is at rest with a Lambda function.",
        "is_correct": false
      },
      {
        "content": "Enable server-side encryption in Kinesis Streams.",
        "is_correct": true
      }
    ],
    "explanation": {
      "content": "[See docs](https://docs.aws.amazon.com/streams/latest/dev/server-side-encryption.html)"
    }
  },
  {
    "content": "A developer is building a highly secure healthcare application using serverless components. This application requires writing temporary data to /tmp storage on an AWS Lambda function.<>How should the developer encrypt this data?",
    "widget": "CI",
    "answers": [
      {
        "content": "Enable Amazon EBS volume encryption with an AWS KMS key in the Lambda function configuration so that all storage attached to the Lambda function is encrypted.",
        "is_correct": false
      },
      {
        "content": "Set up the Lambda function with a role and key policy to access an AWS KMS key. Use the key to generate a data key used to encrypt all data prior to writing to /tmp storage.",
        "is_correct": true
      },
      {
        "content": "Use OpenSSL to generate a symmetric encryption key on Lambda startup. Use this key to encrypt the data prior to writing to /tmp.",
        "is_correct": false
      },
      {
        "content": "Use an on-premises hardware security module (HSM) to generate keys, where the Lambda function requests a data key from the HSM and uses that to encrypt data on all requests to the function.",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "[See docs](https://aws.amazon.com/blogs/compute/using-larger-ephemeral-storage-for-aws-lambda/)"
    }
  },
  {
    "content": "A developer creates a web service that performs many critical activities. The web service code uses an AWS SDK to publish noncritical metrics to Amazon CloudWatch by using the PutMetricData API. The web service must return results to the caller as quickly as possible. The response data from the PutMetricData API is not necessary to create the web service response.<>Which solution will MOST improve the response time of the web service?",
    "widget": "CI",
    "answers": [
      {
        "content": "Upgrade to the latest version of the AWS SDK.",
        "is_correct": false
      },
      {
        "content": "Call the PutMetricData API in a background thread.",
        "is_correct": false
      },
      {
        "content": "Use the AWS SDK to perform a synchronous call to an AWS Lambda function. Call the PutMetricData API within the Lambda function.",
        "is_correct": false
      },
      {
        "content": "Send metric data to an Amazon Simple Queue Service (Amazon SQS) queue. Configure an AWS Lambda function with the queue as the event source. Call the PutMetricData API within the Lambda function.",
        "is_correct": true
      }
    ],
    "explanation": {
      "content": "PutMetricData can handle 500 transactions per second (TPS), which is the maximum number of operation requests that you can make per second without being throttled. Using SQS will make our request async. We will send the metric to a queue to be handled by another Lambda function. We won't have to wait for the answer from CloudWatch. This will improve response time.<><>Call the PutMetricData API in a background thread -- If a background thread is started within a Lambda function and any work is still running after the lambda timeout expires, the thread will be forcibly terminated. Starting a background thread within a Lambda function can be a useful technique, but it's important to be aware of the potential implications on function execution time."
    }
  },
  {
    "content": "A developer is creating a Ruby application and needs to automate the deployment, scaling, and management of an environment without requiring knowledge of the underlying infrastructure.<>Which service would best accomplish this task?",
    "widget": "CI",
    "answers": [
      {
        "content": "AWS CodeDeploy",
        "is_correct": false
      },
      {
        "content": "AWS CloudFormation",
        "is_correct": false
      },
      {
        "content": "AWS OpsWorks",
        "is_correct": false
      },
      {
        "content": "AWS Elastic Beanstalk",
        "is_correct": true
      }
    ],
    "explanation": {
      "content": "The keyword here is 'without requiring knowledge of the underlying infrastructure'. This should immediately ring the bell for you - it's ElasticBeanstalk. [See docs](https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/create_deploy_Ruby.html)"
    }
  },
  {
    "content": "A company has an Amazon S3 bucket that contains sensitive data. The data must be encrypted in transit and at rest. The company encrypts the data in the S3 bucket by using an AWS Key Management Service (AWS KMS) key. A developer needs to grant several other AWS accounts the permission to use the S3 GetObject operation to retrieve the data from the S3 bucket.<>How can the developer enforce that all requests to retrieve the data provide encryption in transit?",
    "widget": "CI",
    "answers": [
      {
        "content": "Define a resource-based policy on the S3 bucket to deny access when a request meets the condition ‘aws:SecureTransport’: ‘false’.",
        "is_correct": true
      },
      {
        "content": "Define a resource-based policy on the S3 bucket to allow access when a request meets the condition ‘aws:SecureTransport’: ‘false’.",
        "is_correct": false
      },
      {
        "content": "Define a role-based policy on the other accounts' roles to deny access when a request meets the condition of ‘aws:SecureTransport’: ‘false’.",
        "is_correct": false
      },
      {
        "content": "Define a resource-based policy on the KMS key to deny access when a request meets the condition of ‘aws:SecureTransport’: ‘false’.",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "This option is the correct solution to enforce that all requests to retrieve the data provide encryption in transit. By defining a resource-based policy on the S3 bucket, you can specify the 'aws:SecureTransport' condition to deny access to any request that does not use encryption in transit. This ensures that all requests to retrieve the data are encrypted in transit, which is a requirement for the sensitive data stored in the S3 bucket. Option 'role-based policy' is also incorrect because it only applies to the other accounts' roles, and not to all requests to retrieve the data."
    }
  },
  {
    "content": "A developer is deploying an application in the AWS Cloud by using AWS CloudFormation. The application will connect to an existing Amazon RDS database. The hostname of the RDS database is stored in AWS Systems Manager Parameter Store as a plaintext value. The developer needs to incorporate the database hostname into the CloudFormation template to initialize the application when the stack is created.<>How should the developer reference the parameter that contains the database hostname?",
    "widget": "CI",
    "answers": [
      {
        "content": "Use the ssm dynamic reference.",
        "is_correct": true
      },
      {
        "content": "Use the Ref intrinsic function.",
        "is_correct": false
      },
      {
        "content": "Use the Fn::ImportValue intrinsic function.",
        "is_correct": false
      },
      {
        "content": "Use the ssm-secure dynamic reference.",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "Given: stored in as a 'plaintext value'<>ssm: Systems Manager Parameter Store plaintext parameter. [[ ==> Correct answer ]]<>ssm-secure: Systems Manager Parameter Store secure string parameter."
    }
  },
  {
    "content": "A developer is building a new complex application on AWS. The application consists of multiple microservices hosted on Amazon EC2. The developer wants to determine which microservice adds the most latency while handling a request.<>Which method should the developer use to make this determination?",
    "widget": "CI",
    "answers": [
      {
        "content": "Instrument each microservice request using the AWS X-Ray SDK. Examine the annotations associated with the requests.",
        "is_correct": false
      },
      {
        "content": "Instrument each microservice request using the AWS X-Ray SDK. Examine the subsegments associated with the requests.",
        "is_correct": true
      },
      {
        "content": "Instrument each microservice request using the AWS X-Ray SDK. Examine the Amazon CloudWatch EC2 instance metrics associated with the requests.",
        "is_correct": false
      },
      {
        "content": "Instrument each microservice request using the Amazon CloudWatch SDK. Examine the CloudWatch EC2 instance metrics associated with the requests.",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "[See docs](https://docs.aws.amazon.com/xray/latest/devguide/xray-concepts.html#xray-concepts-subsegments)"
    }
  },
  {
    "content": "A developer is working on an AWS Lambda function that accesses Amazon DynamoDB. The Lambda function must retrieve an item and update some of its attributes, or create the item if it does not exist. The Lambda function has access to the primary key.<>Which IAM permissions should the developer request for the Lambda function to achieve this functionality?",
    "widget": "CI",
    "answers": [
      {
        "content": "dynamodb:DeleteItem dynamodb:GetItem dynamodb:PutItem",
        "is_correct": false
      },
      {
        "content": "dynamodb:UpdateItem dynamodb:GetItem dynamodb:DescribeTable",
        "is_correct": false
      },
      {
        "content": "dynamodb:GetRecords dynamodb:PutItem dynamodb:UpdateTable",
        "is_correct": false
      },
      {
        "content": "dynamodb:UpdateItem dynamodb:GetItem dynamodb:PutItem",
        "is_correct": true
      }
    ],
    "explanation": {
      "content": ""
    }
  },
  {
    "content": "A company is using AWS CloudFormation templates to deploy AWS resources. The company needs to update one of its AWS CloudFormation stacks.<>What can the company do to find out how the changes will impact the resources that are running?",
    "widget": "CI",
    "answers": [
      {
        "content": "Investigate the change sets.",
        "is_correct": true
      },
      {
        "content": "Investigate the stack policies.",
        "is_correct": false
      },
      {
        "content": "Investigate the Metadata section.",
        "is_correct": false
      },
      {
        "content": "Investigate the Resources section.",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "[See docs](https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-updating-stacks-changesets.html)"
    }
  },
  {
    "content": "A developer is building an application integrating an Amazon API Gateway with an AWS Lambda function. When calling the API. the developer receives the following error:<><>Wed Nov 08 01:13:00 UTC 2017 : Method completed with status: 502<><>What should the developer do to resolve the error?",
    "widget": "CI",
    "answers": [
      {
        "content": "Change the HTTP endpoint of the API to an HTTPS endpoint.",
        "is_correct": false
      },
      {
        "content": "Change the format of the payload sent to the API Gateway.",
        "is_correct": false
      },
      {
        "content": "Change the format of the Lambda function response to the API call.",
        "is_correct": true
      },
      {
        "content": "Change the authorization header in the API call to access the Lambda function.",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "key = error 5xx is server side"
    }
  },
  {
    "content": "A development team is designing a mobile app that requires multi-factor authentication.<>Which steps should be taken to achieve this? (Choose two.)",
    "widget": "CM",
    "answers": [
      {
        "content": "Use Amazon Cognito to create a user pool and create users in the user pool.",
        "is_correct": true
      },
      {
        "content": "Send multi-factor authentication text codes to users with the Amazon SNS Publish API call in the app code.",
        "is_correct": false
      },
      {
        "content": "Enable multi-factor authentication for the Amazon Cognito user pool.",
        "is_correct": true
      },
      {
        "content": "Use AWS IAM to create IAM users.",
        "is_correct": false
      },
      {
        "content": "Enable multi-factor authentication for the users created in AWS IAM.",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "The first step will be to 'Create the User Pool' in Cognito and the second step will be to 'Enable the MFA'. The question is not asking about the process of how code will be sent. Here is an exam tip: if you get a question mentioning a mobile app or any type of app that has to do with users and authentication, look for Cognito in the answers."
    }
  },
  {
    "content": "A developer is creating an application for a company. The application needs to read the file doc txt that is placed in the root folder of an Amazon S3 bucket that is named DOC-EXAMPLE-BUCKET. The company’s security team requires the principle of least privilege to be applied to the application’s IAM policy.<>Which IAM policy statement will meet these security requirements?",
    "widget": "CI",
    "answers": [
      {
        "content": "{‘Action’: [‘s3:GetObject’], ‘Effect’: ‘Allow’, ‘Resource’: ‘arn:aws:s3:::DOC-EXAMPLE-BUCKET/doc.txt’}",
        "is_correct": true
      },
      {
        "content": "{‘Action’: [‘s3:*], ‘Effect’: ‘Allow’, ‘Resource’: ‘*’}",
        "is_correct": false
      },
      {
        "content": "{‘Action’: [‘s3:GetObject’], ‘Effect’: ‘Allow’, ‘Resource’: ‘arn:aws:s3:::DOC-EXAMPLE-BUCKET/*’}",
        "is_correct": false
      },
      {
        "content": "{‘Action’: [‘s3:*’], ‘Effect’: ‘Allow’, ‘Resource’: ‘arn:aws:s3:::DOC-EXAMPLE-BUCKET/doc.txt’}",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "least privilege"
    }
  },
  {
    "content": "A company has migrated an application to Amazon EC2 instances. Automatic scaling is working well for the application user interface. However, the process to deliver shipping requests to the company's warehouse staff is encountering issues. Duplicate shipping requests are arriving, and some requests are lost or arrive out of order.<>The company must avoid duplicate shipping requests and must process the requests in the order that the requests arrive. Requests are never more than 250 KB in size and take 5-10 minutes to process. A developer needs to rearchitect the application to improve the reliability of the delivery and processing of the requests.<>What should the developer do to meet these requirements?",
    "widget": "CI",
    "answers": [
      {
        "content": "Create an Amazon Kinesis Data Firehose delivery stream to process the requests. Create an Amazon Kinesis data stream. Modify the application to write the requests to the Kinesis data stream.",
        "is_correct": false
      },
      {
        "content": "Create an AWS Lambda function to process the requests. Create an Amazon Simple Notification Service (Amazon SNS) topic. Subscribe the Lambda function to the SNS topic. Modify the application to write the requests to the SNS topic.",
        "is_correct": false
      },
      {
        "content": "Create an AWS Lambda function to process the requests. Create an Amazon Simple Queue Service (Amazon SQS) standard queue. Set the SQS queue as an event source for the Lambda function. Modify the application to write the requests to the SQS queue.",
        "is_correct": false
      },
      {
        "content": "Create an AWS Lambda function to process the requests. Create an Amazon Simple Queue Service (Amazon SQS) FIFO queue. Set the SQS queue as an event source for the Lambda function. Modify the application to write the requests to the SQS queue.",
        "is_correct": true
      }
    ],
    "explanation": {
      "content": "The question is asking 'avoid duplicate shipping requests and must process the requests in the order that the requests arrive' which definitely means it's SQS FIFO. "
    }
  },
  {
    "content": "A team of developers must migrate an application running inside an AWS Elastic Beanstalk environment from a Classic Load Balancer to an Application Load Balancer.<><>Which steps should be taken to accomplish the task using the AWS Management Console?",
    "widget": "CI",
    "answers": [
      {
        "content": "1. Update the application code in the existing deployment. 2. Select a new load balancer type before running the deployment 3. Deploy the new version of the application code to the environment",
        "is_correct": false
      },
      {
        "content": "1. Create a new environment with the same configurations except for the load balancer type. 2. Deploy the same application version as used in the original environment. 3. Run the swap-environment-cnames action",
        "is_correct": true
      },
      {
        "content": "1. Clone the existing environment, changing the associated load balancer type. 2. Deploy the same application version as used in the original environment. 3. Run the swap-environment-cnames action",
        "is_correct": false
      },
      {
        "content": "1. Edit the environment definitions in the existing deployment. 2. Change the associated load balancer type according to the requirements. 3. Rebuild the environment with the new load balancer type.",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "[See docs](https://repost.aws/questions/QUEemsrTM_TPG3r_HdoG08EQ/how-to-remove-or-change-a-load-balancer-type-with-elastic-beanstalk)"
    }
  },
  {
    "content": "What does an Amazon SQS delay queue accomplish?",
    "widget": "CI",
    "answers": [
      {
        "content": "Messages are hidden for a configurable amount of time when they are first added to the queue.",
        "is_correct": true
      },
      {
        "content": "Messages are hidden for a configurable amount of time after they are consumed from the queue.",
        "is_correct": false
      },
      {
        "content": "The consumer can poll the queue for a configurable amount of time before retrieving a message.",
        "is_correct": false
      },
      {
        "content": "Messages cannot be deleted for a configurable amount of time after they are consumed from the queue.",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "[See docs](https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-delay-queues.html)"
    }
  },
  {
    "content": "A company stores the photographs in an Amazon S3 bucket. The company wants to resize the photographs automatically after writing the photographs to the S3 bucket. The company creates an AWS Lambda function to resize the photographs.<>Which solution will meet these requirements?",
    "widget": "CI",
    "answers": [
      {
        "content": "Configure S3 Event Notifications to invoke the Lambda function",
        "is_correct": true
      },
      {
        "content": "Configure an S3 Lifecycle rule to invoke the Lambda function.",
        "is_correct": false
      },
      {
        "content": "Configure S3 Select on a schedule to invoke the Lambda function.",
        "is_correct": false
      },
      {
        "content": "Configure S3 Storage Lens to invoke the Lambda function.",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "You can use Lambda to process event notifications from Amazon Simple Storage Service. Amazon S3 can send an event to a Lambda function when an object is created or deleted. You configure notification settings on a bucket, and grant Amazon S3 permission to invoke a function on the function's resource-based permissions policy."
    }
  },
  {
    "content": "A developer is automating a new application deployment with AWS Serverless Application Model (AWS SAM). The new application has one AWS Lambda function and one Amazon S3 bucket. The Lambda function must access the S3 bucket to only read objects.<>How should the developer configure AWS SAM to grant the necessary read privilege to the S3 bucket?",
    "widget": "CI",
    "answers": [
      {
        "content": "Reference a second Lambda authorizer function.",
        "is_correct": false
      },
      {
        "content": "Add a custom S3 bucket policy to the Lambda function.",
        "is_correct": false
      },
      {
        "content": "Create an Amazon Simple Queue Service (SQS) topic for only S3 object reads Reference the topic in the template.",
        "is_correct": false
      },
      {
        "content": "Add the S3ReadPolicy template to the Lambda function's execution role.",
        "is_correct": true
      }
    ],
    "explanation": {
      "content": "In AWS SAM, the permissions for an AWS Lambda function can be managed through its execution role. To grant the necessary read privilege to the S3 bucket, the developer should add the S3ReadPolicy template to the Lambda function's execution role.<>The S3ReadPolicy template is an AWS managed policy that grants read-only access to an S3 bucket. It includes permissions to list the objects in the bucket and to read the objects themselves. By adding this policy to the Lambda function's execution role, the function will have the necessary permissions to access the S3 bucket to only read objects."
    }
  },
  {
    "content": "A company hosts a web application that writes to an Amazon DynamoDB table. Application users frequently observe and report errors. The development team examines Amazon CloudWatch logs and frequently sees the following error:<>'400 Bad Request ProvisionedThroughputExceededException.'<>What is the cause of this error?",
    "widget": "CI",
    "answers": [
      {
        "content": "The application does not have the required permissions for the DynamoDB table.",
        "is_correct": false
      },
      {
        "content": "The item that the application is placing on the table exceeds the item size limit.",
        "is_correct": false
      },
      {
        "content": "The development team has not allocated enough space for the table and its indexes.",
        "is_correct": false
      },
      {
        "content": "The development team has not allocated enough write capacity units (WCU) for the table and Its indexes.",
        "is_correct": true
      }
    ],
    "explanation": {
      "content": "[See docs](https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Programming.Errors.html)"
    }
  },
  {
    "content": "A company's developer is building a static website to be deployed in Amazon S3 for a production environment. The website integrates with an Amazon Aurora PostgreSQL database by using an AWS Lambda function. The website that is deployed to production will use a Lambda alias that points to a specific version of the Lambda function.<>The company must rotate the database credentials every 2 weeks. Lambda functions that the company deployed previously must be able to use the most recent credentials.<>Which solution will meet these requirements?",
    "widget": "CI",
    "answers": [
      {
        "content": "Store the database credentials in AWS Secrets Manager. Turn on rotation. Write code in the Lambda function to retrieve the credentials from Secrets Manager.",
        "is_correct": true
      },
      {
        "content": "Include the database credentials as part of the Lambda function code. Update the credentials periodically and deploy the new Lambda function.",
        "is_correct": false
      },
      {
        "content": "Use Lambda environment variables. Update the environment variables when new credentials are available.",
        "is_correct": false
      },
      {
        "content": "Store the database credentials in AWS Systems Manager Parameter Store Turn on rotation. Write code in the Lambda function to retrieve the credentials from Systems Manager Parameter Store.",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "Parameter Store doesn't provide automatic rotation. Rotate credentials => secrets manager."
    }
  },
  {
    "content": "An application reads data from an Amazon DynamoDB table. Several times a day, for a period of 15 seconds, me application receives multiple ProvisionedThroughputExceeded errors.<>How should this exception be handled?",
    "widget": "CI",
    "answers": [
      {
        "content": "Create a new global secondary index for the table to help with the additional requests.",
        "is_correct": false
      },
      {
        "content": "Retry the failed read requests with exponential backoff.",
        "is_correct": true
      },
      {
        "content": "Immediately retry the failed read requests.",
        "is_correct": false
      },
      {
        "content": "Use the DynamoDB 'UpdateItem' API to increase the provisioned throughput capacity of the table.",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "ProvisionedThroughputExceeded --> Exponential backoff<><>Use the DynamoDB 'UpdateItem' API to increase the provisioned throughput capacity of the table. -- it will be full of wastage akin to using a sledgehammer to kill an ant as you will be over provisioning your WCU for that few spikes in write requests (read as constant higher cost). By using exponential backoff, you will be able to smoothen out your write request."
    }
  },
  {
    "content": "A company wants to migrate its web application to AWS and leverage auto scaling to handle peak workloads. The solutions architect determined that the best metric for an auto scaling event is the number of concurrent users.<>Based on this information, what should the developer use to auto scale based on concurrent users?",
    "widget": "CI",
    "answers": [
      {
        "content": "An Amazon SNS topic to be invoked when a concurrent user threshold is met",
        "is_correct": false
      },
      {
        "content": "An Amazon Cloudwatch NetworkIn metric",
        "is_correct": false
      },
      {
        "content": "Amazon CloudFront to leverage AWS edge locations",
        "is_correct": false
      },
      {
        "content": "A custom Amazon CloudWatch metric for concurrent users",
        "is_correct": true
      }
    ],
    "explanation": {
      "content": "There is no 'concurrent user threshold' metric for Auto Scaling. You should create custom metric, if you want to scale depending on this value. [See docs](https://docs.aws.amazon.com/connect/latest/adminguide/monitoring-cloudwatch.html)"
    }
  },
  {
    "content": "A company is managing a NoSQL database on-premises to host a critical component of an application, which is starting to have scaling issues. The company wants to migrate the application to Amazon DynamoDB with the following considerations:<><>• Optimize frequent queries<>• Reduce read latencies<>• Plan for frequent queries on certain key attributes of the table<><>Which solution would help achieve these objectives?",
    "widget": "CI",
    "answers": [
      {
        "content": "Create global secondary indexes on keys that are frequently queried. Add the necessary attributes into the indexes.",
        "is_correct": true
      },
      {
        "content": "Create local secondary indexes on keys that are frequently queried. DynamoDB will fetch needed attributes from the table.",
        "is_correct": false
      },
      {
        "content": "Create DynamoDB global tables to speed up query responses. Use a scan to fetch data from the table.",
        "is_correct": false
      },
      {
        "content": "Create an AWS Auto Scaling policy for the DynamoDB table.",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "Global secondary index—An index with a partition key and a sort key that can be different from those on the base table. A global secondary index is considered 'global' because queries on the index can span all of the data in the base table, across all partitions.<><>Local secondary index—An index that has the same partition key as the base table, but a different sort key. A local secondary index is 'local' in the sense that every partition of a local secondary index is scoped to a base table partition that has the same partition key value. [See docs](https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-indexes-general.html)"
    }
  },
  {
    "content": "A developer is storing sensitive data generated by an application in Amazon S3. The developer wants to encrypt the data at rest A company policy requires an audit trail of when the AWS Key Management Service (AWS KMS) key was used and by whom.<><>Which encryption option will meet these requirements?",
    "widget": "CI",
    "answers": [
      {
        "content": "Server-side encryption with Amazon S3 managed keys (SSE-S3)",
        "is_correct": false
      },
      {
        "content": "Server-side encryption with AWS KMS managed keys (SSE-KMS)",
        "is_correct": true
      },
      {
        "content": "Server-side encryption with customer-provided keys (SSE-C)",
        "is_correct": false
      },
      {
        "content": "Server-side encryption with self-managed keys",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "KMS allows auditing"
    }
  },
  {
    "content": "A company is running Amazon EC2 instances in multiple AWS accounts. A developer needs to implement an application that collects all the lifecycle events of the EC2 instances. The application needs to store the lifecycle events in a single Amazon Simple Queue Service (Amazon SQS) queue in the company's main AWS account for further processing.<><>Which solution will meet these requirements?",
    "widget": "CI",
    "answers": [
      {
        "content": "Configure Amazon EC2 to deliver the EC2 instance lifecycle events from all accounts to the Amazon EventBridge event bus of the main account. Add an EventBridge rule to the event bus of the main account that matches all EC2 instance lifecycle events. Add the SQS queue as a target of the rule.",
        "is_correct": false
      },
      {
        "content": "Use the resource policies of the SQS queue in the main account to give each account permissions to write to that SQS queue. Add to the Amazon EventBridge event bus of each account an EventBridge rule that matches all EC2 instance lifecycle events. Add the SQS queue in the main account as a target of the rule.",
        "is_correct": false
      },
      {
        "content": "Write an AWS Lambda function that scans through all EC2 instances in the company accounts to detect EC2 instance lifecycle changes. Configure the Lambda function to write a notification message to the SQS queue in the main account if the function detects an EC2 instance lifecycle change. Add an Amazon EventBridge scheduled rule that invokes the Lambda function every minute.",
        "is_correct": false
      },
      {
        "content": "Configure the permissions on the main account event bus to receive events from all accounts. Create an Amazon EventBridge rule in each account to send all the EC2 instance lifecycle events to the main account event bus. Add an EventBridge rule to the main account event bus that matches all EC2 instance lifecycle events. Set the SQS queue as a target for the rule.",
        "is_correct": true
      }
    ],
    "explanation": {
      "content": "?"
    }
  },
  {
    "content": "An application is using Amazon Cognito user pools and identity pools for secure access. A developer wants to integrate the user-specific file upload and download features in the application with Amazon S3. The developer must ensure that the files are saved and retrieved in a secure manner and that users can access only their own files. The file sizes range from 3 KB to 300 MB.<>Which option will meet these requirements with the HIGHEST level of security?",
    "widget": "CI",
    "answers": [
      {
        "content": "Use S3 Event Notifications to validate the file upload and download requests and update the user interface (UI).",
        "is_correct": false
      },
      {
        "content": "Save the details of the uploaded files in a separate Amazon DynamoDB table. Filter the list of files in the user interface (UI) by comparing the current user ID with the user ID associated with the file in the table.",
        "is_correct": false
      },
      {
        "content": "Use Amazon API Gateway and an AWS Lambda function to upload and download files. Validate each request in the Lambda function before performing the requested operation.",
        "is_correct": false
      },
      {
        "content": "Use an IAM policy within the Amazon Cognito identity prefix to restrict users to use their own folders in Amazon S3.",
        "is_correct": true
      }
    ],
    "explanation": {
      "content": "See [here](https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_examples_s3_cognito-bucket.html) and [here](https://docs.amplify.aws/lib/storage/getting-started/q/platform/js/)"
    }
  },
  {
    "content": "A company is building a scalable data management solution by using AWS services to improve the speed and agility of development. The solution will ingest large volumes of data from various sources and will process this data through multiple business rules and transformations.<>The solution requires business rules to run in sequence and to handle reprocessing of data if errors occur when the business rules run. The company needs the solution to be scalable and to require the least possible maintenance.<>Which AWS service should the company use to manage and automate the orchestration of the data flows to meet these requirements?",
    "widget": "CI",
    "answers": [
      {
        "content": "AWS Batch",
        "is_correct": false
      },
      {
        "content": "AWS Step Functions",
        "is_correct": true
      },
      {
        "content": "AWS Glue",
        "is_correct": false
      },
      {
        "content": "AWS Lambda",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "?"
    }
  },
  {
    "content": "A developer has created an AWS Lambda function that is written in Python. The Lambda function reads data from objects in Amazon S3 and writes data to an Amazon DynamoDB table. The function is successfully invoked from an S3 event notification when an object is created. However, the function fails when it attempts to write to the DynamoDB table.<>What is the MOST likely cause of this issue?",
    "widget": "CI",
    "answers": [
      {
        "content": "The Lambda function's concurrency limit has been exceeded.",
        "is_correct": false
      },
      {
        "content": "DynamoDB table requires a global secondary index (GSI) to support writes.",
        "is_correct": false
      },
      {
        "content": "The Lambda function does not have IAM permissions to write to DynamoDB.",
        "is_correct": true
      },
      {
        "content": "The DynamoDB table is not running in the same Availability Zone as the Lambda function.",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": ""
    }
  },
  {
    "content": "A developer is creating an AWS CloudFormation template to deploy Amazon EC2 instances across multiple AWS accounts. The developer must choose the EC2 instances from a list of approved instance types.<>How can the developer incorporate the list of approved instance types in the CloudFormation template?",
    "widget": "CI",
    "answers": [
      {
        "content": "Create a separate CloudFormation template for each EC2 instance type in the list.",
        "is_correct": false
      },
      {
        "content": "In the Resources section of the CloudFormation template, create resources for each EC2 instance type in the list.",
        "is_correct": false
      },
      {
        "content": "In the CloudFormation template, create a separate parameter for each EC2 instance type in the list.",
        "is_correct": false
      },
      {
        "content": "In the CloudFormation template, create a parameter with the list of EC2 instance types as AllowedValues.",
        "is_correct": true
      }
    ],
    "explanation": {
      "content": "[See docs](https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/parameters-section-structure.html)"
    }
  },
  {
    "content": "A company is running a custom application on a set of on-premises Linux servers that are accessed using Amazon API Gateway. AWS X-Ray tracing has been enabled on the API test stage.<><>How can a developer enable X-Ray tracing on the on-premises servers with the LEAST amount of configuration?",
    "widget": "CI",
    "answers": [
      {
        "content": "Install and run the X-Ray SDK on the on-premises servers to capture and relay the data to the X-Ray service.",
        "is_correct": false
      },
      {
        "content": "Install and run the X-Ray daemon on the on-premises servers to capture and relay the data to the X-Ray service.",
        "is_correct": true
      },
      {
        "content": "Capture incoming requests on-premises and configure an AWS Lambda function to pull, process, and relay relevant data to X-Ray using the PutTraceSegments API call.",
        "is_correct": false
      },
      {
        "content": "Capture incoming requests on-premises and configure an AWS Lambda function to pull, process, and relay relevant data to X-Ray using the PutTelemetryRecords API call.",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "[See docs](https://docs.aws.amazon.com/xray/latest/devguide/xray-daemon.html)"
    }
  },
  {
    "content": "A company wants to share information with a third party. The third party has an HTTP API endpoint that the company can use to share the information. The company has the required API key to access the HTTP API.<>The company needs a way to manage the API key by using code. The integration of the API key with the application code cannot affect application performance.<>Which solution will meet these requirements MOST securely?",
    "widget": "CI",
    "answers": [
      {
        "content": "Store the API credentials in AWS Secrets Manager. Retrieve the API credentials at runtime by using the AWS SDK. Use the credentials to make the API call.",
        "is_correct": true
      },
      {
        "content": "Store the API credentials in a local code variable. Push the code to a secure Git repository. Use the local code variable at runtime to make the API call.",
        "is_correct": false
      },
      {
        "content": "Store the API credentials as an object in a private Amazon S3 bucket. Restrict access to the S3 object by using IAM policies. Retrieve the API credentials at runtime by using the AWS SDK. Use the credentials to make the API call.",
        "is_correct": false
      },
      {
        "content": "Store the API credentials in an Amazon DynamoDB table. Restrict access to the table by using resource-based policies. Retrieve the API credentials at runtime by using the AWS SDK. Use the credentials to make the API call.",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "secrets manager seems most likely since it is meant for storing items like API keys. We should not forget performance is not only the factor but most securely. Hence Secrets Manager access at runtime sounds good"
    }
  },
  {
    "content": "A developer is deploying a new application to Amazon Elastic Container Service (Amazon ECS). The developer needs to securely store and retrieve different types of variables. These variables include authentication information for a remote API, the URL for the API, and credentials. The authentication information and API URL must be available to all current and future deployed versions of the application across development, testing, and production environments.<><>How should the developer retrieve the variables with the FEWEST application changes?",
    "widget": "CI",
    "answers": [
      {
        "content": "Update the application to retrieve the variables from AWS Systems Manager Parameter Store. Use unique paths in Parameter Store for each variable in each environment. Store the credentials in AWS Secrets Manager in each environment.",
        "is_correct": true
      },
      {
        "content": "Update the application to retrieve the variables from AWS Key Management Service (AWS KMS). Store the API URL and credentials as unique keys for each environment.",
        "is_correct": false
      },
      {
        "content": "Update the application to retrieve the variables from an encrypted file that is stored with the application. Store the API URL and credentials in unique files for each environment.",
        "is_correct": false
      },
      {
        "content": "Update the application to retrieve the variables from each of the deployed environments. Define the authentication information and API URL in the ECS task definition as unique names during the deployment process.",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "The application has credentials and URL, so it’s convenient to store them in ssm parameter store restive them."
    }
  },
  {
    "content": "A company is migrating legacy internal applications to AWS. Leadership wants to rewrite the internal employee directory to use native AWS services. A developer needs to create a solution for storing employee contact details and high-resolution photos for use with the new application.<><>Which solution will enable the search and retrieval of each employee's individual details and high-resolution photos using AWS APIs?",
    "widget": "CI",
    "answers": [
      {
        "content": "Encode each employee's contact information and photos using Base64. Store the information in an Amazon DynamoDB table using a sort key.",
        "is_correct": false
      },
      {
        "content": "Store each employee's contact information in an Amazon DynamoDB table along with the object keys for the photos stored in Amazon S3.",
        "is_correct": true
      },
      {
        "content": "Use Amazon Cognito user pools to implement the employee directory in a fully managed software-as-a-service (SaaS) method.",
        "is_correct": false
      },
      {
        "content": "Store employee contact information in an Amazon RDS DB instance with the photos stored in Amazon Elastic File System (Amazon EFS).",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "Dynamo DB & S3. Cognito is also an attractive option here but the problem is it doesn't have a built-in feature for photos and we would need S3.<><>Use Amazon Cognito user pools to implement the employee directory in a fully managed software-as-a-service (SaaS) method -- is a better option if combined with S3 as cognito doesn't offer photo storage on its own."
    }
  },
  {
    "content": "A developer is creating an application that will give users the ability to store photos from their cellphones in the cloud. The application needs to support tens of thousands of users. The application uses an Amazon API Gateway REST API that is integrated with AWS Lambda functions to process the photos. The application stores details about the photos in Amazon DynamoDB.<><>Users need to create an account to access the application. In the application, users must be able to upload photos and retrieve previously uploaded photos. The photos will range in size from 300 KB to 5 MB.<><>Which solution will meet these requirements with the LEAST operational overhead?",
    "widget": "CI",
    "answers": [
      {
        "content": "Use Amazon Cognito user pools to manage user accounts. Create an Amazon Cognito user pool authorizer in API Gateway to control access to the API. Use the Lambda function to store the photos and details in the DynamoDB table. Retrieve previously uploaded photos directly from the DynamoDB table.",
        "is_correct": false
      },
      {
        "content": "Use Amazon Cognito user pools to manage user accounts. Create an Amazon Cognito user pool authorizer in API Gateway to control access to the API. Use the Lambda function to store the photos in Amazon S3. Store the object's S3 key as part of the photo details in the DynamoDB table. Retrieve previously uploaded photos by querying DynamoDB for the S3 key.",
        "is_correct": true
      },
      {
        "content": "Create an IAM user for each user of the application during the sign-up process. Use IAM authentication to access the API Gateway API. Use the Lambda function to store the photos in Amazon S3. Store the object's S3 key as part of the photo details in the DynamoDB table. Retrieve previously uploaded photos by querying DynamoDB for the S3 key.",
        "is_correct": false
      },
      {
        "content": "Create a users table in DynamoDB. Use the table to manage user accounts. Create a Lambda authorizer that validates user credentials against the users table. Integrate the Lambda authorizer with API Gateway to control access to the API. Use the Lambda function to store the photos in Amazon S3. Store the object's S3 key as par of the photo details in the DynamoDB table. Retrieve previously uploaded photos by querying DynamoDB for the S3 key.",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "Item size in dynamodb is less than this scenario"
    }
  },
  {
    "content": "A company receives food orders from multiple partners. The company has a microservices application that uses Amazon API Gateway APIs with AWS Lambda integration. Each partner sends orders by calling a customized API that is exposed through API Gateway. The API call invokes a shared Lambda function to process the orders.<><>Partners need to be notified after the Lambda function processes the orders. Each partner must receive updates for only the partner's own orders. The company wants to add new partners in the future with the fewest code changes possible.<><>Which solution will meet these requirements in the MOST scalable way?",
    "widget": "CI",
    "answers": [
      {
        "content": "Create a different Amazon Simple Notification Service (Amazon SNS) topic for each partner. Configure the Lambda function to publish messages for each partner to the partner's SNS topic.",
        "is_correct": false
      },
      {
        "content": "Create a different Lambda function for each partner. Configure the Lambda function to notify each partner's service endpoint directly.",
        "is_correct": false
      },
      {
        "content": "Create an Amazon Simple Notification Service (Amazon SNS) topic. Configure the Lambda function to publish messages with specific attributes to the SNS topic. Subscribe each partner to the SNS topic. Apply the appropriate filter policy to the topic subscriptions.",
        "is_correct": true
      },
      {
        "content": "Create one Amazon Simple Notification Service (Amazon SNS) topic. Subscribe all partners to the SNS topic.",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "[See docs](https://docs.aws.amazon.com/sns/latest/dg/sns-message-filtering.html)"
    }
  },
  {
    "content": "A financial company must store original customer records for 10 years for legal reasons. A complete record contains personally identifiable information (PII). According to local regulations, PII is available to only certain people in the company and must not be shared with third parties. The company needs to make the records available to third-party organizations for statistical analysis without sharing the PII.<><>A developer wants to store the original immutable record in Amazon S3. Depending on who accesses the S3 document, the document should be returned as is or with all the PII removed. The developer has written an AWS Lambda function to remove the PII from the document. The function is named removePii.<><>What should the developer do so that the company can meet the PII requirements while maintaining only one copy of the document?",
    "widget": "CI",
    "answers": [
      {
        "content": "Set up an S3 event notification that invokes the removePii function when an S3 GET request is made. Call Amazon S3 by using a GET request to access the object without PII.",
        "is_correct": false
      },
      {
        "content": "Set up an S3 event notification that invokes the removePii function when an S3 PUT request is made. Call Amazon S3 by using a PUT request to access the object without PII.",
        "is_correct": false
      },
      {
        "content": "Create an S3 Object Lambda access point from the S3 console. Select the removePii function. Use S3 Access Points to access the object without PII.",
        "is_correct": true
      },
      {
        "content": "Create an S3 access point from the S3 console. Use the access point name to call the GetObjectLegalHold S3 API function. Pass in the removePii function name to access the object without PII.",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "[See docs](https://aws.amazon.com/s3/features/object-lambda/)"
    }
  },
  {
    "content": "A developer is deploying an AWS Lambda function The developer wants the ability to return to older versions of the function quickly and seamlessly.<><>How can the developer achieve this goal with the LEAST operational overhead?",
    "widget": "CI",
    "answers": [
      {
        "content": "Use AWS OpsWorks to perform blue/green deployments.",
        "is_correct": false
      },
      {
        "content": "Use a function alias with different versions.",
        "is_correct": true
      },
      {
        "content": "Maintain deployment packages for older versions in Amazon S3.",
        "is_correct": false
      },
      {
        "content": "Use AWS CodePipeline for deployments and rollbacks.",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "Since this is serverless, you can easily use an older version of a Lambda function using that version's alias."
    }
  },
  {
    "content": "A developer has written an AWS Lambda function. The function is CPU-bound. The developer wants to ensure that the function returns responses quickly.<><>How can the developer improve the function's performance?",
    "widget": "CI",
    "answers": [
      {
        "content": "Increase the function's CPU core count.",
        "is_correct": false
      },
      {
        "content": "Increase the function's memory.",
        "is_correct": true
      },
      {
        "content": "Increase the function's reserved concurrency.",
        "is_correct": false
      },
      {
        "content": "Increase the function's timeout.",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "Cpu utilisation => increase memory"
    }
  },
  {
    "content": "For a deployment using AWS Code Deploy, what is the run order of the hooks for in-place deployments?",
    "widget": "CI",
    "answers": [
      {
        "content": "BeforeInstall -> ApplicationStop -> ApplicationStart -> AfterInstall",
        "is_correct": false
      },
      {
        "content": "ApplicationStop -> BeforeInstall -> AfterInstall -> ApplicationStart",
        "is_correct": true
      },
      {
        "content": "BeforeInstall -> ApplicationStop -> ValidateService -> ApplicationStart",
        "is_correct": false
      },
      {
        "content": "ApplicationStop -> BeforeInstall -> ValidateService -> ApplicationStart",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "[See docs](https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html)"
    }
  },
  {
    "content": "A company is building a serverless application on AWS. The application uses an AWS Lambda function to process customer orders 24 hours a day, 7 days a week. The Lambda function calls an external vendor's HTTP API to process payments.<>During load tests, a developer discovers that the external vendor payment processing API occasionally times out and returns errors. The company expects that some payment processing API calls will return errors.<><>The company wants the support team to receive notifications in near real time only when the payment processing external API error rate exceed 5% of the total number of transactions in an hour. Developers need to use an existing Amazon Simple Notification Service (Amazon SNS) topic that is configured to notify the support team.<><>Which solution will meet these requirements?",
    "widget": "CI",
    "answers": [
      {
        "content": "Write the results of payment processing API calls to Amazon CloudWatch. Use Amazon CloudWatch Logs Insights to query the CloudWatch logs. Schedule the Lambda function to check the CloudWatch logs and notify the existing SNS topic.",
        "is_correct": false
      },
      {
        "content": "Publish custom metrics to CloudWatch that record the failures of the external payment processing API calls. Configure a CloudWatch alarm to notify the existing SNS topic when error rate exceeds the specified rate.",
        "is_correct": true
      },
      {
        "content": "Publish the results of the external payment processing API calls to a new Amazon SNS topic. Subscribe the support team members to the new SNS topic.",
        "is_correct": false
      },
      {
        "content": "Write the results of the external payment processing API calls to Amazon S3. Schedule an Amazon Athena query to run at regular intervals. Configure Athena to send notifications to the existing SNS topic when the error rate exceeds the specified rate.",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "You can use the Embedded Metrics format to embed custom metrics alongside detailed log event data. CloudWatch automatically extracts the custom metrics so you can visualize and alarm on them, for real-time incident detection. [See docs](https://docs.aws.amazon.com/lambda/latest/operatorguide/custom-metrics.html)"
    }
  },
  {
    "content": "A company is offering APIs as a service over the internet to provide unauthenticated read access to statistical information that is updated daily. The company uses Amazon API Gateway and AWS Lambda to develop the APIs. The service has become popular, and the company wants to enhance the responsiveness of the APIs.<><>Which action can help the company achieve this goal?",
    "widget": "CI",
    "answers": [
      {
        "content": "Enable API caching in API Gateway.",
        "is_correct": true
      },
      {
        "content": "Configure API Gateway to use an interface VPC endpoint.",
        "is_correct": false
      },
      {
        "content": "Enable cross-origin resource sharing (CORS) for the APIs.",
        "is_correct": false
      },
      {
        "content": "Configure usage plans and API keys in API Gateway.",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "[See docs](https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-caching.html)"
    }
  },
  {
    "content": "A developer wants to store information about movies. Each movie has a title, release year, and genre. The movie information also can include additional properties about the cast and production crew. This additional information is inconsistent across movies. For example, one movie might have an assistant director, and another movie might have an animal trainer.<>The developer needs to implement a solution to support the following use cases:<><>For a given title and release year, get all details about the movie that has that title and release year.<>For a given title, get all details about all movies that have that title.<>For a given genre, get all details about all movies in that genre.<>Which data store configuration will meet these requirements?",
    "widget": "CI",
    "answers": [
      {
        "content": "Create an Amazon DynamoDB table. Configure the table with a primary key that consists of the title as the partition key and the release year as the sort key. Create a global secondary index that uses the genre as the partition key and the title as the sort key.",
        "is_correct": true
      },
      {
        "content": "Create an Amazon DynamoDB table. Configure the table with a primary key that consists of the genre as the partition key and the release year as the sort key. Create a global secondary index that uses the title as the partition key.",
        "is_correct": false
      },
      {
        "content": "On an Amazon RDS DB instance, create a table that contains columns for title, release year, and genre. Configure the title as the primary key.",
        "is_correct": false
      },
      {
        "content": "On an Amazon RDS DB instance, create a table where the primary key is the title and all other data is encoded into JSON format as one additional column.",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "we have to search on the basis of title so it is better to partition by title. Also we have to search by genre so it is good option to make GSI using genre as partition key."
    }
  },
  {
    "content": "A developer maintains an Amazon API Gateway REST API. Customers use the API through a frontend UI and Amazon Cognito authentication.<>The developer has a new version of the API that contains new endpoints and backward-incompatible interface changes. The developer needs to provide beta access to other developers on the team without affecting customers.<>Which solution will meet these requirements with the LEAST operational overhead?",
    "widget": "CI",
    "answers": [
      {
        "content": "Define a development stage on the API Gateway API. Instruct the other developers to point the endpoints to the development stage.",
        "is_correct": true
      },
      {
        "content": "Define a new API Gateway API that points to the new API application code. Instruct the other developers to point the endpoints to the new API.",
        "is_correct": false
      },
      {
        "content": "Implement a query parameter in the API application code that determines which code version to call.",
        "is_correct": false
      },
      {
        "content": "Specify new API Gateway endpoints for the API endpoints that the developer wants to add.",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "An API stage is a logical reference to a lifecycle state of your API (for example, dev, prod, beta, or v2).<><>You can consider an API Gateway stage as a specific lifecycle stage or version of your API. For example: `production-stage development-stage beta-stage` From these stages you can configure things like throttling and caching."
    }
  },
  {
    "content": "A developer is creating an application that will store personal health information (PHI). The PHI needs to be encrypted at all times. An encrypted Amazon RDS for MySQL DB instance is storing the data. The developer wants to increase the performance of the application by caching frequently accessed data while adding the ability to sort or rank the cached datasets.<><>Which solution will meet these requirements?",
    "widget": "CI",
    "answers": [
      {
        "content": "Create an Amazon ElastiCache for Redis instance. Enable encryption of data in transit and at rest. Store frequently accessed data in the cache.",
        "is_correct": true
      },
      {
        "content": "Create an Amazon ElastiCache for Memcached instance. Enable encryption of data in transit and at rest. Store frequently accessed data in the cache.",
        "is_correct": false
      },
      {
        "content": "Create an Amazon RDS for MySQL read replica. Connect to the read replica by using SSL. Configure the read replica to store frequently accessed data.",
        "is_correct": false
      },
      {
        "content": "Create an Amazon DynamoDB table and a DynamoDB Accelerator (DAX) cluster for the table. Store frequently accessed data in the DynamoDB table.",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "ElastiCache for Redis also features Online Cluster Resizing, supports encryption, and is HIPAA eligible and PCI DSS compliant.<><>You can use Amazon Elasticache for Redis Sorted Sets to easily implement a dashboard that keeps a list of sorted data by their rank."
    }
  },
  {
    "content": "A company has a multi-node Windows legacy application that runs on premises. The application uses a network shared folder as a centralized configuration repository to store configuration files in .xml format. The company is migrating the application to Amazon EC2 instances. As part of the migration to AWS, a developer must identify a solution that provides high availability for the repository.<><>Which solution will meet this requirement MOST cost-effectively?",
    "widget": "CI",
    "answers": [
      {
        "content": "Mount an Amazon Elastic Block Store (Amazon EBS) volume onto one of the EC2 instances. Deploy a file system on the EBS volume. Use the host operating system to share a folder. Update the application code to read and write configuration files from the shared folder.",
        "is_correct": false
      },
      {
        "content": "Deploy a micro EC2 instance with an instance store volume. Use the host operating system to share a folder. Update the application code to read and write configuration files from the shared folder.",
        "is_correct": false
      },
      {
        "content": "Create an Amazon S3 bucket to host the repository. Migrate the existing .xml files to the S3 bucket. Update the application code to use the AWS SDK to read and write configuration files from Amazon S3.",
        "is_correct": true
      },
      {
        "content": "Create an Amazon S3 bucket to host the repository. Migrate the existing .xml files to the S3 bucket. Mount the S3 bucket to the EC2 instances as a local volume. Update the application code to read and write configuration files from the disk.",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "The question is about more cost effectively. I didn't know you could mount a S3. But I read the documentation and there are lots of things that must be set up (Private Link, File Gateway) and I am sure these will cost money. Yes, S3 can be mounted to an EC2 instance. But you don't need to do that in this scenario. Just use the same S3 bucket and all EC2 instances can access the XML file in S3. if you only perform controlled GET and PUT API calls as this questions states then API option will be more cost effective."
    }
  },
  {
    "content": "A company wants to deploy and maintain static websites on AWS. Each website's source code is hosted in one of several version control systems, including AWS CodeCommit, Bitbucket, and GitHub.<><>The company wants to implement phased releases by using development, staging, user acceptance testing, and production environments in the AWS Cloud. Deployments to each environment must be started by code merges on the relevant Git branch. The company wants to use HTTPS for all data exchange. The company needs a solution that does not require servers to run continuously.<><>Which solution will meet these requirements with the LEAST operational overhead?",
    "widget": "CI",
    "answers": [
      {
        "content": "Host each website by using AWS Amplify with a serverless backend. Conned the repository branches that correspond to each of the desired environments. Start deployments by merging code changes to a desired branch.",
        "is_correct": true
      },
      {
        "content": "Host each website in AWS Elastic Beanstalk with multiple environments. Use the EB CLI to link each repository branch. Integrate AWS CodePipeline to automate deployments from version control code merges.",
        "is_correct": false
      },
      {
        "content": "Host each website in different Amazon S3 buckets for each environment. Configure AWS CodePipeline to pull source code from version control. Add an AWS CodeBuild stage to copy source code to Amazon S3.",
        "is_correct": false
      },
      {
        "content": "Host each website on its own Amazon EC2 instance. Write a custom deployment script to bundle each website's static assets. Copy the assets to Amazon EC2. Set up a workflow to run the script when code is merged.",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "AWS Amplify is an all in one service for the requirement.<><>Host each website in AWS Elastic Beanstalk with multiple environments. Use the EB CLI to link each repository branch. Integrate AWS CodePipeline to automate deployments from version control code merges. -- needs to keep running servers.<><>Host each website in different Amazon S3 buckets for each environment. Configure AWS CodePipeline to pull source code from version control. Add an AWS CodeBuild stage to copy source code to Amazon S3. -- is almost correct, but it does not mention how to implement HTTPS.<><>Host each website on its own Amazon EC2 instance. Write a custom deployment script to bundle each website's static assets. Copy the assets to Amazon EC2. Set up a workflow to run the script when code is merged. -- needs to keep running servers."
    }
  },
  {
    "content": "A company is migrating an on-premises database to Amazon RDS for MySQL. The company has read-heavy workloads. The company wants to refactor the code to achieve optimum read performance for queries.<><>Which solution will meet this requirement with LEAST current and future effort?",
    "widget": "CI",
    "answers": [
      {
        "content": "Use a multi-AZ Amazon RDS deployment. Increase the number of connections that the code makes to the database or increase the connection pool size if a connection pool is in use.",
        "is_correct": false
      },
      {
        "content": "Use a multi-AZ Amazon RDS deployment. Modify the code so that queries access the secondary RDS instance.",
        "is_correct": false
      },
      {
        "content": "Deploy Amazon RDS with one or more read replicas. Modify the application code so that queries use the URL for the read replicas.",
        "is_correct": true
      },
      {
        "content": "Use open source replication software to create a copy of the MySQL database on an Amazon EC2 instance. Modify the application code so that queries use the IP address of the EC2 instance.",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "Heavy reads, use read replica"
    }
  },
  {
    "content": "A developer is creating an application that will be deployed on IoT devices. The application will send data to a RESTful API that is deployed as an AWS Lambda function. The application will assign each API request a unique identifier. The volume of API requests from the application can randomly increase at any given time of day.<><>During periods of request throttling, the application might need to retry requests. The API must be able to handle duplicate requests without inconsistencies or data loss.<><>Which solution will meet these requirements?",
    "widget": "CI",
    "answers": [
      {
        "content": "Create an Amazon RDS for MySQL DB instance. Store the unique identifier for each request in a database table. Modify the Lambda function to check the table for the identifier before processing the request.",
        "is_correct": false
      },
      {
        "content": "Create an Amazon DynamoDB table. Store the unique identifier for each request in the table. Modify the Lambda function to check the table for the identifier before processing the request.",
        "is_correct": true
      },
      {
        "content": "Create an Amazon DynamoDB table. Store the unique identifier for each request in the table. Modify the Lambda function to return a client error response when the function receives a duplicate request.",
        "is_correct": false
      },
      {
        "content": "Create an Amazon ElastiCache for Memcached instance. Store the unique identifier for each request in the cache. Modify the Lambda function to check the cache for the identifier before processing the request.",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "The resolution is to make the Lambda function idempotent. [See docs](https://aws.amazon.com/builders-library/making-retries-safe-with-idempotent-APIs/)"
    }
  },
  {
    "content": "A company hosts a client-side web application for one of its subsidiaries on Amazon S3. The web application can be accessed through Amazon CloudFront from https://www.example.com. After a successful rollout, the company wants to host three more client-side web applications for its remaining subsidiaries on three separate S3 buckets.<><>To achieve this goal, a developer moves all the common JavaScript files and web fonts to a central S3 bucket that serves the web applications. However, during testing, the developer notices that the browser blocks the JavaScript files and web fonts.<><>What should the developer do to prevent the browser from blocking the JavaScript files and web fonts?",
    "widget": "CI",
    "answers": [
      {
        "content": "Create four access points that allow access to the central S3 bucket. Assign an access point to each web application bucket.",
        "is_correct": false
      },
      {
        "content": "Create a bucket policy that allows access to the central S3 bucket. Attach the bucket policy to the central S3 bucket",
        "is_correct": false
      },
      {
        "content": "Create a cross-origin resource sharing (CORS) configuration that allows access to the central S3 bucket. Add the CORS configuration to the central S3 bucket.",
        "is_correct": true
      },
      {
        "content": "Create a Content-MD5 header that provides a message integrity check for the central S3 bucket. Insert the Content-MD5 header for each web application request.",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "This is a frequent trouble. Web applications cannot access the resources in other domains by default, except some exceptions. You must configure CORS on the resources to be accessed."
    }
  },
  {
    "content": "A company has an application that uses Amazon Cognito user pools as an identity provider. The company must secure access to user records. The company has set up multi-factor authentication (MFA). The company also wants to send a login activity notification by email every time a user logs in.<><>What is the MOST operationally efficient solution that meets this requirement?",
    "widget": "CI",
    "answers": [
      {
        "content": "Create an AWS Lambda function that uses Amazon Simple Email Service (Amazon SES) to send the email notification. Add an Amazon API Gateway API to invoke the function. Call the API from the client side when login confirmation is received.",
        "is_correct": false
      },
      {
        "content": "Create an AWS Lambda function that uses Amazon Simple Email Service (Amazon SES) to send the email notification. Add an Amazon Cognito post authentication Lambda trigger for the function.",
        "is_correct": true
      },
      {
        "content": "Create an AWS Lambda function that uses Amazon Simple Email Service (Amazon SES) to send the email notification. Create an Amazon CloudWatch Logs log subscription filter to invoke the function based on the login status.",
        "is_correct": false
      },
      {
        "content": "Configure Amazon Cognito to stream all logs to Amazon Kinesis Data Firehose. Create an AWS Lambda function to process the streamed logs and to send the email notification based on the login status of each user.",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "[See docs](https://docs.aws.amazon.com/cognito/latest/developerguide/user-pool-lambda-post-authentication.html)"
    }
  },
  {
    "content": "A developer has an application that stores data in an Amazon S3 bucket. The application uses an HTTP API to store and retrieve objects. When the PutObject API operation adds objects to the S3 bucket the developer must encrypt these objects at rest by using server-side encryption with Amazon S3 managed keys (SSE-S3).<><>Which solution will meet this requirement?",
    "widget": "CI",
    "answers": [
      {
        "content": "Create an AWS Key Management Service (AWS KMS) key. Assign the KMS key to the S3 bucket.",
        "is_correct": false
      },
      {
        "content": "Set the x-amz-server-side-encryption header when invoking the PutObject API operation.",
        "is_correct": true
      },
      {
        "content": "Provide the encryption key in the HTTP header of every request.",
        "is_correct": false
      },
      {
        "content": "Apply TLS to encrypt the traffic to the S3 bucket.",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "[See docs](https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingServerSideEncryption.html)"
    }
  },
  {
    "content": "A developer needs to perform geographic load testing of an API. The developer must deploy resources to multiple AWS Regions to support the load testing of the API.<><>How can the developer meet these requirements without additional application code?",
    "widget": "CI",
    "answers": [
      {
        "content": "Create and deploy an AWS Lambda function in each desired Region. Configure the Lambda function to create a stack from an AWS CloudFormation template in that Region when the function is invoked.",
        "is_correct": false
      },
      {
        "content": "Create an AWS CloudFormation template that defines the load test resources. Use the AWS CLI create-stack-set command to create a stack set in the desired Regions.",
        "is_correct": true
      },
      {
        "content": "Create an AWS Systems Manager document that defines the resources. Use the document to create the resources in the desired Regions.",
        "is_correct": false
      },
      {
        "content": "Create an AWS CloudFormation template that defines the load test resources. Use the AWS CLI deploy command to create a stack from the template in each Region.",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "[See docs](https://aws.amazon.com/ru/about-aws/whats-new/2021/04/deploy-cloudformation-stacks-concurrently-across-multiple-aws-regions-using-aws-cloudformation-stacksets/)"
    }
  },
  {
    "content": "A developer is creating an application that includes an Amazon API Gateway REST API in the us-east-2 Region. The developer wants to use Amazon CloudFront and a custom domain name for the API. The developer has acquired an SSL/TLS certificate for the domain from a third-party provider.<><>How should the developer configure the custom domain for the application?",
    "widget": "CI",
    "answers": [
      {
        "content": "Import the SSL/TLS certificate into AWS Certificate Manager (ACM) in the same Region as the API. Create a DNS A record for the custom domain.",
        "is_correct": false
      },
      {
        "content": "Import the SSL/TLS certificate into CloudFront. Create a DNS CNAME record for the custom domain.",
        "is_correct": false
      },
      {
        "content": "Import the SSL/TLS certificate into AWS Certificate Manager (ACM) in the same Region as the API. Create a DNS CNAME record for the custom domain.",
        "is_correct": false
      },
      {
        "content": "Import the SSL/TLS certificate into AWS Certificate Manager (ACM) in the us-east-1 Region. Create a DNS CNAME record for the custom domain.",
        "is_correct": true
      }
    ],
    "explanation": {
      "content": "To use a certificate in AWS Certificate Manager (ACM) to require HTTPS between viewers and CloudFront, make sure you request (or import) the certificate in the US East (N. Virginia) Region (us-east-1).<><>[See docs](https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/cnames-and-https-requirements.html)"
    }
  },
  {
    "content": "A developer is creating a template that uses AWS CloudFormation to deploy an application. The application is serverless and uses Amazon API Gateway, Amazon DynamoDB, and AWS Lambda.<><>Which AWS service or tool should the developer use to define serverless resources in YAML?",
    "widget": "CI",
    "answers": [
      {
        "content": "CloudFormation serverless intrinsic functions",
        "is_correct": false
      },
      {
        "content": "AWS Elastic Beanstalk",
        "is_correct": false
      },
      {
        "content": "AWS Serverless Application Model (AWS SAM)",
        "is_correct": true
      },
      {
        "content": "AWS Cloud Development Kit (AWS CDK)",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "[See docs](https://aws.amazon.com/serverless/sam/)"
    }
  },
  {
    "content": "A developer wants to insert a record into an Amazon DynamoDB table as soon as a new file is added to an Amazon S3 bucket.<><>Which set of steps would be necessary to achieve this?",
    "widget": "CI",
    "answers": [
      {
        "content": "Create an event with Amazon EventBridge that will monitor the S3 bucket and then insert the records into DynamoDB.",
        "is_correct": false
      },
      {
        "content": "Configure an S3 event to invoke an AWS Lambda function that inserts records into DynamoDB.",
        "is_correct": true
      },
      {
        "content": "Create an AWS Lambda function that will poll the S3 bucket and then insert the records into DynamoDB.",
        "is_correct": false
      },
      {
        "content": "Create a cron job that will run at a scheduled time and insert the records into DynamoDB.",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "[See docs](https://docs.aws.amazon.com/AmazonS3/latest/userguide/NotificationHowTo.html)"
    }
  },
  {
    "content": "A development team maintains a web application by using a single AWS CloudFormation template. The template defines web servers and an Amazon RDS database. The team uses the Cloud Formation template to deploy the Cloud Formation stack to different environments.<><>During a recent application deployment, a developer caused the primary development database to be dropped and recreated. The result of this incident was a loss of data. The team needs to avoid accidental database deletion in the future.<><>Which solutions will meet these requirements? (Choose two.)",
    "widget": "CM",
    "answers": [
      {
        "content": "Add a CloudFormation Deletion Policy attribute with the Retain value to the database resource.",
        "is_correct": true
      },
      {
        "content": "Update the CloudFormation stack policy to prevent updates to the database.",
        "is_correct": true
      },
      {
        "content": "Modify the database to use a Multi-AZ deployment.",
        "is_correct": false
      },
      {
        "content": "Create a CloudFormation stack set for the web application and database deployments.",
        "is_correct": false
      },
      {
        "content": "Add a Cloud Formation DeletionPolicy attribute with the Retain value to the stack.",
        "is_correct": false
      }
    ],
    "explanation": {
      "content": "[See docs](https://repost.aws/knowledge-center/cloudformation-accidental-updates)"
    }
  }
]
